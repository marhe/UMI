{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://tau-data.id/umi/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img alt=\"\" src=\"images/0_Cover.jpg\"/></center> \n",
    "\n",
    "## <center><font color=\"blue\">Modul 07: PendahuluanText Mining I</font></center>\n",
    "<b><center>(C) Taufik Sutanto - 2019</center>\n",
    "<center>tau-data Indonesia ~ https://tau-data.id ~ taufik@tau-data.id</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><font color=\"blue\">Pendahuluan Text Mining I: Natural Language Processing & Text Preprocessing</font></center>\n",
    "<img alt=\"\" src=\"images/PDS_logo.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <font color=\"blue\">Workshop Schedule</font>\n",
    "\n",
    "## <font color=\"green\">Hari ke-4 (Kamis, 30 Jan 2020)</font>\n",
    "\n",
    "**Pendahuluan Natural Language Processing & Text PreProcessing**\n",
    "* 09:00 – 11:00 Text Preprocessing\n",
    "* 11:00 – 12:00\tCrawling, Streaming, Scraping\n",
    "* 13:00 – 14:00\tText Analytics\n",
    "* 14:00 – 15.00\tSentiment Analysis\n",
    "* 15:00 – 16.00\tLatihan Text Analytics dan Sentiment Analysis \n",
    "\n",
    "Studi Kasus: **Text Analytics data media sosial perbincangan agama di media sosial**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline Bahasan NLP :\n",
    "* Tokenisasi\n",
    "* Stemming dan Lemma\n",
    "* Pos tag \n",
    "* WordNet dan WSD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenisasi\n",
    "\n",
    "<p>Tokenisasi adalah pemisahan kata, simbol, frase, dan entitas penting lainnya (yang disebut sebagai token) dari sebuah teks untuk kemudian di analisa lebih lanjut. Token dalam NLP sering dimaknai dengan &quot;sebuah kata&quot;, walau tokenisasi juga bisa dilakukan ke kalimat, paragraf, atau entitas penting lainnya (misal suatu pola string DNA di Bioinformatika).</p>\n",
    "\n",
    "<p><strong>Mengapa perlu tokenisasi?</strong></p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Langkah penting dalam preprocessing, menghindari kompleksitas mengolah langsung pada string asal.</li>\n",
    "\t<li>Menghindari masalah (semantic) saat pemrosesan model-model natural language.</li>\n",
    "\t<li>Suatu tahapan sistematis dalam merubah unstructured (text) data ke bentuk terstruktur yang lebih mudah di olah.</li>\n",
    "</ul>\n",
    "\n",
    "<p><img alt=\"\" src=\"images\\2_Pipeline_Tokenization.png\" style=\"height:300px; width:768px\" /><br />\n",
    "[<a href=\"https://www.softwareadvice.com/resources/what-is-text-analytics/\" target=\"_blank\"><strong>Image Source</strong></a>]</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Tokenisasi-dengan-modul-NLTK\">Tokenisasi dengan modul NLTK</h2>\n",
    "\n",
    "<p><strong>Kelebihan</strong>:</p>\n",
    "\n",
    "<ol>\n",
    "\t<li>Well established dengan dukungan bahasa yang beragam</li>\n",
    "\t<li>Salah satu modul NLP dengan fungsi terlengkap, termasuk WordNet</li>\n",
    "\t<li>Free dan mendapat banyak dukungan akademis.</li>\n",
    "</ol>\n",
    "\n",
    "<p><strong>Kekurangan</strong>:</p>\n",
    "\n",
    "<ol>\n",
    "\t<li>&quot;Tidak support&quot;&nbsp;bahasa Indonesia</li>\n",
    "\t<li>Murni Python: relatif lebih lambat</li>\n",
    "</ol>\n",
    "\n",
    "<p><big><strong><a href=\"https://www.nltk.org/\" target=\"_blank\">https://www.nltk.org/</a></strong></big></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'Mr.', 'Man', '.', 'He', 'smiled', '!', '!', 'This', ',', 'i.e', '.', 'that', ',', 'is', 'it', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "T = \"Hello, Mr. Man. He smiled!! This, i.e. that, is it.\"\n",
    "Word_Tokens = nltk.word_tokenize(T)\n",
    "print(Word_Tokens) # tokenisasi kata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', 'Mr.', 'Man.', 'He', 'smiled!!', 'This,', 'i.e.', 'that,', 'is', 'it.']\n"
     ]
    }
   ],
   "source": [
    "# Bandingkan jika menggunakan fungsi split di Python, apakah bedanya? \n",
    "print(T.split())\n",
    "# Apakah kesimpulan yang bisa kita tarik?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, Mr. Man.', 'He smiled!!', 'This, i.e.', 'that, is it.']\n"
     ]
    }
   ],
   "source": [
    "Sentence_Tokens = nltk.sent_tokenize(T)\n",
    "print(Sentence_Tokens) # Tokenisasi kalimat\n",
    "# Perhatikan hasilnya, ada berapa kalimat yang di deteksi? setuju?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how are you\n",
      " are you okay?\n",
      "['how are you\\n are you okay?']\n"
     ]
    }
   ],
   "source": [
    "T = \"how are you\\n are you okay?\"\n",
    "print(T)\n",
    "Sentence_Tokens = nltk.sent_tokenize(T)\n",
    "print(Sentence_Tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Diskusi/Latihan:\"><font color=\"blue\">Diskusi/Latihan:</font></h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Apakah tanda baca seperti &quot;?&quot; atau &quot;!&quot; akan memisahkan kalimat?</li>\n",
    "\t<li>Apakah tanda &quot;carriage return&quot;/enter/ganti baris memisahkan kalimat?</li>\n",
    "\t<li>Apakah &quot;;&quot; memisahkan kalimat?</li>\n",
    "\t<li>Apakah tanda dash &quot;-&quot; memisahkan kata? Dalam bahasa Indonesia/Inggris?</li>\n",
    "</ul>\n",
    "\n",
    "<strong>Tips</strong>: Perhatikan bentuk <em>struktur data</em> &quot;output&quot; dari tokenisasi NLTK.<br />\n",
    "<strong>Catatan</strong>: pindah baris di Python string bisa dilakukan dengan menggunakan symbol &quot;\\n&quot;<br />\n",
    "<strong>Contoh</strong>:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baris Pertama\n",
      "Baris Kedua\n"
     ]
    }
   ],
   "source": [
    "Txt = \"Baris Pertama\\nBaris Kedua\"\n",
    "print(Txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jawaban Latihan di cell ini\n",
    "# Tekan tombol \"ctrl + enter\" untuk menjalankan cell-nya\n",
    "Txt = '...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenisasi dengan modul <font color=\"blue\">Spacy</font>\n",
    "<strong>Kelebihan</strong>:</p>\n",
    "<ol>\n",
    "\t<li>Di claim lebih cepat (C-based)</li>\n",
    "\t<li>License termasuk untuk komersil</li>\n",
    "\t<li>Dukungan bahasa yang lebih banyak dari NLTK (termasuk bahasa Indonesia*)</li>\n",
    "</ol>\n",
    "\n",
    "<p><strong>Kekurangan</strong>:</p>\n",
    "<ol>\n",
    "\t<li>Fungsi yang lebih terbatas (dibandingkan NLTK).</li>\n",
    "\t<li>Karena berbasis compiler, sehingga instalasi cukup menantang.</li>\n",
    "</ol>\n",
    "\n",
    "<p><big><strong><a href=\"https://spacy.io/\" target=\"_blank\">https://spacy.io/</a></strong></big></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p><img alt=\"\" src=\"images/2_Spacy.png\" style=\"height:434px; width:800px\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, ,, Mr., Man, ., He, smiled, !, !, This, ,, i.e., that, ,, is, it, ., "
     ]
    }
   ],
   "source": [
    "# Contoh tokenisasi menggunakan Spacy\n",
    "from spacy.lang.en import English\n",
    "nlp_en = English()\n",
    "\n",
    "T = \"Hello, Mr. Man. He smiled!! This, i.e. that, is it.\"\n",
    "nlp = nlp_en(T)\n",
    "for token in nlp:\n",
    "    print(token.text, end =', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Mr. Man.\n",
      "He smiled!!\n",
      "This, i.e. that, is it.\n"
     ]
    }
   ],
   "source": [
    "nlp_en.add_pipe(nlp_en.create_pipe('sentencizer')) # New in latest Spacy\n",
    "\n",
    "nlp = nlp_en(T)\n",
    "for kalimat in nlp.sents:\n",
    "    print(kalimat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Diskusi/Latihan:\"><font color=\"blue\">Diskusi/Latihan:</font></h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Apakah hasil tokenisasi Spacy = NLTK? Mengapa?</li>\n",
    "\t<li>Lakukan latihan seperti yang dilakukan sebelumnya dengan modul NLTK, apakah hasilnya sama dengan Spacy?</li>\n",
    "</ul>\n",
    "\n",
    "<strong>Catatan</strong>: Contoh sederhana ini menekankan perbedaan ilmu linguistik dan computational linguistic.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenisasi dengan <font color=\"blue\"> TextBlob</font>\n",
    "<strong>Kelebihan</strong>:</p>\n",
    "<ol>\n",
    "\t<li>Sederhana &amp; mudah untuk digunakan/pelajari.</li>\n",
    "\t<li>Textblob objects punya behaviour/properties yang sama dengan string di Python.</li>\n",
    "\t<li>TextBlob dibangun dari kombinasi modul NLTK dan (Clips) Pattern</li>\n",
    "</ol>\n",
    "\n",
    "<p><strong>Kekurangan</strong>:</p>\n",
    "<ol>\n",
    "\t<li>Tidak secepat Spacy dan NLTK</li>\n",
    "\t<li>Language Model terbatas: English, German, French</li>\n",
    "</ol>\n",
    "\n",
    "<p>*Blob : Binary large Object</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr', 'Man', 'He', 'smiled', 'This', 'i.e', 'that', 'is', 'it']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing di TextBlob\n",
    "from textblob import TextBlob\n",
    "\n",
    "T = \"Hello, Mr. Man. He smiled!! This, i.e. that, is it.\"\n",
    "print(TextBlob(T).words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, Mr. Man.', 'He smiled!!', 'This, i.e.', 'that, is it.']\n"
     ]
    }
   ],
   "source": [
    "kalimatS = TextBlob(T).sentences\n",
    "print([str(kalimat) for kalimat in kalimatS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Diskusi/Latihan:\"><font color=\"blue\">Diskusi/Latihan:</font></h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Ada yang berbeda dari hasilnya?&nbsp;Apakah lebih baik seperti ini?</li>\n",
    "</ul>\n",
    "\n",
    "<p><strong>Tips</strong>: TextBlob biasa digunakan untuk prototyping pada data yang tidak terlalu besar.<br />\n",
    "<strong>Catatan</strong>: Hati-hati tipe data Blob tidak biasa (objek).</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Mr. Man. <class 'textblob.blob.Sentence'>\n",
      "Hello <class 'textblob.blob.Word'>\n"
     ]
    }
   ],
   "source": [
    "# Saat melakukan coding di Python, selalu perhatikan \"tipe data\" yang dihasilkan oleh modul.\n",
    "A = TextBlob(T).sentences\n",
    "B = TextBlob(T).words\n",
    "print(A[0], type(A[0]))\n",
    "print(B[0], type(B[0]))\n",
    "# Apakah bedanya dengan tipe data str biasa di python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = B[0] # <class 'textblob.blob.Word'>\n",
    "D = 'teks' # tipe string biasa di Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LancasterStemmer', 'PorterStemmer', 'SnowballStemmer', '__add__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mod__', '__module__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'capitalize', 'casefold', 'center', 'correct', 'count', 'define', 'definitions', 'detect_language', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'format_map', 'get_synsets', 'index', 'isalnum', 'isalpha', 'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'lemma', 'lemmatize', 'ljust', 'lower', 'lstrip', 'maketrans', 'partition', 'pluralize', 'pos_tag', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'singularize', 'spellcheck', 'split', 'splitlines', 'startswith', 'stem', 'string', 'strip', 'swapcase', 'synsets', 'title', 'translate', 'translator', 'upper', 'zfill']\n"
     ]
    }
   ],
   "source": [
    "# \"properties\" Blob word\n",
    "print(dir(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mod__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'maketrans', 'partition', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']\n"
     ]
    }
   ],
   "source": [
    "# \"properties\" string di Python\n",
    "print(dir(D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tokenisasi tidak hanya language dependent, tapi juga environment dependent\n",
    "\n",
    "<p>Tokenization sebenarnya tidak sesederhana memisahkan berdasarkan spasi dan removing symbol. Sebagai contoh dalam bahasa Jepang/Cina/Arab suatu kata bisa terdiri dari beberapa karakter.</p>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/2_Tokenization_Complexity.jpg\" style=\"height:500px; width:686px\" /><br />\n",
    "[<a href=\"http://aclweb.org/anthology/Y/Y11/Y11-1038.pdf\" target=\"_blank\"><strong>Image Source</strong></a>]</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@Kirana_Sutanto', 'I', 'am', 'so', 'happpppppppy']\n"
     ]
    }
   ],
   "source": [
    "# Contoh Tokenizer untuk twitter\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "Tokenizer = TweetTokenizer(strip_handles=False, reduce_len=False)\n",
    "tweet = \"@Kirana_Sutanto I am so happpppppppy\"\n",
    "print(Tokenizer.tokenize(tweet))\n",
    "\n",
    "# Masih salah (i.e. \"happpy\"), nanti kita akan perbaiki ini dengan \"spell check\"\n",
    "# catatan: pada permasalahan \"Sentiment analysis\" kata yang ditulis panjang seperti diatas \n",
    "# bisa mengindikasikan sentiment yang kuat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tokenisasi (NLP) Bahasa Indonesia:\n",
    "\n",
    "<p>NLTK belum support Bahasa Indonesia, bahkan module NLP Python yang support bahasa Indonesia secara umum masih sangat langka. Beberapa <u><strong>resources </strong></u>yang dapat digunakan:</p>\n",
    "\n",
    "<ol>\n",
    "\t<li><strong><a href=\"https://github.com/kirralabs/indonesian-NLP-resources\" target=\"_blank\">KirraLabs</a></strong>: Mix of NLP-TextMining resources</li>\n",
    "\t<li><strong><a href=\"https://pypi.python.org/pypi/Sastrawi/1.0.1\" target=\"_blank\">Sastrawi 1.0.1</a>:</strong>&nbsp;untuk &quot;stemming&quot; &amp;&nbsp;<strong><a href=\"https://devtrik.com/python/stopword-removal-bahasa-indonesia-python-sastrawi/\" target=\"_blank\">stopwords&nbsp;</a></strong>bahasa Indonesia.</li>\n",
    "\t<li><strong><a href=\"http://stop-words-list-bahasa-indonesia.blogspot.co.id/2012/09/daftar-kata-dasar-bahasa-indonesia.html\" target=\"_blank\">Daftar Kata Dasar Indonesia</a></strong>:&nbsp;Bisa di load sebagai dictionary di Python</li>\n",
    "\t<li><strong><a href=\"https://id.wiktionary.org/wiki/Wiktionary:ProyekWiki_bahasa_Indonesia/Daftar_kata\" target=\"_blank\">Wiktionary</a></strong>: ProyekWiki bahasa Indonesia [termasuk Lexicon]</li>\n",
    "\t<li><a href=\"http://wn-msa.sourceforge.net/\" target=\"_blank\"><strong>WordNet Bahasa Indonesia</strong></a>: Bisa di load&nbsp;sebagai dictionary (atau NLTK<em>*</em>) di Python.</li>\n",
    "\t<li><strong><a href=\"http://kakakpintar.com/daftar-kata-baku-dan-tidak-baku-a-z-dalam-bahasa-indonesia/\" target=\"_blank\">Daftar Kata Baku-Tidak Baku</a></strong>: Bisa di load sebagai dictionary di Python.</li>\n",
    "\t<li><strong><a href=\"https://spacy.io/\" target=\"_blank\">Spacy</a></strong>: Cepat/efisien, MIT License, tapi language model Indonesia masih terbatas.</li>\n",
    "\t<li><a href=\"http://ufal.mff.cuni.cz/udpipe\" target=\"_blank\"><strong>UdPipe</strong></a>: Online request &amp; restricted license (support berbagai bahasa -&nbsp;pemrograman).</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sore, itu, ,, Hamzah, melihat, kupu-kupu, di, taman, ., Ibu, membeli, oleh-oleh, di, pasar]\n"
     ]
    }
   ],
   "source": [
    "# Contoh Tokenisasi dalam bahasa Indonesia dengan Spacy\n",
    "from spacy.lang.id import Indonesian\n",
    "nlp_id = Indonesian()  # Language Model\n",
    "\n",
    "teks = 'Sore itu, Hamzah melihat kupu-kupu di taman. Ibu membeli oleh-oleh di pasar'\n",
    "tokenS_id = nlp_id(teks)\n",
    "#T = []\n",
    "#for token in tokenS_id:\n",
    "#    T.append(token)\n",
    "print([t for t in tokenS_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = []\n",
    "for i in range(7):\n",
    "    A.append(i)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in range(7)] # list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sore', 'itu', ',', 'Hamzah', 'melihat', 'kupu', '-', 'kupu', 'di', 'taman', '.', 'Ibu', 'membeli', 'oleh', '-', 'oleh', 'di', 'pasar']\n"
     ]
    }
   ],
   "source": [
    "# Jika menggunakan Language model English:\n",
    "tokenS_en = nlp_en(teks)\n",
    "print([token.text for token in tokenS_en])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p><u><big><strong>Word Case</strong></big></u><big> (Huruf BESAR/kecil):</big></p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Untuk menganalisa makna (<em>semantic</em>) dari suatu (frase) kata dan mencari informasi dalam proses textmining, seringnya (*) kita tidak membutuhkan informasi huruf besar/kecil dari kata&nbsp;tersebut.</li>\n",
    "\t<li><em>Text case normaliation</em> dapat dilakukan pada string secara efisien tanpa melalui tokenisasi (mengapa?).</li>\n",
    "\t<li>Namun, bergantung pada analisa teks yang akan digunakan pengguna harus berhati-hati dengan urutan proses (pipelining) dalam preprocessing. Mengapa dan apa contohnya?</li>\n",
    "</ul>\n",
    "\n",
    "<p>(*) Coba temukan minimal 2 pengecualian dimana&nbsp; huruf kapital/kecil (case) mempengaruhi makna/pemrosesan teks.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi there!, i am a student. nice to meet you :)\n",
      "HI THERE!, I AM A STUDENT. NICE TO MEET YOU :)\n"
     ]
    }
   ],
   "source": [
    "# Ignore case (huruf besar/kecil)\n",
    "T = \"Hi there!, I am a student. Nice to meet you :)\"\n",
    "print(T.lower())\n",
    "print(T.upper())\n",
    "# Perintah ini sangat efisien karena hanya merubah satu bit di setiap (awal) bytes dari setiap karakter\n",
    "# Sehingga tetap efisien jika ingin dilakukan sebelum tokenisasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Morphological-Linguistic Normalization: Stemming &amp; Lemmatization\n",
    "(Canonical Representation)\n",
    "<p><img alt=\"\" src=\"images/2_yoda.jpg\" style=\"height:400px; width:400px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Stemming dan Lemma</font>\n",
    "\n",
    "<ol>\n",
    "\t<li>\n",
    "\t<p><strong>Stemmer</strong>&nbsp;akan menghasilkan sebuah bentuk kata yang disepakati oleh suatu sistem tanpa mengindahkan konteks kalimat. Syaratnya beberapa kata dengan makna serupa hanya perlu dipetakan secara konsisten ke sebuah kata baku.&nbsp;Banyak digunakan di IR &amp;&nbsp;komputasinya relatif sedikit. Biasanya dilakukan dengan menghilangkan imbuhan (suffix/prefix).</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><strong>lemmatisation</strong> akan menghasilkan kata baku (dictionary word) dan bergantung konteks.</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p>Lemma &amp; stemming bisa jadi sama-sama menghasilkan suatu akar kata (root word). Misal : <em>Melompat </em>==&gt; <em>lompat</em></p>\n",
    "\t</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Mengapa melakukan Stemming &amp; Lemmatisasi</strong>?</p>\n",
    "\n",
    "<ol>\n",
    "\t<li>Sering digunakan di IR (Information Retrieval) agar ketika seseorang mencari kata tertentu, maka seluruh kata yang terkait juga diikutsertakan.<br />\n",
    "\tMisal:&nbsp;<em>organize</em>,&nbsp;<em>organizes</em>, and&nbsp;<em>organizing&nbsp;</em>&nbsp;dan&nbsp;<em>democracy</em>,&nbsp;<em>democratic</em>, and&nbsp;<em>democratization</em>.</li>\n",
    "\t<li>Di Text Mining Stemming dan Lemmatisasi akan mengurangi dimensi (mengurangi variasi morphologi), yang terkadang akan meningkatkan akurasi.</li>\n",
    "\t<li>Tapi di IR efeknya malah berkebalikan: <strong><font color=\"blue\">meningkatkan recall, tapi menurunkan akurasi&nbsp;</font></strong>[<a href=\"https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\" target=\"_blank\"><strong>Link</strong></a>]. Contoh: kata&nbsp;<em>operate, operating, operates, operation, operative, operatives, dan operational</em>&nbsp;jika di stem menjadi <em>operate</em>, maka ketika seseorang mencari &quot;<em>operating system</em>&quot;, maka entry seperti&nbsp;<em>operational and research</em> dan&nbsp;<em>operative and dentistry</em>&nbsp;akan muncul sebagai entry dengan relevansi yang cukup tinggi.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p><strong>Stemming tidak perlu &quot;benar&quot;, hanya perlu konsisten. Sehingga memiliki berbagai variansi, (sebagian) contoh di NLTK:</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  presumably I would like to MultiPly my provision, saying tHat without crYing\n",
      "Lancaster :  presum i would lik to multiply my provision, say that without cry\n",
      "Porter :  presum I would like to multipli my provision, say that without cri\n",
      "SnowBall :  presum i would like to multipli my provision, say that without cri\n"
     ]
    }
   ],
   "source": [
    "# Contoh Stemming di NLTK\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "T = 'presumably I would like to MultiPly my provision, saying tHat without crYing'\n",
    "print('Sentence: ',T)\n",
    "\n",
    "StemmerS = [LancasterStemmer, PorterStemmer, SnowballStemmer] \n",
    "Names = ['Lancaster', 'Porter', 'SnowBall']\n",
    "\n",
    "for stemmer_name,stem in zip(Names,StemmerS):\n",
    "    if stemmer_name == 'SnowBall':\n",
    "        st = stem('english')\n",
    "    else:\n",
    "        st = stem()\n",
    "        \n",
    "    print(stemmer_name,': ',' '.join(st.stem(t) for t in T.split()))\n",
    "# perhatikan, kita tidak melakukan case normalization (lowercase) \n",
    "# Hasil stemming bisa tidak bermakna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  apples and oranges are similar. boots and hippos aren't, don't you think?\n",
      "Lemmatize:  apple and orange are similar. boot and hippo aren't, don't you think?\n"
     ]
    }
   ],
   "source": [
    "# Contoh Lemmatizer di NLTK\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "T = \"apples and oranges are similar. boots and hippos aren't, don't you think?\"\n",
    "print('Sentence: ', T)\n",
    "print('Lemmatize: ',' '.join(lemmatizer.lemmatize(t) for t in T.split()))\n",
    "# Lemma case sensitive. Dengan kata lain string harus diubah ke dalam bentuk huruf kecil (lower case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "better\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizer menggunakan informasi pos. \"pos\" (part-of-speech) akan dibahas di segmen berikutnya\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\")) # adjective\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"v\")) # verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "go\n"
     ]
    }
   ],
   "source": [
    "# TextBlob Stemming & Lemmatizer\n",
    "from textblob import Word\n",
    "# Stemming\n",
    "print(Word('running').stem()) # menggunakan NLTK Porter stemmer\n",
    "\n",
    "# Lemmatizer\n",
    "print(Word('went').lemmatize('v'))\n",
    "\n",
    "# default Noun, plural akan menjadi singular dari akar katanya\n",
    "# Juga case sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I be sure Apples and orange be similar . Boots and hippo be not , do not you think ?\n"
     ]
    }
   ],
   "source": [
    "# Spacy Lemmatizer English\n",
    "E = \"I am sure Apples and oranges are similar. Boots and hippos aren't, don't you think?\"\n",
    "en = nlp_en(E)\n",
    "print( ' '.join( k.lemma_ for k in en ) )\n",
    "\n",
    "# Perhatikan huruf besar/kecil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apa', 'kabar', 'hari', 'ini?']\n",
      "apa kabar hari ini?\n"
     ]
    }
   ],
   "source": [
    "A = ['apa', 'kabar', 'hari', 'ini?']\n",
    "print(A)\n",
    "print(' '.join(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raya itu urus dengan saat kita tonton ke Jogjakarta\n"
     ]
    }
   ],
   "source": [
    "# Spacy Lemmatizer Indonesia\n",
    "I = \"perayaan itu mengurus dengan saat kita menonton ke Jogjakarta\"\n",
    "idn = nlp_id(I)\n",
    "print( ' '.join( k.lemma_ for k in idn ) )\n",
    "# Hati-hati huruf besar/kecil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy \"tidak\" (bukan belum) support Stemming:\n",
    "\n",
    "<p><strong><img alt=\"\" src=\"images/2_Spacy_Pipelines.jpg\" style=\"height:400px; width:487px\" /></strong></p>\n",
    "\n",
    "<p>[<a href=\"https://spacy.io/usage/spacy-101#lightning-tour\" target=\"_blank\"><strong>Image Source</strong></a>]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raya itu bareng dengan saat kita pergi ke makassar\n",
      "raya pergi suara\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizer dengan Sastrawi\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "\n",
    "I = \"perayaan itu Berbarengan dengan saat kita bepergian ke Makassar\"\n",
    "print(stemmer.stem(I))\n",
    "print(stemmer.stem(\"Perayaan Bepergian Menyuarakan\"))\n",
    "# Ada beberapa hal yang berbeda antara Sastrawi dan modul-modul diatas.\n",
    "# Apa sajakah?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 id=\"Tips:\">Tips:</h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Secara umum &#39;biasanya&#39; di Text Mining yang kita butuhkan hanyalah <strong><font color=\"blue\">Lemma</font></strong>.</li>\n",
    "\t<li>&quot;Kecuali&quot; di aplikasi IR, spelling correction, variasi kata, clustering, atau terkadang klasifikasi. Pada aplikasi-aplikasi tersebut stemming terkadang lebih diinginkan.</li>\n",
    "\t<li>Stemming jauh lebih cepat, tapi tidak selalu tersedia di modul NLP.</li>\n",
    "\t<li>Beberapa algoritma tertentu membutuhkan tanda &quot;.&quot; dan &quot;,&quot; : contohnya untuk document summarization di textRank.</li>\n",
    "\t<li>&quot;_&quot; juga biasa digunakan untuk menyatakan frase kata di representasi n-grams (misal &quot;buah_tangan&quot;).</li>\n",
    "\t<li>Stemming juga digunakan pada Word Sense Disambiguation (WSD)</li>\n",
    "</ul>\n",
    "\n",
    "<h3 id=\"Diskusi:\">Diskusi:</h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Untuk menghemat storage database, apakah sebaiknya kita menyimpan saja hasil preprocessed texts/documents?</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech (pos) di ilmu bahasa (Linguistik)\n",
    "<p><img alt=\"\" src=\"images/2_parts-of-speech-chart.jpg\" style=\"height:400px; width:404px\" /></p>\n",
    "<p>[<a href=\"https://www.paperrater.com/page/parts-of-speech\" target=\"_blank\">image source</a>]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('currently', 'RB'), ('learning', 'VBG'), ('NLP', 'NNP'), ('in', 'IN'), ('English', 'NNP'), (',', ','), ('but', 'CC'), ('if', 'IN'), ('possible', 'JJ'), ('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('know', 'VB'), ('NLP', 'NNP'), ('in', 'IN'), ('Indonesian', 'JJ'), ('language', 'NN'), ('too', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "# POS tags in NLTK (English)\n",
    "import nltk\n",
    "T = 'I am currently learning NLP in English, but if possible I want to know NLP in Indonesian language too'\n",
    "\n",
    "nltk_tokens = nltk.word_tokenize(T)\n",
    "print(nltk.pos_tag(nltk_tokens))\n",
    "# Tidak lagi hanya 9 macam tags seperti yang dibahas ahli bahasa (linguist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('too', 'RB')"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = nltk.pos_tag(nltk_tokens)\n",
    "Z[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 9]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [2,3,9]\n",
    "B = [2, 5, 3, 9, 7, 8, 1]\n",
    "\n",
    "[x for x in B if x in A]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('currently', 'RB'),\n",
       " ('learning', 'VBG'),\n",
       " ('NLP', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('English', 'NNP'),\n",
       " (',', ','),\n",
       " ('but', 'CC'),\n",
       " ('if', 'IN'),\n",
       " ('possible', 'JJ'),\n",
       " ('I', 'PRP'),\n",
       " ('want', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('know', 'VB'),\n",
       " ('NLP', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('Indonesian', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('too', 'RB')]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['possible', 'Indonesian', 'language']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtering berdasarkan \"pos\"\n",
    "pos = ['NN','JJ']\n",
    "hasil = []\n",
    "for pt in Z:\n",
    "    if pt[1] in pos:\n",
    "        hasil.append(pt[0])\n",
    "hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['possible', 'Indonesian', 'language']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[h[0] for h in Z if h[1] in pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I_PRP', 'am_VBP', 'currently_RB', 'learning_VBG', 'NLP_NNP', 'in_IN', 'English_NNP', ',_,', 'but_CC', 'if_IN', 'possible_JJ', 'I_PRP', 'want_VBP', 'to_TO', 'know_VB', 'NLP_NNP', 'in_IN', 'Indonesian_JJ', 'language_NN', 'too_RB']\n"
     ]
    }
   ],
   "source": [
    "# Penggunaan di text mining jika suatu kata ingin dibedakan jika berbeda pos\n",
    "hasil = []\n",
    "for pt in Z:\n",
    "    hasil.append(pt[0]+'_'+pt[1])\n",
    "print(hasil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daftar NLTK pos-tags:\n",
    "<img alt=\"\" src=\"images/2_post_tags_NLTK.png\" style=\"height:400px; width:516px\" /></h3>\n",
    "\n",
    "<p>[<a href=\"http://gitqwerty777.github.io/natural-language-processing/\" target=\"_blank\">image source</a>]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRP, am VBP, currently RB, learning VBG, NLP NNP, in IN, English NNP, but CC, if IN, possible JJ, I PRP, want VBP, to TO, know VB, NLP NNP, in IN, Indonesian JJ, language NN, too RB, "
     ]
    }
   ],
   "source": [
    "# Pos tags in TextBlob (English)\n",
    "for word, pos in TextBlob(T).tags:\n",
    "    print(word, pos, end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRP, am VBP, currently RB, learning VBG, NLP NNP, in IN, English NNP, , ,, but CC, if IN, possible JJ, I PRP, want VBP, to TO, know VB, NLP NNP, in IN, Indonesian JJ, language NN, too RB, "
     ]
    }
   ],
   "source": [
    "# Pos Tag Spacy English\n",
    "#from spacy.lang.en import English\n",
    "#nlp_en = English()\n",
    "import spacy\n",
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "tokens = nlp_en(T)\n",
    "for tok in tokens:\n",
    "    print(tok, tok.tag_, end = ', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noun, proper singular'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spacy tidak perlu tabel pos tag ...  bisa pakai perintah \"explain\"\n",
    "spacy.explain('NNP')\n",
    "# Daftar Lengkap: https://spacy.io/api/annotation#pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saat \n",
      "pergi \n",
      "ke \n",
      "Jogjakarta \n",
      "jangan \n",
      "lupa \n",
      "beli \n",
      "oleh-oleh \n"
     ]
    }
   ],
   "source": [
    "# Pos Tags in Spacy - Bahasa Indonesia?\n",
    "\n",
    "Ti = \"Saat bepergian ke Jogjakarta jangan lupa membeli oleh-oleh\"\n",
    "Teks = nlp_id(Ti)\n",
    "for token in Teks:\n",
    "    print(token.lemma_, token.tag_)\n",
    "# Fungsi pos-tags belum tersedia untuk bahasa indonesia .. :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Saya', 'PRP'), ('bekerja', 'VB'), ('di', 'IN'), ('Bandung', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "# Pos Tag Bahasa Indonesia lewat NLTK\n",
    "# https://yudiwbs.wordpress.com/2018/02/20/pos-tagger-bahasa-indonesia-dengan-pytho/\n",
    "from nltk.tag import CRFTagger\n",
    "ct = CRFTagger()\n",
    "ct.set_model_file('data/all_indo_man_tag_corpus_model.crf.tagger')\n",
    "\n",
    "hasil = ct.tag_sents([['Saya','bekerja','di','Bandung']])\n",
    "hasil = hasil[0]\n",
    "print(hasil)\n",
    "# Hati-hati dengan struktur data inputnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Saya', 'PRP')"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasil[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saat: n, pergi: v, ke: pre, Jogjakarta: n, jangan: adv, lupa: v, beli: v, oleh-oleh: n, "
     ]
    }
   ],
   "source": [
    "# pos tag Indonesia sederhana dari daftar kata\n",
    "# Secara manual memuat kata dasar dan informasi pos-nya. \n",
    "# Catatan: tanpa \"sense\"/hierarchy/synonyms dan gagal untuk idioms (istilah)\n",
    "import taudata as tau\n",
    "kata_pos = tau.loadPos_id()\n",
    "\n",
    "Teks = nlp_id(Ti)\n",
    "for token in Teks:\n",
    "    print(token.lemma_, end= ': ')\n",
    "    try:\n",
    "        print(kata_pos[token.lemma_], end= ', ')\n",
    "    except:\n",
    "        print('n', end= ', ') # default pos to noun seperti NLTK, Spacy, & TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pip install --only-binary :all: mysqlclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diskusi : kapan pakai pos tag: Topic Modelling, Summarization, Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh penggunaan pos-tag dengan hanya mengambil \"Noun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh penggunaan pos tag dengan menambahkan tag ke setiap kata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 id=\"Outline-Module-I.3.-:\">Outline PreProcessing Data Text :</h2>\n",
    "\n",
    "<ul>\n",
    "\t<li>Review String di Python</li>\n",
    "\t<li>Filtering (stopwords)</li>\n",
    "\t<li>Replace slang/typos/singkatan</li>\n",
    "    <li>Spell Check</li>\n",
    "\t<li>Machine translation</li>\n",
    "\t<li>Reguler expression</li>\n",
    "\t<li>Encodings</li>\n",
    "    <li>Latihan</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Tipe Variabel di Python (string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2.3 # Floating Point (tidak sama dengan bilangan Real)\n",
    "b = 3.0   # Integer\n",
    "c = True # T/F Boolean\n",
    "d = 'python' # String\n",
    "e = [a,b,c,d] # List\n",
    "f = (a,b,c,d) # Tuple\n",
    "g = set([a,b,c,d]) # Set\n",
    "h = {'a':1, 'b':2, 7:'abc'} # Dictionary : keys, values, items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = b/2\n",
    "type(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tekan \"d.\" lalu tekan tombol \"tab\"\n",
    "#d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coba \n",
      "coba\n",
      "ciba \n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Sehingga kita bisa melakukan preprocessing dasar dengan string manipulation seperti berikut:\n",
    "S = 'coba '\n",
    "print(S)\n",
    "print(S.strip())\n",
    "print(S.replace(\"o\",\"i\"))\n",
    "print(S.isalnum())\n",
    "print(S.strip().isalnum()) # dsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function isdecimal:\n",
      "\n",
      "isdecimal(...) method of builtins.str instance\n",
      "    S.isdecimal() -> bool\n",
      "    \n",
      "    Return True if there are only decimal characters in S,\n",
      "    False otherwise.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = '764'\n",
    "X.isdigit() \n",
    "help(X.isdecimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apa kabar'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# String adalah Tuple, sehingga\n",
    "S1 = 'apa '\n",
    "S2 = 'kabar'\n",
    "S1+S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jklmnopqrst'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S2 = 'abcdefghijklmnopqrst'\n",
    "S2[9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 character pertama:  NLP dan\n",
      "7 character terakhir:   Python\n"
     ]
    }
   ],
   "source": [
    "# String adalah Tuple sehingga bisa di akses seperti List\n",
    "S = 'NLP dan TextMining di Python'\n",
    "print('7 character pertama: ', S[:7])\n",
    "print('7 character terakhir: ', S[-7:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error, tuple tidak bisa dirubah nilainya (inplace)\n"
     ]
    }
   ],
   "source": [
    "# Tapi hati-hati karena ia tuple maka:\n",
    "try:\n",
    "    S[5] = 'o'\n",
    "except:\n",
    "    print('Error, tuple tidak bisa dirubah nilainya (inplace)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 0\n",
      "error\n",
      "sesuatu\n"
     ]
    }
   ],
   "source": [
    "x = input('x = ')\n",
    "try:\n",
    "    print(1/int(x))\n",
    "except:\n",
    "    print('error')\n",
    "print('sesuatu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Level Normalization: StopWords\n",
    "<p><u>Di Text Mining</u> kata-kata yang <strong>sering muncul </strong>(dan jarang sekali muncul) memiliki sedikit sekali informasi (signifikansi) terhadap model (machine learning) yang digunakan. Hal ini di karenakan kata-kata tersebut muncul di semua kategori (di permasalahan klasifikasi) atau di semua cluster (di permasalahan pengelompokan/clustering). Kata-kata yang sering muncul ini biasa disebut &quot;StopWords&quot;. Stopwords berbeda-beda bergantung dari Bahasa dan Environment (aplikasi)-nya.<br />\n",
    "<strong>Contoh</strong>:<br />\n",
    "\n",
    "<ul>\n",
    "\t<li>Stopwords bahasa Inggris: am, is, are, do, the, of, etc.</li>\n",
    "\t<li>Stopwords bahasa Indonesia: adalah, dengan, yang, di, ke, dsb</li>\n",
    "\t<li>Stopwords twitter: RT, ...<br />\n",
    "\t<img alt=\"\" src=\"images/2_StopWords.png\" style=\"height:250px; width:419px\" /></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we']\n",
      "['yang', 'untuk', 'pada', 'ke', 'para']\n",
      "['&gt', '&lt', '&nbsp', 'a', 'able']\n",
      "['ada', 'adalah', 'adanya', 'adapun', 'agak']\n",
      "126 755 179 2659\n"
     ]
    }
   ],
   "source": [
    "# Loading Stopwords: Ada beberapa cara\n",
    "import taudata as tau\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "factory = StopWordRemoverFactory()\n",
    "\n",
    "NLTK_StopWords = stopwords.words('english')\n",
    "Sastrawi_StopWords_id = factory.get_stop_words()\n",
    "Personal_StopWords_en = [t.strip() for t in tau.LoadDocuments(file = 'data/stopwords_eng.txt')[0]]\n",
    "Personal_StopWords_id = [t.strip() for t in tau.LoadDocuments(file = 'data/stopwords_id.txt')[0]]\n",
    "\n",
    "print(NLTK_StopWords[:5])\n",
    "print(Sastrawi_StopWords_id[:5])\n",
    "print(Personal_StopWords_en[:5])\n",
    "print(Personal_StopWords_id[:5])\n",
    "print(len(Sastrawi_StopWords_id), len(Personal_StopWords_id), len(NLTK_StopWords), len(Personal_StopWords_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diskusi: Apakah sebaiknya kita menggunakan daftar stopwords bawaan modul atau custom milik kita sendiri?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tipe variabel memiliki aplikasi optimal yang berbeda-beda, misal\n",
    "L = list(range(10**7))\n",
    "S = set(range(10**7)) # selain unik dan tidak memiliki keterurutan, set memiliki fungsi lain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109 ms ± 18.6 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "9000000 in L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.5 ns ± 17.6 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "9000000 in S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tips: selalu rubah list stopwords ke bentuk set, karena di Python jauh lebih cepat untuk cek existence di set ketimbang list\n",
    "NLTK_StopWords = set(NLTK_StopWords)\n",
    "Sastrawi_StopWords_id = set(Sastrawi_StopWords_id)\n",
    "Personal_StopWords_en = set(Personal_StopWords_en)\n",
    "Personal_StopWords_id = set(Personal_StopWords_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "St = ['ada', 'pesawat']\n",
    "'Ada' in St"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp ittc belajar nlp ittc\n"
     ]
    }
   ],
   "source": [
    "# Cara menggunakan stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "T = \"I am doing NLP at ITTC,... \\\n",
    "    adapun saya anu sedang belajar NLP di ITTC\"\n",
    "T = T.lower()\n",
    "Personal_StopWords_id.add('anu')\n",
    "Tokens = TextBlob(T).words # Tokenisasi \n",
    "T2 = [t for t in Tokens if t not in Personal_StopWords_id] # Sastrawi_StopWords_id Personal_StopWords_en Personal_StopWords_id\n",
    "T2 = [t for t in T2 if t not in Personal_StopWords_en] # Sastrawi_StopWords_id Personal_StopWords_en Personal_StopWords_id\n",
    "print(' '.join(T2))\n",
    "# Catatan: Selalu lakukan Stopword filtering setelah tokenisasi (dan normalisasi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/2_Tokenization_Stopwords.png\" style=\"height:400px; width:765px\" /></p>\n",
    "\n",
    "<p>[<a href=\"http://chdoig.github.io/acm-sigkdd-topic-modeling/#/6/2\" target=\"_blank\">image source</a>]</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menangani Slang atau Singkatan di Data Teks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "janjuragan ragu juragan, langsung saja di order pajanjuragannya.\n"
     ]
    }
   ],
   "source": [
    "# Sebuah contoh sederhana \n",
    "T = 'jangan ragu gan, langsung saja di order pajangannya.'\n",
    "# Misal kita hendak mengganti setiap singkatan (slang) dengan bentuk penuhnya. \n",
    "# Dalam hal ini kita hendak mengganti 'gan' dengan 'juragan'\n",
    "H = T.replace('gan','juragan')\n",
    "print(H)\n",
    "# Kita tidak bisa melakukan ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'juragan'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = {'yg':'yang', 'gan':'juragan'}\n",
    "D['gan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0\n",
      "5 1\n",
      "1 2\n",
      "3 3\n"
     ]
    }
   ],
   "source": [
    "A = [2,5,1,3]\n",
    "\n",
    "for i, a in enumerate(A):\n",
    "    print(a, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['jangan', 'ragu', 'gan', 'langsung', 'saja', 'di', 'order', 'pajangan', 'yg', 'diatas'])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dengan tokenisasi\n",
    "slangs = {'gan':'juragan', 'yg':'yang', 'dgn':'dengan'} #dictionary sederhana berisi daftar singkatan dan kepanjangannya\n",
    "\n",
    "T = 'jangan ragu gan, langsung saja di order pajangan yg diatas.'\n",
    "T = TextBlob(T).words\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jangan ragu juragan langsung saja di order pajangan yang diatas\n"
     ]
    }
   ],
   "source": [
    "for i,t in enumerate(T):\n",
    "    if t in slangs.keys():\n",
    "        T[i] = slangs[t]\n",
    "print(' '.join(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u: you\\n',\n",
       " 'luv :love\\n',\n",
       " 'gan:juragan\\n',\n",
       " 'say:sayang\\n',\n",
       " 'ndak:tidak\\n',\n",
       " 'tau:tahu\\n',\n",
       " 'sesok:besok\\n',\n",
       " 'dgn:dengan\\n',\n",
       " 'wkt:waktu\\n',\n",
       " 'yg:yang\\n',\n",
       " 'tsb:tersebut\\n',\n",
       " 'shg:sehingga\\n',\n",
       " 'kalo:kalau\\n',\n",
       " 'dpt:dapat\\n',\n",
       " 'sbg:sebagai\\n',\n",
       " 'kpd:kepada\\n',\n",
       " 'utk:untuk\\n',\n",
       " 'gak:tidak\\n',\n",
       " 'bgmn:bagaimana\\n',\n",
       " 'dsb:dan sebagainya\\n']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Slang dan Singkatan dari File\n",
    "# Contoh memuat word fix melalui import file. \n",
    "slangS = tau.LoadDocuments(file = 'data/slang.dic')[0]\n",
    "slangS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u: you', 'luv :love', 'gan:juragan', 'say:sayang', 'ndak:tidak', 'tau:tahu', 'sesok:besok', 'dgn:dengan', 'wkt:waktu', 'yg:yang', 'tsb:tersebut', 'shg:sehingga', 'kalo:kalau', 'dpt:dapat', 'sbg:sebagai', 'kpd:kepada', 'utk:untuk', 'gak:tidak', 'bgmn:bagaimana', 'dsb:dan sebagainya']\n"
     ]
    }
   ],
   "source": [
    "slangS = [t.strip('\\n').strip() for t in slangS]\n",
    "print(slangS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['luv', 'love']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = 'luv:love'\n",
    "B = A.split(':')\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apa'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A ='  apa  '\n",
    "A.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['u', 'you'], ['luv', 'love'], ['gan', 'juragan'], ['say', 'sayang'], ['ndak', 'tidak'], ['tau', 'tahu'], ['sesok', 'besok'], ['dgn', 'dengan'], ['wkt', 'waktu'], ['yg', 'yang'], ['tsb', 'tersebut'], ['shg', 'sehingga'], ['kalo', 'kalau'], ['dpt', 'dapat'], ['sbg', 'sebagai'], ['kpd', 'kepada'], ['utk', 'untuk'], ['gak', 'tidak'], ['bgmn', 'bagaimana'], ['dsb', 'dan sebagainya']]\n"
     ]
    }
   ],
   "source": [
    "# pisahkan berdasarkan ':'\n",
    "slangS = [t.split(\":\") for t in slangS]\n",
    "slangS = [[k.strip(), v.strip()] for k,v in slangS]\n",
    "print(slangS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'u': 'you',\n",
       " 'luv': 'love',\n",
       " 'gan': 'juragan',\n",
       " 'say': 'sayang',\n",
       " 'ndak': 'tidak',\n",
       " 'tau': 'tahu',\n",
       " 'sesok': 'besok',\n",
       " 'dgn': 'dengan',\n",
       " 'wkt': 'waktu',\n",
       " 'yg': 'yang',\n",
       " 'tsb': 'tersebut',\n",
       " 'shg': 'sehingga',\n",
       " 'kalo': 'kalau',\n",
       " 'dpt': 'dapat',\n",
       " 'sbg': 'sebagai',\n",
       " 'kpd': 'kepada',\n",
       " 'utk': 'untuk',\n",
       " 'gak': 'tidak',\n",
       " 'bgmn': 'bagaimana',\n",
       " 'dsb': 'dan sebagainya'}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = {} # dictionary kosong (inisialisasi)\n",
    "for k,v in slangS:\n",
    "    D[k]=v\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sebagai'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test it!\n",
    "tweet = 'I luv u say. serius gan!, tapi ndak tau kalau sesok.'\n",
    "T = TextBlob(tweet).words\n",
    "\n",
    "for i,t in enumerate(T):\n",
    "    if t in slangS.keys():\n",
    "        T[i] = slangS[t]\n",
    "        \n",
    "print(' '.join(T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Spell Check:</h3>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/2_SpellCheck.jpg\" style=\"height:414px; width:499px\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tujuan Spellcheck:</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>\tCleaning Data\t</li>\n",
    "\t<li>Word suggestions</li>\n",
    "\t<li>OCR/hand writing (Image) recognition</li>\n",
    "\t<li>Speech Recognition</li>\n",
    "\t<li>Machine Translation</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('industry', 1.0)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aplikasi spell check di textBlob\n",
    "from textblob import Word\n",
    "\n",
    "w = Word('industri')\n",
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('japan', 0.7777777777777778),\n",
       " ('waggon', 0.1111111111111111),\n",
       " ('jagged', 0.07407407407407407),\n",
       " ('pagan', 0.037037037037037035)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Word('jaggan')\n",
    "w.spellcheck()\n",
    "# Kendalanya kalau Bahasa Indonesia ==> perlu pendekatan umum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Norvig Spell Checker: Menggunakan Aturan Probabilitas Bayes&nbsp;</h3>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/2_Bayes_Norig.JPG\" style=\"height:500px; width:919px\" /></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "industri\n",
      "jangan\n"
     ]
    }
   ],
   "source": [
    "# http://norvig.com/spell-correct.html\n",
    "# corpus = 'data/kata_dasar.txt'\n",
    "print(tau.correction('industr1'))\n",
    "print(tau.correction('jang4n'))\n",
    "# prinsip yang sama kelak bisa diterapkan untuk n-grams dengan merubah karakter ==> kata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Language Detection and Translation\n",
    "<p><img alt=\"\" src=\"images/2_translation.jpg\" style=\"height:400px; width:534px\" /></p>\n",
    "\n",
    "\n",
    "<p>[<a href=\"https://www.slideshare.net/PhuLeTruongVinh/ltvp-thesis-defense\" target=\"_blank\">image Source</a>]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "tl\n"
     ]
    }
   ],
   "source": [
    "#Language Detection (TextBlob)\n",
    "from textblob import TextBlob\n",
    "T = \"Aku ingin mengerti NLP dalam bahasa Inggris\"\n",
    "U = \"opo iso mangan sa iki\"\n",
    "print(TextBlob(T).detect_language())\n",
    "print(TextBlob(U).detect_language())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to understand NLP in English. I love you\n",
      "أريد أن أفهم البرمجة اللغوية العصبية باللغة الإنجليزية. انا احبك\n",
      "私は英語でNLPを理解したいです。私はあなたを愛しています\n"
     ]
    }
   ],
   "source": [
    "# Machine Translation (TextBlob)\n",
    "# Butuh koneksi internet, limited calls. Error otherwise. Need \"try\" and \"catch\".\n",
    "T = \"Aku ingin mengerti NLP dalam bahasa Inggris. I love you\"\n",
    "print(TextBlob(T).translate(to='en'))\n",
    "print(TextBlob(T).translate(to='ar-sa'))\n",
    "print(TextBlob(T).translate(to='ja'))\n",
    "# daftar kode bahasa : http://www.cardinalpath.com/resources/tools/google-analytics-language-codes/\n",
    "# Perhatikan TextBlob secara automatis akan mendeteksi bahasa asal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I just want to say: I Love you\n",
      "Aku hanya ingin mengatakan: I Love you\n"
     ]
    }
   ],
   "source": [
    "# Kalau secara spesifik ingin translate dari suatu bahasa ke bahasa lain:\n",
    "T = \"Aku hanya ingin mengatakan : I Love you\"\n",
    "print(TextBlob(T).translate(from_lang ='id', to='en'))\n",
    "print(TextBlob(T).translate(from_lang ='en', to='id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/2_regex_meme.jpg\" style=\"height:397px; width:599px\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Beberapa Reguler Expression yang sering digunakan di NLP/Text Mining</strong></p>\n",
    "\n",
    "<ol>\n",
    "\t<li>Menghilangkan/extract email</li>\n",
    "\t<li>Menghilangkan/extract nomer telephone</li>\n",
    "\t<li>Menghilangkan/extract URL di string.</li>\n",
    "\t<li>Alpha Numeric filtering</li>\n",
    "\t<li>Wild Card Search</li>\n",
    "\t<li>Cleaning hashTags di Media Sosial</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contact kami di  ,  , atau  \n",
      "email yang ditemukan:  ['admin@nlpindonesia.org', 'nlp.indonesia@sci.yahoo.co.id', 'nlp_nusantara@internet.net']\n"
     ]
    }
   ],
   "source": [
    "# Extracting atau replacing eMail.\n",
    "import re\n",
    "emailPattern = re.compile(r'[\\w._%+-]+@[\\w\\.-]+\\.[a-zA-Z]{2,4}')\n",
    "\n",
    "txt = 'Contact kami di admin@nlpindonesia.org, nlp.indonesia@sci.yahoo.co.id, atau nlp_nusantara@internet.net'\n",
    "\n",
    "print( re.sub(emailPattern, ' ', txt) )# clean email\n",
    "eMailS = re.findall( emailPattern, txt )\n",
    "print( 'email yang ditemukan: ', str(eMailS) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/2_email_regex.gif\" style=\"height:403px; width:687px\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contact kami di *** atau *** atau *** atau *** atau +***88\n",
      "Nomer telephone yang ditemukan:  ['021-7634562', '021-763-4562', '021 763 4562', '0822959020', '6281992586']\n"
     ]
    }
   ],
   "source": [
    "# Pola telephone : \\d penanda angka di reguler Expression, \\s spasi, dan \"|\" adalah \"atau\"\n",
    "# \"?\" menyatakan pilihan (optional): colou?r sesuai dengan colour atau color.\n",
    "\n",
    "phonePattern = re.compile(r'(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})')\n",
    "txt = 'Contact kami di 021-7634562 atau 021-763-4562 atau 021 763 4562 atau 0822959020 atau +628199258688'\n",
    "print(re.sub(phonePattern,'***',txt))# clean phone\n",
    "    \n",
    "noTelp = re.findall(phonePattern,txt)\n",
    "print('Nomer telephone yang ditemukan: ',str(noTelp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contact kami di *** atau +***\n",
      "082295203040\n",
      "+6282295203040\n"
     ]
    }
   ],
   "source": [
    "# Pola telephone 2: untuk setiap angka 8-14 digits dipisahkan oleh \"spasi\", \",\" atau \".\"\n",
    "phonePattern = re.compile(r'\\b\\d{8,14}\\b')\n",
    "txt = 'Contact kami di 082295203040 atau +6282295203040'\n",
    "print(re.sub(phonePattern,'***',txt))# clean phone\n",
    "    \n",
    "noTelp = re.findall(phonePattern,txt)\n",
    "\n",
    "for no in noTelp:\n",
    "    if no[0]!='0':\n",
    "        print('+' + no)\n",
    "    else:\n",
    "        print(no)\n",
    "#print('Nomer telephone yang ditemukan: ',str(noTelp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "website reguler expression & my site :   &  \n",
      "URL yang ditemukan:  ['https://www.regular-expressions.info/', 'https://tau-data.id']\n"
     ]
    }
   ],
   "source": [
    "# Website URLS http(s) .... untuk ftp trivial\n",
    "urlPattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "\n",
    "txt = 'website reguler expression & my site : https://www.regular-expressions.info/ & https://tau-data.id'\n",
    "print(re.sub(urlPattern,' ',txt))# clean urls\n",
    "\n",
    "URLs = re.findall(urlPattern,txt)# get URLs\n",
    "print('URL yang ditemukan: ',str(URLs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "time.sleep(5)\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi   Mukidi  apa kabar   sapa_Pagi \n"
     ]
    }
   ],
   "source": [
    "# cleaning non alpha-numeric\n",
    "txt = 'Hi! @Mukidi, apa kabar? #sapa_Pagi.'\n",
    "print(re.sub(r'[^\\w]',' ',txt))\n",
    "# atau jika ingin exclude titik dan koma \n",
    "# re.sub(r'[^.,a-zA-Z0-9 \\n\\.]','',txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Mukidi apa kabar sapaPagi\n"
     ]
    }
   ],
   "source": [
    "# alternative 2:\n",
    "print(''.join([t for t in txt if t.isalnum() or t==' ']))\n",
    "# ada perbedaan?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"\" src=\"images/2_nonStandard_Language.jpg\" style=\"height:359px; width:638px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags = ['AndaiSajaIaTahu', 'ApaYangAkuRasah', 'AlayersTweet', 'd2d']\n"
     ]
    }
   ],
   "source": [
    "# Cleaning hashTags dalam posting media sosial\n",
    "tweet = 'oh IoT, #AndaiSajaIaTahu #ApaYangAkuRasah... #AlayersTweet #d2d'\n",
    "\n",
    "getHashtags = re.compile(r\"#(\\w+)\")\n",
    "print(\"Tags = {0}\".format(re.findall(getHashtags, tweet)))\n",
    "# temukan hanya tags ... perhatikan IoT bukan Tags walau ada huruf besar & kecil dalam satu kata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Andai', 'Saja', 'Ia', 'Tahu']\n",
      "['Apa', 'Yang', 'Aku', 'Rasah']\n",
      "['Alayers', 'Tweet']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "pisahtags = re.compile(r'[A-Z][^A-Z]*')\n",
    "\n",
    "for tags in re.findall(getHashtags, tweet):\n",
    "    print(re.findall(pisahtags, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh IoT, Andai Saja Ia Tahu Apa Yang Aku Rasah... Alayers Tweet\n"
     ]
    }
   ],
   "source": [
    "# Mengganti hashtags dengan kata dasar pembentuknya\n",
    "tweet = 'oh IoT, #AndaiSajaIaTahu #ApaYangAkuRasah... #AlayersTweet'\n",
    "tagS = re.findall(getHashtags, tweet)\n",
    "for tag in tagS:\n",
    "    proper_words = ' '.join(re.findall(pisahtags, tag))\n",
    "    tweet = tweet.replace('#'+tag,proper_words)\n",
    "\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'The25XYZ3abc'\n",
    "re.split('(\\d+)',s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding-Decoding:\n",
    "\n",
    "<ul>\n",
    "\t<li>Hal berikutnya yang perlu diperhatikan dalam memproses data teks adalah encoding-decoding.</li>\n",
    "\t<li>Contoh Encoding: ASCII, utf, latin, dsb.</li>\n",
    "\t<li>saya membahas lebih jauh tetang encoding disini:&nbsp;<br />\n",
    "\t<a href=\"https://taufiksutanto.blogspot.co.id/2018/01/pereda-sakit-kepala-urgensi-memahami.html\" target=\"_blank\">https://taufiksutanto.blogspot.co.id/2018/01/pereda-sakit-kepala-urgensi-memahami.html</a></li>\n",
    "\t<li>Berikut adalah sebuah contoh sederhana tantangan proses encoding-decoding ketika kita hendak memproses data yang berasal dari internet atau media sosial.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dear god, please help me\n"
     ]
    }
   ],
   "source": [
    "# kita bisa menggunakan modul unidecode untuk mendapatkan representasi ASCII terdekat\n",
    "from unidecode import unidecode\n",
    "\n",
    "T = \"ḊḕḀṙ ₲ØĐ, p̾l̾e̾a̾s̾e ḧḕḶṖ ṁḕ\"\n",
    "print(unidecode(T).lower())\n",
    "# Bahasa Indonesia dan Inggris secara umum mampu direpresentasikan dalam encoding ASCII: \n",
    "# https://en.wikipedia.org/wiki/ASCII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Satu < Tiga & © adalah simbol Copyright\n"
     ]
    }
   ],
   "source": [
    "# Kita juga bisa membersihkan posting media sosial/website dengan entitas html menggunakan fungsi \"unescape\" di modul \"html\"\n",
    "from html import unescape\n",
    "\n",
    "print(unescape('Satu &lt; Tiga&nbsp;&amp; &#169; adalah simbol Copyright'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pungsi(x,y):\n",
    "    z=x**y\n",
    "    return z\n",
    "\n",
    "pungsi(2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Latihan 1</strong>:</p>\n",
    "\n",
    "<p>Diberikan&nbsp;tweet berikut:<br />\n",
    "<strong>tweet&nbsp;</strong>=&nbsp; &quot;<em><strong>The #OctopiPower is &amp;gt; Sharks! &amp;amp; they&#39;re awsm! So happy to see them here <a href=\"http://www.octopusVSshark.com\" target=\"_blank\">http://www.octopusVSshark.com</a> !</strong></em>&quot;</p>\n",
    "\n",
    "<p>preprocess tweet diatas sehingga didapatkan tweet seperti ini (Gunakan sembarang modul yang mendukung):<br />\n",
    "<strong>tweet</strong>= &quot;<em><strong>the octopus power is &gt; shark ! &amp; they are awesome ! so happy to see them here !</strong></em>&quot;</p>\n",
    "\n",
    "<p><u><strong>Petunjuk/Hints</strong></u>:</p>\n",
    "\n",
    "<ol>\n",
    "\t<li>Buat satu atau lebih fungsi untuk memudahkan, misal fungsi <em>fixTags </em>dan <em>cleanText</em>.</li>\n",
    "\t<li>fix kata &quot;<em>they&#39;re</em>&quot; dan <em>awsm </em>dengan teknik sederhana <em>dictionary fix</em>.</li>\n",
    "\t<li>Hati-hati terhadap <u><strong>urutan </strong></u>aksi di preprocessing karena akan mengakibatkan hasil yang berbeda.<br />\n",
    "\tKelak di segmen berikutnya urutan aksi ini akan disebut sebagai &quot;<big><strong>Pipelining</strong></big>&quot;.</li>\n",
    "\t<li>Code solusi latihan ini dengan dasar fikiran bahwa solusinya nanti akan bisa digunakan untuk sembarang preprocessing.</li>\n",
    "\t<li>At this point, jangan hawatirkan dulu scalability/efisiensi.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# Coba jawaban Latihan [1] di cell ini: \n",
    "# Salah satu contoh jawaban latihan ini ada di cell paling bawah. \n",
    "# Namun coba untuk tidak melihat jawaban tersebut\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Latihan 2</strong>:</p>\n",
    "\n",
    "<p>Bagaimana caranya memfilter kata-kata (token) yang terdiri dari huruf dan angka (misal <em>b29nf, _24x_,&nbsp;</em>dsb)?</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# Coba jawaban Latihan [2] di cell ini: \n",
    "# Salah satu contoh jawaban latihan ini ada di cell paling bawah. \n",
    "# Namun coba untuk tidak melihat jawaban tersebut\n",
    "T = 'pesawat b29 dan mig276 adalah kepunyaan fhg347x dan _24x_'\n",
    "# Harapan jawaban T = 'pesawat dan adalah kepunyaan dan'\n",
    "# Petunjuk: gunakan property \"isalpha\" pada variabel string di Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color=\"blue\">Outline Representasi Dokumen :</font>\n",
    "* Loading documents (text data): pdf, json, csv, xls, url, doc, Datasets Modul\n",
    "* Representasi Sparse (VSM): Binary, tf &amp;-/ idf, Custom tf-idf, BM25\n",
    "* Frequency filtering, n-grams, vocabulary based\n",
    "* Representasi Dense:&nbsp;Word Embedding (Word2Vec dan FastText)\n",
    "* Tensor to Matrix representation untuk model Machine Learning di Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Documents (Text Data)\n",
    "<p><img alt=\"\" src=\"images/2_importing_Files.jpg\" style=\"height:400px; width:510px\" /><br />\n",
    "[<a href=\"http://www.clearcounsel.com/what-happens-to-your-digital-property-after-you-die/\" target=\"_blank\"><strong>Image Source</strong></a>]</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>me-load file teks dapat dilakukan dengan berbagai cara, bergantung format dokumen. Misal:</p>\n",
    "\n",
    "<div style=\"background:#eee; border:1px solid #ccc; padding:5px 10px\"><strong>PDF(file).string</strong> # file pdf menggunakan modul <strong>pattern</strong></div>\n",
    "\n",
    "<div style=\"background:#eee; border:1px solid #ccc; padding:5px 10px\"><strong>docx2txt.process(file)</strong> # file docx emnggunakan modul <strong>docx2txt</strong></div>\n",
    "\n",
    "<div style=\"background:#eee; border:1px solid #ccc; padding:5px 10px\"><strong>open(file,&quot;r&quot;,encoding=&quot;utf-8&quot;, errors=&#39;replace&#39;) </strong># text file dengan Python &quot;<strong>open</strong>&quot;</div>\n",
    "\n",
    "<div style=\"background:#eee; border:1px solid #ccc; padding:5px 10px\"><strong>bz2(file, &quot;r&quot;)</strong> # untuk file text terkompresi menggunakan modul <strong>BZ2</strong></div>\n",
    "\n",
    "<p>Catatan: encoder/parser diatas memiliki keterbatasan masing-masing.</p>\n",
    "\n",
    "<p>Berikut ini contoh import beberapa file dokumen sekaligus dari folder .&#39;<em><strong>data</strong></em>&#39;</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gunakan Fungsi LoadDocuments untuk \n",
    "import taudata as tau\n",
    "\n",
    "dPath = 'data/'\n",
    "Docs, Files = tau.LoadDocuments(dPath,types=['txt','bz2','csv']) \n",
    "# csv is supported but imported as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/dataTweet_plain.txt', 'data/kata_dasar.txt', 'data/Rekomendasi_Data.txt', 'data/Shakespeare_Hamlet.txt', 'data/stopwords_eng.txt', 'data/stopwords_id.txt', 'data/Zen_of_Python.txt.bz2', 'data/test.csv']\n"
     ]
    }
   ],
   "source": [
    "print(Files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['priyobudis aman berdemokrasi mohon ustadz relawan prabowo sandiuno bantu menginsyafkan polisi2 yg diperalat utk ..\\n', 'aldi_lho ngobrol mbah konservatif sayap kanan kampung ubah halauan gara gara gaya isi debat kemarin ..\\n', 'mas_brewoks debat capres sdh saksikan kira2 yg unggul dlm debat td malam 01 jokowi    ..\\n', 'buana_aj bangpino 1030muh prabowo jokowi sarkasme wkwkwkwk\\n', 'negativisme dengar jokowi ngantuk tapi dengar prabowo semangat soekarno\\n', 'mahendradatta debat capres hasil google trends kejut 62 warga net cari telusur prabowo marah2 debat capres audience    data prabowo slh yenny wahid telusur jokowi kait prestasi langkah konkret program dilan ..\\n', 'liem_id kampanye gak seramai tukang jamu .. dibandingkan dg kampanye prabowo sandi prabowo sapa jatim ..\\n', 'marierteman maaf prabowo jokowi tapi jokowi tidak prabowo\\n', 'marierteman maaf prabowo jokowi tapi jokowi tidak prabowo\\n', 'w_runturambi jokowi baca komik shinchan tdk    kpk swot strength weakness opportunity threat ..\\n', 'indonesia pilih pimpin yg gerak cekat indonesia maju jokowi lihat digitalisasi cepat layan prabowo alat mahal ga coblos 01 jokowi amin jokowi maju prabowo mundur\\n', 'saididu makassar beda rakyatbergerak dg rakyat digerakkan foto    rakyat kampanye prabowo foto    ra ..\\n', 'kumpar dpw pan maluku kecewa tidak dilibatkan rapat koordinasi menang prabowo sandi publisherstory https ..\\n', 'fahrihamzah prabowo jokowi bapaknya banyumas tpi cina calls jokowi amin\\n', 'whyhidayat jokowi    yg sederhana rakyat prabowo    yg wibawa     yg sabar setia tunggu ..\\n', 'anonlokal gerindra prabowo jokowi ..\\n', 'aksioma_02 akta_id bawaslu_ri divhumas_polri prabowo jokowi ide yg bagus tapi daerah batas aja kayak dikalbar papua nugini .. biar pensiun disana save polri from politics\\n', 'pollinglagi perang tagar netizen avs vs ivs    polling aja pilpres ..\\n', 'priyobudis aman berdemokrasi mohon ustadz relawan prabowo sandiuno bantu menginsyafkan polisi2 yg diperalat utk ..\\n', 'fadelholic yth prabowo titip harap suara hadap jokowi kondisi desa .. tapi ..\\n', 'teddygusnaidi utk uji nyali prabowo berani gak prabowo nyata libas juang hizbut tahrir ..\\n', 'cnnindonesia gerindra klaim prabowo kuliahi jokowi debat keempat debat pilpres 2019 debat keempat pilpres 2019 ..\\n', 'mahendradatta debat capres hasil google trends kejut 62 warga net cari telusur prabowo marah2 debat capres audience    data prabowo slh yenny wahid telusur jokowi kait prestasi langkah konkret program dilan ..\\n', 'mahendradatta debat capres hasil google trends kejut 62 warga net cari telusur prabowo marah2 debat capres audience    data prabowo slh yenny wahid telusur jokowi kait prestasi langkah konkret program dilan ..\\n', 'tempoinstitute    prabowo jokowi capres tampil debatpilpres 2019 malam pemilunesia debatpilpres 2019\\n', '\\n', 'mahendradatta debat capres hasil google trends kejut 62 warga net cari telusur prabowo marah2 debat capres audience    data prabowo slh yenny wahid telusur jokowi kait prestasi langkah konkret program dilan ..\\n', 'gm_gm prabowo kekhalifahan jokowi kabar buruk hti dukung ..\\n', 'kurawa simpul debat keempat pilpres 2019 malam     jokowi presiden prabowo jenderal briefing .. pas ..\\n', 'aryprasetyo_85 prabowo jokowi fagtng gra2 tecnologi jadul\\n', 'afutami gue bilang lawan bohong periksa fakta tapi kontra narasi jokowi lakukan ..\\n', 'andiarief mega seneng ubah pidato tangis aceh bbm berluasa operasi militer aceh da ..\\n', 'slthnrafi prabowo jokowi arguing without crying\\n', 'pollinglagi perang tagar netizen avs vs ivs    polling aja pilpres ..\\n', 'jokowi presiden ri capres duduk prabowo     ngapain takut pecat \\n', 'nataliuspigai2 palembang dihadiri prabowo sandi massa tumpah ruah jalan raya pembang tanda kemenan ..\\n', 'iraskaminsky adab manusia sifat kontinuitas dijalankan kembang teknologi prabowo subianto putus rant ..\\n', 'marierteman maaf prabowo jokowi tapi jokowi tidak prabowo\\n', 'prabowo tni tni tapi percaya tni coblos 01 jokowi amin jokowi maju prabowo mundur indonesia pilih pimpin yg percaya dgn bangsanya indonesia maju\\n', 'jokowi maju prabowo mundur fyi prabowo buka negara tukar ilmu jokowi buka dgn negara tukar ilmu coblos 01 jokowi amin bocahsosmed sukangetweet\\n', 'slthnrafi prabowo jokowi arguing without crying\\n', 'christwamea debat semlm hendro    luhut dengar kata2 tulus bpk jokowi bhw beliau sgt prabowo subianto adlh ..\\n', 'andiarief mega seneng ubah pidato tangis aceh bbm berluasa operasi militer aceh da ..\\n', 'nataliuspigai2 palembang dihadiri prabowo sandi massa tumpah ruah jalan raya pembang tanda kemenan ..\\n', 'jokowi vs prabowo jlas sdh calon presiden trus kpn jdi presiden rumah tangga ..\\n', 'greschinov prabowo ngomong gebu gebu arti kasar galak jokowi ngomong pelan arti lembe ..\\n', 'kandargalang ooh .. nyata bpk yg referensi jokowi .. .. dlm 20 thn kedepan indonesia aman2 tdk ..\\n', 'priyobudis aman berdemokrasi mohon ustadz relawan prabowo sandiuno bantu menginsyafkan polisi2 yg diperalat utk ..\\n', 'mahendradatta jalan sehat palembang dukung jokowi gak dukung prabowo sandi\\n', 'tempoinstitute    prabowo jokowi capres tampil debatpilpres 2019 malam pemilunesia debatpilpres 2019\\n', 'andiarief mega seneng ubah pidato tangis aceh bbm berluasa operasi militer aceh da ..\\n', 'raja_polling menang debat pilpres layak presiden nkri debat capres 2019    jokowi    prabowo    share da ..\\n', 'andiarief mega seneng ubah pidato tangis aceh bbm berluasa operasi militer aceh da ..\\n', 'marierteman maaf prabowo jokowi tapi jokowi tidak prabowo\\n', 'uusbiasaaja untung aja pemilu nya jokowi vs prabowo coba klo prabowo vs amin rais lu jg golput us hehe\\n', 'whyhidayat jokowi    yg sederhana rakyat prabowo    yg wibawa     yg sabar setia tunggu ..\\n', 'marierteman numpang izin tawa yaa .. jokowi jaga utuh keluarganya prabowo .. isi\\n', 'sahabat prabowo jokowi hangat penuh tawa debat capres 2019\\n', 'slthnrafi prabowo jokowi arguing without crying\\n', 'mahendradatta jalan sehat palembang dukung jokowi gak dukung prabowo sandi\\n', 'ernijasin closing jokowi sangat indah sentuh hati .. prabowo sepeda rantainya putus ..\\n', 'ngobrol mbah konservatif sayap kanan kampung ubah halauan gara gara gaya isi debat kemarin pro swing voter prabowo jokowi nyata efek debat capres apatis hasil debat capres2an gini\\n', 'marierteman maaf prabowo jokowi tapi jokowi tidak prabowo\\n', 'priyobudis aman berdemokrasi mohon ustadz relawan prabowo sandiuno bantu menginsyafkan polisi2 yg diperalat utk ..\\n', 'aldi_lho ngobrol mbah konservatif sayap kanan kampung ubah halauan gara gara gaya isi debat kemarin ..\\n', 'mas_brewoks debat capres sdh saksikan kira2 yg unggul dlm debat td malam 01 jokowi    ..\\n', 'buana_aj bangpino 1030muh prabowo jokowi sarkasme wkwkwkwk\\n', 'negativisme dengar jokowi ngantuk tapi dengar prabowo semangat soekarno\\n', 'mahendradatta debat capres hasil google trends kejut 62 warga net cari telusur prabowo marah2 debat capres audience    data prabowo slh yenny wahid telusur jokowi kait prestasi langkah konkret program dilan ..\\n', 'liem_id kampanye gak seramai tukang jamu .. dibandingkan dg kampanye prabowo sandi prabowo sapa jatim ..\\n', 'marierteman maaf prabowo jokowi tapi jokowi tidak prabowo\\n', 'marierteman maaf prabowo jokowi tapi jokowi tidak prabowo\\n', 'w_runturambi jokowi baca komik shinchan tdk    kpk swot strength weakness opportunity threat ..\\n', 'indonesia pilih pimpin yg gerak cekat indonesia maju jokowi lihat digitalisasi cepat layan prabowo alat mahal ga coblos 01 jokowi amin jokowi maju prabowo mundur\\n', 'saididu makassar beda rakyatbergerak dg rakyat digerakkan foto    rakyat kampanye prabowo foto    ra ..\\n', 'kumpar dpw pan maluku kecewa tidak dilibatkan rapat koordinasi menang prabowo sandi publisherstory https ..\\n', 'fahrihamzah prabowo jokowi bapaknya banyumas tpi cina calls jokowi amin\\n', 'whyhidayat jokowi    yg sederhana rakyat prabowo    yg wibawa     yg sabar setia tunggu ..\\n', 'anonlokal gerindra prabowo jokowi ..\\n', 'aksioma_02 akta_id bawaslu_ri divhumas_polri prabowo jokowi ide yg bagus tapi daerah batas aja kayak dikalbar papua nugini .. biar pensiun disana save polri from politics\\n', 'pollinglagi perang tagar netizen avs vs ivs    polling aja pilpres ..\\n', 'priyobudis aman berdemokrasi mohon ustadz relawan prabowo sandiuno bantu menginsyafkan polisi2 yg diperalat utk ..\\n', 'fadelholic yth prabowo titip harap suara hadap jokowi kondisi desa .. tapi ..\\n', 'teddygusnaidi utk uji nyali prabowo berani gak prabowo nyata libas juang hizbut tahrir ..\\n', 'cnnindonesia gerindra klaim prabowo kuliahi jokowi debat keempat debat pilpres 2019 debat keempat pilpres 2019 ..\\n', 'mahendradatta debat capres hasil google trends kejut 62 warga net cari telusur prabowo marah2 debat capres audience    data prabowo slh yenny wahid telusur jokowi kait prestasi langkah konkret program dilan ..\\n', 'mahendradatta debat capres hasil google trends kejut 62 warga net cari telusur prabowo marah2 debat capres audience    data prabowo slh yenny wahid telusur jokowi kait prestasi langkah konkret program dilan ..\\n', 'tempoinstitute    prabowo jokowi capres tampil debatpilpres 2019 malam pemilunesia debatpilpres 2019\\n', '\\n', 'mahendradatta debat capres hasil google trends kejut 62 warga net cari telusur prabowo marah2 debat capres audience    data prabowo slh yenny wahid telusur jokowi kait prestasi langkah konkret program dilan ..\\n', 'gm_gm prabowo kekhalifahan jokowi kabar buruk hti dukung ..\\n', 'kurawa simpul debat keempat pilpres 2019 malam     jokowi presiden prabowo jenderal briefing .. pas ..\\n', 'aryprasetyo_85 prabowo jokowi fagtng gra2 tecnologi jadul\\n', 'afutami gue bilang lawan bohong periksa fakta tapi kontra narasi jokowi lakukan ..\\n', 'andiarief mega seneng ubah pidato tangis aceh bbm berluasa operasi militer aceh da ..\\n', 'slthnrafi prabowo jokowi arguing without crying\\n', 'pollinglagi perang tagar netizen avs vs ivs    polling aja pilpres ..\\n', 'jokowi presiden ri capres duduk prabowo     ngapain takut pecat \\n', 'nataliuspigai2 palembang dihadiri prabowo sandi massa tumpah ruah jalan raya pembang tanda kemenan ..\\n', 'iraskaminsky adab manusia sifat kontinuitas dijalankan kembang teknologi prabowo subianto putus rant ..\\n', 'marierteman maaf prabowo jokowi tapi jokowi tidak prabowo\\n', 'prabowo tni tni tapi percaya tni coblos 01 jokowi amin jokowi maju prabowo mundur indonesia pilih pimpin yg percaya dgn bangsanya indonesia maju\\n', 'jokowi maju prabowo mundur fyi prabowo buka negara tukar ilmu jokowi buka dgn negara tukar ilmu coblos 01 jokowi amin bocahsosmed sukangetweet\\n', 'slthnrafi prabowo jokowi arguing without crying\\n', 'christwamea debat semlm hendro    luhut dengar kata2 tulus bpk jokowi bhw beliau sgt prabowo subianto adlh ..\\n', 'andiarief mega seneng ubah pidato tangis aceh bbm berluasa operasi militer aceh da ..\\n', 'nataliuspigai2 palembang dihadiri prabowo sandi massa tumpah ruah jalan raya pembang tanda kemenan ..\\n', 'jokowi vs prabowo jlas sdh calon presiden trus kpn jdi presiden rumah tangga ..\\n', 'greschinov prabowo ngomong gebu gebu arti kasar galak jokowi ngomong pelan arti lembe ..\\n', 'kandargalang ooh .. nyata bpk yg referensi jokowi .. .. dlm 20 thn kedepan indonesia aman2 tdk ..\\n', 'priyobudis aman berdemokrasi mohon ustadz relawan prabowo sandiuno bantu menginsyafkan polisi2 yg diperalat utk ..\\n', 'mahendradatta jalan sehat palembang dukung jokowi gak dukung prabowo sandi\\n', 'tempoinstitute    prabowo jokowi capres tampil debatpilpres 2019 malam pemilunesia debatpilpres 2019\\n', 'andiarief mega seneng ubah pidato tangis aceh bbm berluasa operasi militer aceh da ..\\n', 'raja_polling menang debat pilpres layak presiden nkri debat capres 2019    jokowi    prabowo    share da ..\\n', 'andiarief mega seneng ubah pidato tangis aceh bbm berluasa operasi militer aceh da ..\\n', 'marierteman maaf prabowo jokowi tapi jokowi tidak prabowo\\n', 'uusbiasaaja untung aja pemilu nya jokowi vs prabowo coba klo prabowo vs amin rais lu jg golput us hehe\\n', 'whyhidayat jokowi    yg sederhana rakyat prabowo    yg wibawa     yg sabar setia tunggu ..\\n', 'marierteman numpang izin tawa yaa .. jokowi jaga utuh keluarganya prabowo .. isi\\n', 'sahabat prabowo jokowi hangat penuh tawa debat capres 2019\\n', 'slthnrafi prabowo jokowi arguing without crying\\n', 'mahendradatta jalan sehat palembang dukung jokowi gak dukung prabowo sandi\\n', 'ernijasin closing jokowi sangat indah sentuh hati .. prabowo sepeda rantainya putus ..\\n', 'ngobrol mbah konservatif sayap kanan kampung ubah halauan gara gara gaya isi debat kemarin pro swing voter prabowo jokowi nyata efek debat capres apatis hasil debat capres2an gini\\n', 'marierteman maaf prabowo jokowi tapi jokowi tidak prabowo\\n', 'banding dukung jokowi prabowo .. jokowi 01 indonesia maju\\n', 'indonesia pilih pimpin yg percaya dgn bangsanya indonesia maju mundur prabowo khawatir negara serbu indonesia jaman hitler coblos 01 jokowi amin jokowi maju prabowo mundur\\n', 'jatamnas pulau tambang jaya tani nelayan ruang hidupnya ancam parahnya jokowi pra ..\\n', 'iwankamah the difference between indonesia old fashioned presidential candidate prabowo subianto and incumbent visionary new technol ..\\n', 'nephilaxmus catat 107641 tweet tagar debat pilpres 2019 jokowi 18189 kali prabowo ..\\n', 'kurawa jokowi prabowo deklarasi kalo sahabat apapun hasil pilpres gue usul derby gak ..\\n', 'matanajwa    jokowi prabowo bumnbersatu kali mediator mengak ..\\n', 'ernijasin closing jokowi sangat indah sentuh hati .. prabowo sepeda rantainya putus ..\\n', 'nataliuspigai2 palembang dihadiri prabowo sandi massa tumpah ruah jalan raya pembang tanda kemenan ..\\n', 'hasil polling debat capres prabowo unggul 65 persen jokowi 35 persen\\n', 'candra_aditya    basi madingnya udah terbit    iconic kalimat mulut jokowi prabowo sandiaga uno ..\\n', 'jokowi nilai lbih pham tni ktimbang prabowo jokowi mliki visi kmitmen utk mnguatkn tni utk mnghadapi perang tknologi perang siber msa dpan smntara prabowo cnderung tdk prcya tknologi tdk pham intelijen strtegis tdk pham mnimum essentisl force\\n', 'desakrinjani jokowi sangat percaya tni negara prabowo pesimis tni negara coblos 01 jokowi amin baju putih jokow ..\\n', 'laskar_minang vintenas jokowi pore bangga sbg negara tapi invest mana2 invest pore kapasitas negeri yg sangat prabowo yg pimpin nkri pore happy gampang diajak cincai olah2 galak ..\\n', 'greschinov prabowo ngomong gebu gebu arti kasar galak jokowi ngomong pelan arti lembe ..\\n', 'desakrinjani prabowo khawatir negara serbu indonesia jaman hitler hahhaha coblos 01 jokowi amin jokowi maju ..\\n', 'desakrinjani prabowo tni tni tapi percaya tni gimana sih otaknya si wowo coblos 01 jokowi amin jokowi ..\\n', 'desakrinjani prabowo bilang aman indonesia lemah jokowi maju prabowo mundur faktanya data global firepower indonesia masuk ..\\n', 'desakrinjani jokowi maju prabowo mundur debat iv disimpulkan jokowi lihat kedepan prabowo lihat coblo ..\\n', 'gregjamesbarton    the longer the debate ran the darker prabowo vision grew on saturday night indonesia president emerged as more ..\\n', 'pollinglagi the right one ma ruf pilih jokowi pilih     netizen pilih prabowo pilih sandi saj ..\\n', 'timurkota argumen dipikir dab disitu kalimat bilang dilibas jokowi ya berati sdh dilibas alias ..\\n', 'andiarief mega seneng ubah pidato tangis aceh bbm berluasa operasi militer aceh da ..\\n', 'marierteman maaf prabowo jokowi tapi jokowi tidak prabowo\\n', 'andiarief mega seneng ubah pidato tangis aceh bbm berluasa operasi militer aceh da ..\\n', 'marierteman maaf prabowo jokowi tapi jokowi tidak prabowo\\n', 'ihwanbudiawan z_ghazali ganjarpranowo hhmm .. iyain aja deh pokoknya prabowo always right jokowi slalu salah\\n', 'desakrinjani jokowi maju prabowo mundur jokowi lihat prabowo khawatir negara laku investasi usaha buk ..\\n', 'desakrinjani prabowo ngasih contoh jaman ubah maju coblos 01 jokowi amin jokowi ma ..\\n', 'fadelholic yth prabowo titip harap suara hadap jokowi kondisi desa .. tapi ..\\n', 'bacotanirfaedah game jokowi game prabowo debat pilpres 2019 debat keempat pilpres 2019\\n', 'priyobudis aman berdemokrasi mohon ustadz relawan prabowo sandiuno bantu menginsyafkan polisi2 yg diperalat utk ..\\n', 'mas_brewoks jokowi gua terima uangnya trus imbal gua jejek2 foto prabowo bilik suara\\n', 'raja_polling jonrugintingnew jokowi prabowo kucingkampung smacanasia\\n', 'slthnrafi prabowo jokowi arguing without crying\\n', 'andiarief mega seneng ubah pidato tangis aceh bbm berluasa operasi militer aceh da ..\\n', 'bintangku206 saksi debat malam pilih rakyat yg dipilih ..\\n', 'argumen dipikir dab disitu kalimat bilang dilibas jokowi ya berati sdh dilibas alias bubar knp prabowo libas pakar adu domba koe dab \\n', 'jonrugintingnew debat malam bukti    jokowi prabowo emosional prabowo emosi pikir selamat ..\\n', 'bacotanirfaedah game jokowi game prabowo debat pilpres 2019 debat keempat pilpres 2019\\n', 'cktbng ribut ribut pamer golput jokowi prabowo golong tapi gak kasih tau layak pilih sjw \\n', 'jokowi maju prabowo mundur prabowo buka negara tukar ilmu jokowi buka dgn negara tukar ilmu coblos 01 jokowi amin\\n', 'prestasi om dosa hashim dosa prabowo bawa ahok dosa bawa jokowi\\n', 'cnnindonesia survei jokowi tenang prabowo emosional\\n', 'fahrihamzah efek government yg sukses petahana .. debat malam .. jokowi prabowo ..\\n', 'murny7 tni .. tapi pecat tapi pecat .. hahaa .. yg debat semalem tni rendahkan prabowo .. ..\\n', 'kalo presiden yg cinta rakyatnya maaf tdk cinta .. tp desyk 2019    besok senin debat tongkrong jokowi prabowo receh recehkantwiter\\n', 'slthnrafi prabowo jokowi arguing without crying\\n', 'mahendradatta debat capres hasil google trends kejut 62 warga net cari telusur prabowo marah2 debat capres audience    data prabowo slh yenny wahid telusur jokowi kait prestasi langkah konkret program dilan digital layan\\n', 'fadlizon yg jokowi government procurement dll th gagal total tuh prabowo ..\\n', 'marierteman maaf prabowo jokowi tapi jokowi tidak prabowo\\n', 'raja_polling jokowi prabowo prabowosandi\\n', 'tweetmiliter mimin prabowo vs jokowi yg nyata ttg hubung internasional tahan militer\\n', 'marierteman maaf prabowo jokowi tapi jokowi tidak prabowo\\n', 'liputan6dotcom calon presiden jokowi prabowo subianto radu gagas debat keempat pilpres 2019 debat pilpres 2019 ..\\n', 'jokowi coba prabowo hidup coba   \\n', 'nataliuspigai2 palembang dihadiri prabowo sandi massa tumpah ruah jalan raya pembang tanda kemenan ..\\n', 'marierteman maaf prabowo jokowi tapi jokowi tidak prabowo\\n', 'marierteman maaf prabowo jokowi tapi jokowi tidak prabowo\\n', 'zulham_setiadi yg batal konsesi jict jokowi prabowo save save our indonesia ports ..\\n', 'marierteman maaf prabowo jokowi tapi jokowi tidak prabowo\\n', 'nataliuspigai2 palembang dihadiri prabowo sandi massa tumpah ruah jalan raya pembang tanda kemenan ..\\n', 'slthnrafi prabowo jokowi arguing without crying\\n', 'teddygusnaidi prabowo anak buah jokowi lapor abs jokowi faktanya data prabo ..\\n', 'parodi_negeri yg cerdas cerdas yg cantik cantik pilih prabowo    bukti .. secantik iblis pilih jokowi hahaha\\n']"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Contoh mengakses isi file\n",
    "print(Docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Becomes the field, but here shows much amiss.\\n', 'Go, bid the soldiers shoot.\\n', 'A dead march. Exeunt, bearing off the dead bodies; after which a peal of ordnance is shot off']\n"
     ]
    }
   ],
   "source": [
    "# Fungsinya juga bisa digunakan untuk me-load satu dokumen saja\n",
    "Doc = tau.LoadDocuments(file = 'data/Shakespeare_Hamlet.txt')[0]\n",
    "print(Doc[-3:]) \n",
    "# Satu baris, satu elemen pada list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Token = 4656\n",
      "contoh Token & frekuensinya:[('the', 1138), ('and', 965), ('to', 737), ('of', 669), ('i', 640)]\n"
     ]
    }
   ],
   "source": [
    "# Menghitung distribusi frekuensi di dokumen hamlet\n",
    "kata, frekuensi = tau.countWords(Doc) # frekuensi terurut descending\n",
    "print('Total Token = {0}'.format(len(kata)))\n",
    "print('contoh Token & frekuensinya:{0}'.format([(k,f) for k,f in zip(kata,frekuensi)][:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Bar Chart frekuensi 100 Tokens pertama\n",
    "tau.barChart(kata,frekuensi,N=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Modul Python lain untuk Parsing Document: &quot;<big>TextRact</big>&quot;</strong></p>\n",
    "\n",
    "<p>* <a href=\"https://textract.readthedocs.io/en/stable/&amp;nbsp\" target=\"_blank\">https://textract.readthedocs.io/</a> <img alt=\"\" src=\"images/2_textract.png\" style=\"height: 239px; width: 600px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data teks dari halaman website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ITTC Universitas Ahmad Dahlan – Pelatihan IT untuk Kalangan Profesional dan Akademisi\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\twindow._wpemojiSettings = {\"baseUrl\":\"https:\\/\\/s.w.org\\/images\\/core\\/emoji\\/11\\/72x72\\/\",\"ext\":\".png\",\"svgUrl\":\"https:\\/\\/s.w.org\\/images\\/core\\/emoji\\/11\\/svg\\/\",\"svgExt\":\".svg\",\"source\":{\"concatemoji\":\"http:\\/\\/ittc.uad.ac.id\\/wp-includes\\/js\\/wp-emoji-release.min.js?ver=5.0.3\"}};\n",
      "\t\t\t!function(a,b,c){function d(a,b){var c=String.fromCharCode;l.clearRect(0,0,k.width,k.height),l.fillText(c.apply(this,a),0,0);var d=k.toDataURL();l.clearRect(0,0,k.width,k.height),l.fillText(c.apply(this,b),0,0);var e=k.toDataURL();return d===e}function e(a){var b;if(!l||!l.fillText)return!1;switch(l.textBaseline=\"top\",l.font=\"600 32px Arial\",a){case\"flag\":return!(b=d([55356,56826,55356,56819],[55356,56826,8203,55356,56819]))&&(b=d([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]),!b);case\"emoji\":return b=d([55358,56760,9792,65039],[55358,56760,8203,9792,65039]),!b}return!1}function f(a){var c=b.createElement(\"script\");c.src=a,c.defer=c.type=\"text/javascript\",b.getElementsByTagName(\"head\")[0].appendChild(c)}var g,h,i,j,k=b.createElement(\"canvas\"),l=k.getContext&&k.getContext(\"2d\");for(j=Array(\"flag\",\"emoji\"),c.supports={everything:!0,everythingExceptFlag:!0},i=0;i<j.length;i++)c.supports[j[i]]=e(j[i]),c.supports.everything=c.supports.everything&&c.supports[j[i]],\"flag\"!==j[i]&&(c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&c.supports[j[i]]);c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&!c.supports.flag,c.DOMReady=!1,c.readyCallback=function(){c.DOMReady=!0},c.supports.everything||(h=function(){c.readyCallback()},b.addEventListener?(b.addEventListener(\"DOMContentLoaded\",h,!1),a.addEventListener(\"load\",h,!1)):(a.attachEvent(\"onload\",h),b.attachEvent(\"onreadystatechange\",function(){\"complete\"===b.readyState&&c.readyCallback()})),g=c.source||{},g.concatemoji?f(g.concatemoji):g.wpemoji&&g.twemoji&&(f(g.twemoji),f(g.wpemoji)))}(window,document,window._wpemojiSettings);\n",
      "\t\t\n",
      "\n",
      "img.wp-smiley,\n",
      "img.emoji {\n",
      "\tdisplay: inline !important;\n",
      "\tborder: none !important;\n",
      "\tbox-shadow: none !important;\n",
      "\theight: 1em !important;\n",
      "\twidth: 1em !important;\n",
      "\tmargin: 0 .07em !important;\n",
      "\tvertical-align: -0.1em !important;\n",
      "\tbackground: none !important;\n",
      "\tpadding: 0 !important;\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Melompat ke konten\n",
      "Loncat ke menu utama\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ITTC Universitas Ahmad Dahlan\n",
      "\n",
      "Pelatihan IT untuk Kalangan Profesional dan Akademisi\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Menu\n",
      "Menu ponsel toggle\n",
      "\n",
      "Beranda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Informatics Doctoral Bootcamp 2019\n",
      "\n",
      "Dikirim pada  26/02/2019 oleh  ittc \n",
      "—\n",
      "Tidak ada komentar ↓ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Indonesia\n",
      "Kekurangan Doktor Bidang Informatika/Komputer\n",
      "Sampai\n",
      "saat ini Indonesia masih krisis jumlah Doktor terutama di bidang\n",
      "Informatika/Ilmu Komputer. Untuk itu Pemerintah melalui Kemristekdikti sudah\n",
      "mencanangkan untuk menghasilkan 20.000 Doktor pada 2019. Salah satu faktor\n",
      "utama lambannya pertumbuhan Doktor di bidang Informatika/Komputer di Indonesia\n",
      "adalah karena lemahnya kompetensi dan keahlian melakukan penelitian. Kelemahan\n",
      "itu mulai dari sisi proposal, metodologi, hingga publikasi ilmiah. Ini terlihat\n",
      "di beberapa PTN menilai hampir 80% di antara pelamar proposal S3 nya dianggap tidak\n",
      "layak. Setelah ditelisik, ternyata kebanyakan proposal yang dibuat kurang dari\n",
      "satu minggu. Alasannya beragam, karena hendak mengejar tenggat waktu, hingga\n",
      "yang sekedar coba-coba. Selain itu proposal yang disiapkan masih\n",
      "sangat umum, cenderung aplikatif setara S1 dan belum fokus pada kebaruan (novelty) yang memang menjadi syarat\n",
      "wajib penelitian S3.\n",
      "Usulan\n",
      "penelitian S3 yang baik dan memiliki peluang besar untuk diterima mesti\n",
      "memenuhi berbagai kriteria penting. Beberapa kriteria utama tersebut di\n",
      "antaranya adalah kajian penelitian terkait, state\n",
      "of the art method, perumusan research\n",
      "problem, research objective dan research questions hingga research gap/novelty. Semua itu tidak\n",
      "bisa hanya ditulis dalam beberapa hari saja, butuh persiapan yang memadai dan\n",
      "sumber-sumber pustaka (jurnal terbaru) primer yang perlu dibaca, dikaji secara\n",
      "seksama dan mendalam. \n",
      "Melihat beban kerja dosen terutama di aspek\n",
      "pengajaran yang cukup besar, maka tidak mengherankan persiapan membuat proposal\n",
      "S3 menjadi terbengkalai. Proposal disiapkan di tengah waktu luang yang tersisa.\n",
      "Untuk itu, guna membantu dan memudahkan para dosen yang bertekad untuk segera\n",
      "S3, maka diselenggarakanlah kegiatan Data Science Doctoral Bootcamp 2019. ITTC UAD\n",
      "menyelenggarakan Doctoral Bootcamp ini untuk membekali para dosen yang berminat\n",
      "melanjutkan studi S3 dengan pengetahuan, pemahaman dan keterampilan komplit\n",
      "untuk bagaimana membuat proposal penelitian S3 yang baik, bermutu dan memikat\n",
      "calon Promotor.\n",
      "Mengikuti Data Science Doctoral Bootcamp ini adalah salah satu investasi sekali seumur hidup yang bakal membantu Anda mempersiapkan proposal S3 dan proposal untuk ujian komprehensif yang 90% diterima. Sehingga diharapkan kelak Anda mampu menyelesaikan S3 dengan tepat waktu. Selain itu bekal pengetahuan dan praktek dari bootcamp ini akan memudahkan Anda dalam membuat proposal-proposal penelitian bergengsi dari hibah dalam dan luar negeri.\n",
      "BENTUK WORKSHOP\n",
      "Pemateri akan membahas\n",
      "tuntas dan detil pengalaman, langkah demi langkah hingga best practice sewaktu melakukan penelitian S3Setiap satu\n",
      "pokok materi selesai dijelaskan/didemokan/ditunjukkan pemateri maka peserta wajib\n",
      "untuk langsung mempraktekkannya.\n",
      "\n",
      "PELAKSANAANHari/Tanggal         : Kamis – Sabtu, 25-27 April 2019Waktu                      : 08-00 – 20.00 WIBTempat                    : Core Hotel Jalan Laksda Adisucipto, Jl. Janti No.KM.8, Caturtunggal, Kec. Depok, Kabupaten Sleman, Daerah Istimewa Yogyakarta 55281\n",
      "\n",
      "JUMLAH DAN SASARAN PESERTA\n",
      "Setiap kelas diikuti maksimal 30 peserta.Peserta berasal dari dosen-dosen PTS/PTN atau umum yang hendak /sedang S3\n",
      "\n",
      "BENTUK WORKSHOP\n",
      "Workshop diselenggarakan secara paralel dengan kelas terpisah sesuai bidang minat yang dipilihBidang minat beserta topik penelitian yang bisa dipilih, yaitu: Software EngineeringComputer Vision & Image ProcessingInformation Systems Sebelum workshop peserta akan diberi review paper dan paper teknikal utama untuk dikaji Paper-paper yang telah dikaji akan menjadi bahan materi utama workshop untuk menentukan judul penelitian yang tepat, masalah penelitian, research gap, state of the art method, kelebihan-kelemahan metode yang ada, kajian penelitian terkait, dsb.   Pemateri akan membahas tuntas dan detil pengalaman, langkah demi langkah hingga best practice sewaktu melakukan penelitian S3Setiap satu pokok materi selesai dijelaskan/didemokan/ditunjukkan pemateri maka peserta wajib untuk langsung mempraktekkannya\n",
      "OUTCOME & BENEFITSetelah workshop ini peserta akan mampu:\n",
      "Menyusun\n",
      "proposal penelitian S3 yang memikat calon Promotor & sukses menghadapi\n",
      "ujian komprehensif S3Menguasai\n",
      "teknik cepat menemukan research gap\n",
      "yang ada pada paper utama 3-5 tahun\n",
      "terakhirMerumuskan\n",
      "dengan baik antara research problem,\n",
      "research objective, dan research\n",
      "questionMenyusun tabel\n",
      "state of the art methodMenguasai\n",
      "strategi menyusun kajian literatur yang benarMembuat peta\n",
      "jalan studi S3 sehingga bisa selesai studi tepat waktuSetiap tahap\n",
      "penelitian dijelaskan secara rinci berdasarkan pengalaman pemateri langsung\n",
      "sewaktu S3. Termasuk strategi, kiat dan tekniknyaSelama\n",
      "workshop peserta di-coach sampai bisa\n",
      "menyusun proposal penelitian S3 yang baik dan layak diterima.\n",
      "PEMATERI\n",
      "Siti Rochimah, Ph.DKepala Laboratorium Rekayasa Perangkat Lunak Departemen Informatika Institut Teknologi Sepuluh Nopember, ITS. Bidang Penelitian: Software Engineering Eligible sebagai: Promotor S3\n",
      "Hanung Adi Nugroho, Ph.DIntelligent Systems Research GroupDepartment of Electrical Engineering and Information Technology, UGMBidang Penelitian: Computer Vision & Image ProcessingEligible sebagai: Promotor S3\n",
      "Paulus Insap Santosa, Ph.DDepartment of Electrical Engineering and Information Technology, UGMBidang Penelitian: Information Systems & Human Computer InteractionEligible sebagai: Promotor S3\n",
      "FASILITAS PESERTA– Certificate of Completion– Seminar Kit– Modul/handout materi workshop– Penginapan hotel selama dua malam (twin share)– 2 hari full board dan 1 hari full day meeting package\n",
      "I N V E S T A S IRp. 3.100.000 (Early bird – batas pembayaran 10 April 2019 )Rp. 2.950.000 (Alumni Workshop ITTC – batas pembayaran 18 April 2019)Rp. 3.350.000 (Normal – batas pembayaran 18 April 2019 )\n",
      "P E N D A F T A R A N\n",
      "Calon peserta mengisi form pendaftaran di bit.ly/idb17Penyelenggara akan mengirim surat elektronik berupa undangan resmi konfirmasi pendaftaran beserta petunjuk pembayaran workshop \n",
      "INFORMASI PENDAFTARAN\n",
      "Bpk. Sigit Arifianto, S.T.Telp/WA/SMS: 0817 910 3604Email: sigitar@staff.uad.ac.id\n",
      "INFORMASI PERIHAL MATERI, BENTUK WORKSHOP DLL\n",
      "Ardiansyah, S.T., M.CsTelp/WA/SMS: 0815 689 2648Email: ardiansyah@tif.uad.ac.id\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\tDiposting di Workshop \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Workshop Text Data Mining & Natural Language Processing (NLP) Batch 4\n",
      "\n",
      "Dikirim pada  25/02/2019 oleh  ittc \n",
      "—\n",
      "Tidak ada komentar ↓ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "OutcomeSetelah workshop ini peserta akan mampu:\n",
      "Mengusai metode-metode penting dalam Text Mining dan NLPMenganalisis social media, graf dan visualisasinyaMeringkas dokumen dan topic modelingMenganalisis sentiment pada teksMembuat model rekomendasiMembuat model text mining sendiri \n",
      "Materi\n",
      "Pengantar text data mining & social media analyticRepresentasi Data TeksUnsupervised Social Media AnalyticDimension ReductionSentiment AnalysisSocial Media Graph AnalysisSocial Media Visualization IIDocument SummarizationRecommendation Model\n",
      "Rundown Pelatihan\n",
      "Hari ke-1: Natural Language Processing (NLP)* Pendahuluan NLP* Tokenization & Reguler Expression* Preprocessing & normalisasi* Lemmatization & Stemming* Names Entity Recognition* n-grams and text prediction* Syntax parsing: tree bank* Word Sense Disambiguation* Spell-Check* Machine Translation* Word Embedding (Word2Vec)Hari ke-2: Text Mining* Representasi Dokumen* Text Clustering* Text Categorization* Recommendation Model* Document SummarizationHari ke-3: Social Media Analytics* Topic Modelling* Sentiment Analysis* Social Media Crawling* Community Detection* Centrality Analysis* Text Visualizations\n",
      "Sasaran PesertaMaksimal 30 orang dengan latar belakang sebagai software engineer, software developer, programmer, business & system analyst, startup founder, practitioner/IT professional, marketing, peneliti dan akademisi\n",
      "Bentuk Workshop\n",
      "Pemaparan konsep dasar Text Data Mining & NLP (20%) dilanjutkan pengenalan studi kasus dan praktek langsung membuat model Text Mining sendiri (80%)Tools yang digunakan: PythonPrasayarat (opsional): Memahami dasar machine learning & data mining\n",
      "Pemateri\n",
      "\n",
      "Taufik Edy Sutanto, Ph.DPhD in Data Science – Queensland University of Technology (QUT) AustraliaDosen Fakultas Sains & Teknologi – Universitas Islam Negeri (UIN) Jakarta\n",
      "Pelaksanaan\n",
      "Hari/Tanggal: Senin s.d Rabu, 1 – 3 April 2019Waktu: 08.00 – 17.00 WIBTempat: Labkom ITTC Lt. 4 Gedung ITC Kampus 1 UAD Jl. Kapas No. 9, Semaki Yogyakarta  \n",
      "Biaya & PendaftaranRp. 1.650.000 (Early Bird pembayaran akhir 13 Maret 2019)Rp. 1.800.000 (Normal pembayaran akhir 20 Maret 2019)Rp. 1.500.000 (Alumni workshop ITTC UAD batas pembayaran 20 Maret 2019)\n",
      "Pendaftaran: klik tautan iniRundown workshop:  https://bit.ly/2DT7sbv \n",
      "Informasi & narahubungSigit Arifianto, S.T.HP. 081 7910 3604Email: sigitar@staff.uad.ac.id \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\tDiposting di Workshop \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pelatihan Machine Learning & Data Mining for Software Engineer Batch 7\n",
      "\n",
      "Dikirim pada  25/02/2019 oleh  ittc \n",
      "—\n",
      "Tidak ada komentar ↓ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Anda sedang bekerja pada perusahaan eCommerce/marketing dan ingin meningkatkan penjualan dengan cara merekomendasikan produk-produk secara cerdas seperti Amazon, BukaLapak atau Netflix? Atau Anda sedang bekerja sebagai konsultan politik, dan ingin memprediksi kemenangan calon kepala daerah sekaligus melakukansentiment analysis terhadap calon yang diusung? Ataukah Anda bekerja di bagian keuangan, human resource, production, fintech, akademik/perguruan tinggi, perpustakaan? Maka workshop ini sangat tepat untuk Anda ikuti.\n",
      "Tahun 1990-2000 an adalah era pengembangan aplikasi, software dan sistem informasi untuk memudahkan bisnis/organisasi/korporat dalam mengelola bisnisnya. Fokus sistem/software/aplikasi yang dikembangkan adalah pada transaction processing, mulai dari penyimpanan data/informasi, mengolahnya menjadi laporan-laporan rutin yang statis. Software akuntansi, ERP, HRIS, payroll, inventori, PoS, sistem informasi akademik (SIA), dan sejenisnya merupakan beberapa contoh di antaranya.\n",
      "Setelah fase tersebut dilalui dan sistem informasi sudah menjadi bagian penting dalam organisasi/bisnis, ternyata saat ini keberadaan sistem-sistem tersebut tidak mampu memenuhi kebutuhan organisasi yang semakin berkembang dan dinamis. Dengan timbunan data yang sudah dikumpulkan terdapat banyak “harta karun” berharga yang belum dimanfaatkan secara optimal. Padahal apabila data-data yang sudah bertumpuk bertahun-tahun tersebut mampu diolah dengan baik maka bisa memberikan informasi yang sangat bernilai untuk kemajuan dan pertumbuhan bisnis/organisasi ke depan.\n",
      "Singkat kata, software/sistem informasi saat ini tidak lagi cukup sekedar mencatat, menyimpan dan menampilkan informasi/laporan. Tapi dituntut untuk bisa “cerdas”/intelligent sehingga bisa memberikan prediksi, saran dan rekomendasi secara akurat dan presisi yang bisa dimanfaatkan dalam mengambil keputusan-keputusan strategis dan penting guna meningkatkan value/penjualan perusahaan.\n",
      "Untuk itulah workshop 3 hari ini akan membekali Anda dengan keterampilan mutakhir dalam hal pemanfaatan data mining & machine learning sehingga bisa memberikan rekomendasi produk yang tepat untuk diberikan ke pelanggan, memprediksi produk yang “hampir pasti” dibeli pelanggan, memprediksi kelayakan nasabah yang mengajukan kredit dan masih banyak lagi yang lainnya.\n",
      "Pelaksanaan\n",
      "Tanggal          : Jumat-Minggu, 29 – 31 Maret 2019\n",
      "Waktu             : 08-00 – 17.00 WIB\n",
      "Tempat           : Labkom ITTC UAD Lt. 4 Gedung ITC UAD Jl. Kapas No. 9 Semaki Yogyakarta\n",
      "Sasaran Peserta\n",
      "Software Engineer, Software Developer, Programer, Business & System Analyst, Startup Founder, Practitioner/IT Professional, Academician.\n",
      "Outcome\n",
      "Setelah workshop ini peserta akan mampu:\n",
      "Mengusai metode-metode penting dalam Machine Learning & Data MiningMembuat model Machine Learning & Data Mining sendiri hingga dalam bentuk API.Menguji akurasi & presisi model yang dibuatMenerapkan/deploy model Machine Learning & Data Mining pada software/aplikasi yang pernah/telah dibuat\n",
      "Bentuk Workshop\n",
      "Pemaparan konsep dasar Machine Learning & Data Mining (20%) dilanjutkan pengenalan studi kasus dan praktek langsung membuat model Machine Learning & Data Mining sendiri (80%).\n",
      "Pemateri\n",
      "Arif Qodari, M.Sc Master in Artificial Intelligence University of AmsterdamData Scientist Ruangguru\n",
      "Materi\n",
      "Introduction to Machine Learning & Data MiningLinear model for regressionLinear model for classificationNaive Bayes & case studyRandom forest and ensembleDecision tree and random forestModel interpretationModel validation and evaluationCase study and develop APINeural network and deep learningConvolutional neural network and case study\n",
      "BiayaRp. 1.500.000 (Early Bird pembayaran akhir 8 Maret 2019)Rp. 1.750.000 (Normal pembayaran akhir 18 Maret 2019)\n",
      "Pendaftaran: klik tautan iniRundown workshop: https://bit.ly/2I0j4O6\n",
      "Informasi & narahubung: Sigit Arifianto: 081 7910 3604Email: sigitar@staff.uad.ac.id \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\tDiposting di Workshop \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Workshop Data Science Week 2019\n",
      "\n",
      "Dikirim pada  07/02/2019 oleh  ittc \n",
      "—\n",
      "4 Komentar ↓ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "WORKSHOP I\n",
      "Data Mining & Machine Learning for Software Engineer (Batch 7)\n",
      "Pemateri\n",
      "Arif Qodari, M.Sc (Data Scientist Ruangguru) – Master of Science in Artificial Intelligence University of Amsterdam – Belanda\n",
      "Pelaksanaan: Jum’at s/d Minggu, 29-31 Maret 2019\n",
      "Waktu: 08:00 s/d 17:00 WIB\n",
      "Tempat: Labkom ITTC UAD Lt. 4 Gedung ITC Universitas Ahmad Dahlan Jl. Kapas No. 9, Semaki Yogyakarta\n",
      "Investasi\n",
      "Rp. 1.750.000 (Normal – batas pembayaran 18 Maret 2019)\n",
      "Rp. 1.500.000 (Early bird – batas pembayaran 8 Maret 2019)\n",
      "Pendaftaran: http://bit.ly/mldmuad17\n",
      "Rundown Materi Workshop: https://bit.ly/2I0j4O6\n",
      "WORKSHOP II\n",
      "Text Data Mining & Natural Language Processing (NLP) (Batch 4)\n",
      "Pemateri\n",
      "Taufik Edy Sutanto, Ph.D\n",
      "(Ph.D in Data Science Queensland University of Technology – Dosen Fakultas Sains & Teknologi UIN Jakarta)\n",
      "Pelaksanaan: Senin s/d Rabu, 1-3 April 2019\n",
      "Waktu: 08:00 s/d 17:00 WIB\n",
      "Tempat: Labkom ITTC UAD Lt. 4 Gedung ITC Universitas Ahmad Dahlan Jl. Kapas No. 9, Semaki Yogyakarta\n",
      "Investasi\n",
      "Rp. 1.800.000 (Normal – batas pembayaran 20 Maret 2019)\n",
      "Rp. 1.650.000 (Early bird – batas pembayaran 13 Maret 2019)\n",
      "Rp. 1.500.000 (Alumni Workshop ITTC – batas pembayaran 20 Maret 2019)\n",
      "Pendaftaran: http://bit.ly/tdmn17\n",
      "Rundown materi workshop: https://bit.ly/2DT7sbv\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\tDiposting di Workshop \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pelatihan Data Mining & Machine Learning for Software Engineer Batch 6\n",
      "\n",
      "Dikirim pada  20/07/2018 oleh  ittc \n",
      "—\n",
      "Tidak ada komentar ↓ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Anda sedang bekerja pada perusahaan eCommerce/marketing dan ingin meningkatkan penjualan dengan cara merekomendasikan produk-produk secara cerdas seperti Amazon, BukaLapak atau Netflix? Atau Anda sedang bekerja sebagai konsultan politik, dan ingin memprediksi kemenangan calon kepala daerah sekaligus melakukan sentiment analysis terhadap calon yang diusung? Ataukah Anda bekerja di bagian keuangan, human resource, production, fintech, akademik/perguruan tinggi, perpustakaan? Maka workshop ini sangat tepat untuk Anda ikuti.\n",
      "Tahun 1990-2000 an adalah era pengembangan aplikasi, software dan sistem informasi untuk memudahkan bisnis/organisasi/korporat dalam mengelola bisnisnya. Fokus sistem/software/aplikasi yang dikembangkan adalah pada transaction processing, mulai dari penyimpanan data/informasi, mengolahnya menjadi laporan-laporan rutin yang statis. Software akuntansi, ERP, HRIS, payroll, inventori, PoS, sistem informasi akademik (SIA), dan sejenisnya merupakan beberapa contoh di antaranya.\n",
      "Setelah fase tersebut dilalui dan sistem informasi sudah menjadi bagian penting dalam organisasi/bisnis, ternyata saat ini keberadaan sistem-sistem tersebut tidak mampu memenuhi kebutuhan organisasi yang semakin berkembang dan dinamis. Dengan timbunan data yang sudah dikumpulkan terdapat banyak “harta karun” berharga yang belum dimanfaatkan secara optimal. Padahal apabila data-data yang sudah bertumpuk bertahun-tahun tersebut mampu diolah dengan baik maka bisa memberikan informasi yang sangat bernilai untuk kemajuan dan pertumbuhan bisnis/organisasi ke depan.\n",
      "Singkat kata, software/sistem informasi saat ini tidak lagi cukup sekedar mencatat, menyimpan dan menampilkan informasi/laporan. Tapi dituntut untuk bisa “cerdas”/intelligent sehingga bisa memberikan prediksi, saran dan rekomendasi secara akurat dan presisi yang bisa dimanfaatkan dalam mengambil keputusan-keputusan strategis dan penting guna meningkatkan value/penjualan perusahaan.\n",
      "Untuk itulah workshop 3 hari ini akan membekali Anda dengan keterampilan mutakhir dalam hal pemanfaatan data mining & machine learning sehingga bisa memberikan rekomendasi produk yang tepat untuk diberikan ke pelanggan, memprediksi produk yang “hampir pasti” dibeli pelanggan, memprediksi kelayakan nasabah yang mengajukan kredit dan masih banyak lagi yang lainnya.\n",
      "Pelaksanaan\n",
      "Tanggal          : Jumat-Minggu, 21 – 23 September 2018\n",
      "Waktu             : 08-00 – 17.00 WIB\n",
      "Tempat           : Labkom ITTC UAD Lt. 4 Gedung ITC UAD Jl. Kapas No. 9 Semaki Yogyakarta\n",
      "Sasaran Peserta\n",
      "Software Engineer, Software Developer, Programer, Business & System Analyst, Startup Founder, Practitioner/IT Professional, Academician.\n",
      "Outcome\n",
      "Setelah workshop ini peserta akan mampu:\n",
      "\n",
      "Mengusai metode-metode penting dalam Machine Learning & Data Mining\n",
      "Membuat model Machine Learning & Data Mining sendiri hingga dalam bentuk API.\n",
      "Menguji akurasi & presisi model yang dibuat\n",
      "Menerapkan/deploy model Machine Learning & Data Mining pada software/aplikasi yang pernah/telah dibuat\n",
      "\n",
      "Bentuk Workshop\n",
      "Pemaparan konsep dasar Machine Learning & Data Mining (20%) dilanjutkan pengenalan studi kasus dan praktek langsung membuat model Machine Learning & Data Mining sendiri (80%).\n",
      "Pemateri\n",
      "Arif Qodari, M.Sc \n",
      "Master in Artificial Intelligence University of Amsterdam\n",
      "Data Scientist Ruangguru\n",
      "Materi\n",
      "\n",
      "Introduction to Machine Learning & Data Mining\n",
      "Technical Setup\n",
      "Python for Machine Learning\n",
      "Basic Data Processing & Transformation with Pandas\n",
      "Basic Data Visualization with Matplotlib\n",
      "Case study 1 – Text Classification. Developing sentiment classifier (positive/negative/neutral) using Naive Bayes\n",
      "(dataset type: product review or movie review)\n",
      "Case study 2 – Ecommerce Product Recommender\n",
      "Case study 3 – Product rating Prediction\n",
      "Evaluating machine learning model (accuracy & precision)\n",
      "Deploy Machine Learning model as a Web Service using Flask\n",
      "\n",
      "Biaya\n",
      "Rp. 1.400.000 (Early Bird pembayaran akhir 7 September 2018)\n",
      "Rp. 1.650.000 (Normal pembayaran akhir 14 September 2018)\n",
      "Pendaftaran: klik tautan ini\n",
      "Informasi & narahubung: \n",
      "Sigit Arifianto: 081 7910 3604\n",
      "Email: sigitar@staff.uad.ac.id \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\tDiposting di Workshop \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Workshop Text Data Mining & Natural Language Processing (NLP) Batch 3\n",
      "\n",
      "Dikirim pada  20/07/2018 oleh  ittc \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Outcome\n",
      "Setelah workshop ini peserta akan mampu:\n",
      "\n",
      "Mengusai metode-metode penting dalam Text Mining dan NLP\n",
      "Menganalisis social media, graf dan visualisasinya\n",
      "Meringkas dokumen dan topic modeling\n",
      "Menganalisis sentiment pada teks\n",
      "Membuat model rekomendasi\n",
      "Membuat model text mining sendiri \n",
      "\n",
      "Materi\n",
      "\n",
      "Pengantar text data mining & social media analytic\n",
      "Representasi Data Teks\n",
      "Unsupervised Social Media Analytic\n",
      "Dimension Reduction dan visualisasinya\n",
      "Sentiment Analysis\n",
      "Social Media Graph Analysis\n",
      "Social Media Visualization II\n",
      "Document Summarization\n",
      "Deep learning pada data teks\n",
      "Gephi untuk media sosial\n",
      "\n",
      "Rundown Pelatihan\n",
      "Hari ke-1: Natural Language Processing (NLP)\n",
      "– Pendahuluan NLP\n",
      "– Tokenization & Reguler Expression\n",
      "– Preprocessing & normalisasi\n",
      "– Lemmatization & Stemming\n",
      "– n-grams and text prediction\n",
      "– Tagging: Syntax parsing: tree bank\n",
      "– Word Sense Disambiguation\n",
      "– Spell-Check\n",
      "– Word Embedding (Word2Vec)\n",
      "Hari ke-2: Text Mining\n",
      "– Deep learning pada data teks\n",
      "– Representasi Dokumen\n",
      "– Text Clustering\n",
      "– Document summarization\n",
      "– Text Categorization\n",
      "Hari ke-3: Social Media Analytics\n",
      "* Topic Modeling\n",
      "* Sentiment Analysis\n",
      "* Social Media Crawling\n",
      "* Community Detection\n",
      "* Centrality Analysis\n",
      "* Text Visualizations\n",
      "Sasaran Peserta\n",
      "Maksimal 30 orang dengan latar belakang sebagai software engineer, software developer, programmer, business & system analyst, startup founder, practitioner/IT professional, marketing, peneliti dan akademisi\n",
      "Bentuk Workshop\n",
      "\n",
      "Pemaparan konsep dasar Text Data Mining & NLP (20%) dilanjutkan pengenalan studi kasus dan praktek langsung membuat model Text Mining sendiri (80%)\n",
      "Tools yang digunakan: Python\n",
      "Prasayarat (opsional): Memahami dasar machine learning & data mining\n",
      "\n",
      "Pemateri\n",
      "\n",
      "Taufik Edy Sutanto, Ph.D\n",
      "PhD in Data Science – Queensland University of Technology (QUT) Australia\n",
      "Dosen Fakultas Sains & Teknologi – Universitas Islam Negeri (UIN) Jakarta\n",
      "Pelaksanaan\n",
      "\n",
      "Hari/Tanggal: Rabu s.d Jum’at, 26 – 28 September 2018\n",
      "Waktu: 08.00 – 17.00 WIB\n",
      "Tempat: Labkom ITTC Lt. 4 Gedung ITC Kampus 1 UAD Jl. Kapas No. 9, Semaki Yogyakarta  \n",
      "\n",
      "Biaya & Pendaftaran\n",
      "Rp. 1.600.000 (Early Bird pembayaran akhir 12 September 2018)\n",
      "Rp. 1.900.000 (Normal pembayaran akhir 17 September 2018)\n",
      "Rp. 1.400.000 (Alumni workshop MLDM/DSDB)\n",
      "Pendaftaran: klik tautan ini\n",
      "Informasi & narahubung\n",
      "Sigit Arifianto, S.T.\n",
      "HP. 081 7910 3604\n",
      "Email: sigitar@staff.uad.ac.id \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\tDiposting di Workshop \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data Science Doctoral Bootcamp 2018\n",
      "\n",
      "Dikirim pada  08/02/2018 oleh  ittc \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“Workshop pertama di Indonesia yang membekali Anda dengan riset DATA SCIENCE yang mutakhir, metodologi penelitian yang benar, sehingga diharapkan bisa LULUS S3 TEPAT WAKTU!”\n",
      "– – –\n",
      "INDONESIA KEKURANGAN DOKTOR BIDANG INFORMATIKA/KOMPUTER\n",
      "Untuk mengatasi krisis Doktor, pemerintah melalui Kemristekdikti sudah mencanangkan untuk menghasilkan 20.000 Doktor pada 2019. Saat ini kelangkaan Doktor masih besar apalagi di bidang Informatika.\n",
      "Salah satu faktor utama lambannya pertumbuhan Doktor di bidang Informatika di Indonesia adalah karena lemahnya kompetensi dan keahlian melakukan penelitian. Kelemahan itu mulai dari sisi proposal, metodologi, hingga publikasi ilmiah. Ini terlihat di beberapa PTN menilai hampir 80% di antara pelamar proposal S3 nya dianggap tidak layak. Setelah ditelisik, ternyata kebanyakan proposal yang dibuat kurang dari satu minggu. Alasannya beragam, karena hendak mengejar tenggat waktu, hingga yang sekedar coba-coba. Hal ini dikarenakan juga proposal yang disiapkan masih sangat umum, cenderung aplikatif setara S1 dan belum fokus pada kebaruan (novelty) yang memang menjadi syarat wajib penelitian S3.\n",
      "Usulan penelitian S3 yang baik dan memiliki peluang besar untuk diterima mesti memenuhi berbagai kriteria penting. Beberapa kriteria utama tersebut di antaranya adalah kajian penelitian terkait, state of the art method, problem statement, research objective dan research questions hingga research gap/novelty. Semua itu tidak bisa hanya ditulis dalam beberapa hari saja. Butuh persiapan yang memadai dan sumber-sumber pustaka (jurnal terbaru) utama yang perlu dibaca, dikaji secara seksama dan mendalam.\n",
      "Dengan melihat beban kerja dosen terutama di aspek pengajaran yang cukup besar, maka tidak mengherankan persiapan membuat proposal S3 menjadi terbengkalai sehingga proposal disiapkan di tengah waktu luang yang tersisa. Untuk itu, guna membantu dan memudahkan para dosen yang bertekad untuk segera S3, maka diselenggarakanlah Data Science Doctoral Bootcamp 2018 (DSDB 2018). DSDB 2018 diselenggarakan khusus untuk mengakomodasi pertumbuhan doktor-doktor baru di bidang yang sedang trend di dunia saat ini yaitu data science.\n",
      "DSDB 2018 ini merupakan investasi sekali seumur hidup yang akan membekali para dosen atau siapapun yang berminat melanjutkan studi S3 dengan pengetahuan, pemahaman dan keterampilan lengkap tentang bagaimana menyusun proposal penelitian S3 yang baik, bermutu dan memikat calon Promotor. Sehingga dengan berhasil menyusun proposal yang baik dan bermutu diharapkan nantinya bisa lulus S3 tepat waktu.\n",
      "SASARAN PESERTA\n",
      "\n",
      "Dosen PTS/PTN atau umum yang bersiap/berminat untuk S3 bidang data science\n",
      "Disediakan dua kelas dengan pemateri berbeda\n",
      "Setiap kelas maksimal diisi oleh 30 peserta\n",
      "\n",
      "BENTUK WORKSHOP\n",
      "\n",
      "Sebelum workshop peserta akan diberi review paper dan paper teknikal utama untuk dikaji \n",
      "Paper-paper yang telah dikaji akan menjadi bahan materi utama workshop untuk menentukan judul penelitian yang tepat, masalah penelitian, research gap, state of the art method, kelebihan-kelemahan metode yang ada, kajian penelitian terkait, dsb.   \n",
      "Pemateri membahas tuntas dan rinci langkah demi langkah pengalaman hingga best practice mereka sewaktu melakukan penelitian S3\n",
      "Setiap satu pokok materi selesai dijelaskan/didemokan/ditunjukkan pemateri maka peserta diminta untuk langsung mempraktekkannya\n",
      "\n",
      "PEMATERI\n",
      "Taufik Edy Sutanto, Ph.D\n",
      "PhD bidang data science dari Queensland University of Technology (QUT), Australia\n",
      "Dosen Fakultas Sains & Teknologi UIN Syarif Hidayatullah Jakarta\n",
      "Disertasi S3: “Scalable fine-grained document clustering via ranking”\n",
      "Dr. Suyanto, M.Sc\n",
      "Doktor bidang data science dari Ilmu Komputer Universitas Gadjah Mada (UGM)\n",
      "Lab. Intelligence Computing Multimedia Fakultas Informatika Telkom University\n",
      "Disertasi S3: “Indonesian Phonemic Syllabification Using Pseudo Nearest Neighbour Rule and Phonotactic Knowledge“\n",
      "FASILITAS\n",
      "\n",
      "Pemateri yang berpengalaman dalam penelitian bidang data science\n",
      "Certificate of Completion\n",
      "Modul workshop\n",
      "Paper teknikal utama 3 tahun terakhir\n",
      "Makan 2 kali & coffee break 2 kali sehari\n",
      "\n",
      "OUTCOME & BENEFIT\n",
      "Setelah workshop ini peserta akan mampu:\n",
      "\n",
      "Menyusun proposal penelitian S3 yang memikat calon Promotor (supervisor) pada program studi S3 yang dituju\n",
      "Menguasai teknik cepat menemukan research gap yang ada pada paper utama 3-5 tahun terakhir\n",
      "Merumuskan dengan baik antara research problem-research objective-research question\n",
      "Menyusun tabel state of the art method\n",
      "Menguasai strategi menyusun kajian literatur yang benar\n",
      "Membuat peta jalan studi S3 sehingga bisa selesai dalam 3 tahun\n",
      "Setiap tahap penelitian dijelaskan secara rinci berdasarkan pengalaman pemateri langsung sewaktu S3. Termasuk strategi, kiat dan tekniknya\n",
      "Selama workshop peserta di-coach sampai bisa menyusun proposal penelitian S3 yang baik dan layak diterima \n",
      "\n",
      "INVESTASI\n",
      "\n",
      "Rp. 2.250.000 (Umum Non-Alumni)\n",
      "Rp. 2.000.000 (Alumni MLDM-TDM-IDB-MPSI)\n",
      "Batas akhir pembayaran 20 April 2018 pukul 14:00 WIB\n",
      "\n",
      "PELAKSANAAN\n",
      "\n",
      "Hari        : Kamis – Sabtu\n",
      "Tanggal  : 26 – 28 April 2018\n",
      "Tempat   : Hotel Puri Artha, Jl. Cenderawasih No.36, Demangan, Gondokusuman, Yogyakarta\n",
      "\n",
      "PENDAFTARAN\n",
      "\n",
      "Calon peserta mengisi form pendaftaran di bit.ly/idb17\n",
      "Penyelenggara akan mengirim surat elektronik berupa undangan resmi konfirmasi pendaftaran beserta petunjuk pembayaran workshop\n",
      "Batas akhir pendaftaran tanggal 17 April 2018 \n",
      "\n",
      "INFORMASI PENDAFTARAN\n",
      "\n",
      "Bpk. Sigit Arifianto, S.T.\n",
      "Telp/WA/SMS: 0817 910 3604\n",
      "Email: sigitar@staff.uad.ac.id\n",
      "\n",
      "INFORMASI PERIHAL MATERI, BENTUK WORKSHOP DLL\n",
      "\n",
      "Ardiansyah, S.T., M.Cs\n",
      "Telp/WA/SMS: 0815 689 2648\n",
      "Email: ardiansyah@tif.uad.ac.id\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\tDiposting di Workshop \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Workshop Text Data Mining & Natural Language Processing (NLP) Batch 2\n",
      "\n",
      "Dikirim pada  02/02/2018 oleh  ittc \n",
      "—\n",
      "Tidak ada komentar ↓ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Outcome\n",
      "Setelah workshop ini peserta akan mampu:\n",
      "\n",
      "Mengusai metode-metode penting dalam Text Mining dan NLP\n",
      "Menganalisis social media, graf dan visualisasinya\n",
      "Meringkas dokumen dan topic modeling\n",
      "Menganalisis sentiment pada teks\n",
      "Membuat model rekomendasi\n",
      "Membuat model text mining sendiri \n",
      "\n",
      "Materi\n",
      "\n",
      "Pengantar text data mining & social media analytic\n",
      "Representasi Data Teks\n",
      "Unsupervised Social Media Analytic\n",
      "Dimension Reduction\n",
      "Sentiment Analysis\n",
      "Social Media Graph Analysis\n",
      "Social Media Visualization II\n",
      "Document Summarization\n",
      "Recommendation Model\n",
      "\n",
      "Rundown Pelatihan\n",
      "\n",
      "Hari ke-1: Natural Language Processing (NLP)\n",
      "* Pendahuluan NLP\n",
      "* Tokenization & Reguler Expression\n",
      "* Preprocessing & normalisasi\n",
      "* Lemmatization & Stemming\n",
      "* Names Entity Recognition\n",
      "* n-grams and text prediction\n",
      "* Syntax parsing: tree bank\n",
      "* Word Sense Disambiguation\n",
      "* Spell-Check\n",
      "* Machine Translation\n",
      "* Word Embedding (Word2Vec)\n",
      "Hari ke-2: Text Mining\n",
      "* Representasi Dokumen\n",
      "* Text Clustering\n",
      "* Text Categorization\n",
      "* Recommendation Model\n",
      "* Document Summarization\n",
      "Hari ke-3: Social Media Analytics\n",
      "* Topic Modelling\n",
      "* Sentiment Analysis\n",
      "* Social Media Crawling\n",
      "* Community Detection\n",
      "* Centrality Analysis\n",
      "* Text Visualizations\n",
      "\n",
      "Sasaran Peserta\n",
      "Maksimal 30 orang dengan latar belakang sebagai software engineer, software developer, programmer, business & system analyst, startup founder, practitioner/IT professional, marketing, peneliti dan akademisi\n",
      "Bentuk Workshop\n",
      "\n",
      "Pemaparan konsep dasar Text Data Mining & NLP (20%) dilanjutkan pengenalan studi kasus dan praktek langsung membuat model Text Mining sendiri (80%)\n",
      "Tools yang digunakan: Python\n",
      "Prasayarat (opsional): Memahami dasar machine learning & data mining\n",
      "\n",
      "Pemateri\n",
      "\n",
      "Taufik Edy Sutanto, Ph.D\n",
      "PhD in Data Science – Queensland University of Technology (QUT) Australia\n",
      "Dosen Fakultas Sains & Teknologi – Universitas Islam Negeri (UIN) Jakarta\n",
      "Pelaksanaan\n",
      "\n",
      "Hari/Tanggal: Kamis s.d Sabtu, 29 – 31 Maret 2018\n",
      "Waktu: 08.00 – 17.00 WIB\n",
      "Tempat: Labkom ITTC Lt. 4 Gedung ITC Kampus 1 UAD Jl. Kapas No. 9, Semaki Yogyakarta  \n",
      "\n",
      "Biaya & Pendaftaran\n",
      "Rp. 1.500.000 (Early Bird pembayaran akhir 10 Maret 2018)\n",
      "Rp. 1.800.000 (Normal pembayaran akhir 24 Maret 2018)\n",
      "Rp. 1.300.000 (Alumni workshop MLDM)\n",
      "Pendaftaran: klik tautan ini\n",
      "Informasi & narahubung\n",
      "Sigit Arifianto, S.T.\n",
      "HP. 081 7910 3604\n",
      "Email: sigitar@staff.uad.ac.id \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\tDiposting di Workshop \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pelatihan Data Mining & Machine Learning for Software Engineer Batch 5\n",
      "\n",
      "Dikirim pada  02/02/2018 oleh  ittc \n",
      "—\n",
      "Tidak ada komentar ↓ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Anda sedang bekerja pada perusahaan eCommerce/marketing dan ingin meningkatkan penjualan dengan cara merekomendasikan produk-produk secara cerdas seperti Amazon, BukaLapak atau Netflix? Atau Anda sedang bekerja sebagai konsultan politik, dan ingin memprediksi kemenangan calon kepala daerah sekaligus melakukan sentiment analysis terhadap calon yang diusung? Ataukah Anda bekerja di bagian keuangan, human resource, production, fintech, akademik/perguruan tinggi, perpustakaan? Maka workshop ini sangat tepat untuk Anda ikuti.\n",
      "Tahun 1990-2000 an adalah era pengembangan aplikasi, software dan sistem informasi untuk memudahkan bisnis/organisasi/korporat dalam mengelola bisnisnya. Fokus sistem/software/aplikasi yang dikembangkan adalah pada transaction processing, mulai dari penyimpanan data/informasi, mengolahnya menjadi laporan-laporan rutin yang statis. Software akuntansi, ERP, HRIS, payroll, inventori, PoS, sistem informasi akademik (SIA), dan sejenisnya merupakan beberapa contoh di antaranya.\n",
      "Setelah fase tersebut dilalui dan sistem informasi sudah menjadi bagian penting dalam organisasi/bisnis, ternyata saat ini keberadaan sistem-sistem tersebut tidak mampu memenuhi kebutuhan organisasi yang semakin berkembang dan dinamis. Dengan timbunan data yang sudah dikumpulkan terdapat banyak “harta karun” berharga yang belum dimanfaatkan secara optimal. Padahal apabila data-data yang sudah bertumpuk bertahun-tahun tersebut mampu diolah dengan baik maka bisa memberikan informasi yang sangat bernilai untuk kemajuan dan pertumbuhan bisnis/organisasi ke depan.\n",
      "Singkat kata, software/sistem informasi saat ini tidak lagi cukup sekedar mencatat, menyimpan dan menampilkan informasi/laporan. Tapi dituntut untuk bisa “cerdas”/intelligent sehingga bisa memberikan prediksi, saran dan rekomendasi secara akurat dan presisi yang bisa dimanfaatkan dalam mengambil keputusan-keputusan strategis dan penting guna meningkatkan value/penjualan perusahaan.\n",
      "Untuk itulah workshop 3 hari ini akan membekali Anda dengan keterampilan mutakhir dalam hal pemanfaatan data mining & machine learning sehingga bisa memberikan rekomendasi produk yang tepat untuk diberikan ke pelanggan, memprediksi produk yang “hampir pasti” dibeli pelanggan, memprediksi kelayakan nasabah yang mengajukan kredit dan masih banyak lagi yang lainnya.\n",
      "Pelaksanaan\n",
      "Tanggal          : Jumat-Minggu, 23 – 25 Maret 2018\n",
      "Waktu             : 08-00 – 17.00 WIB\n",
      "Tempat           : Labkom ITTC UAD Lt. 4 Gedung ITC UAD Jl. Kapas No. 9 Semaki Yogyakarta\n",
      "Sasaran Peserta\n",
      "Software Engineer, Software Developer, Programer, Business & System Analyst, Startup Founder, Practitioner/IT Professional, Academician.\n",
      "Outcome\n",
      "Setelah workshop ini peserta akan mampu:\n",
      "\n",
      "Mengusai metode-metode penting dalam Machine Learning & Data Mining\n",
      "Membuat model Machine Learning & Data Mining sendiri hingga dalam bentuk API.\n",
      "Menguji akurasi & presisi model yang dibuat\n",
      "Menerapkan/deploy model Machine Learning & Data Mining pada software/aplikasi yang pernah/telah dibuat\n",
      "\n",
      "Bentuk Workshop\n",
      "Pemaparan konsep dasar Machine Learning & Data Mining (20%) dilanjutkan pengenalan studi kasus dan praktek langsung membuat model Machine Learning & Data Mining sendiri (80%).\n",
      "Pemateri\n",
      "Arif Qodari, M.Sc \n",
      "Master in Artificial Intelligence University of Amsterdam\n",
      "Data Scientist Salestock.id\n",
      "Materi\n",
      "\n",
      "Introduction to Machine Learning & Data Mining\n",
      "Technical Setup\n",
      "Python for Machine Learning\n",
      "Basic Data Processing & Transformation with Pandas\n",
      "Basic Data Visualization with Matplotlib\n",
      "Case study 1 – Text Classification. Developing sentiment classifier (positive/negative/neutral) using Naive Bayes\n",
      "(dataset type: product review or movie review)\n",
      "Case study 2 – Ecommerce Product Recommender\n",
      "Case study 3 – Product rating Prediction\n",
      "Evaluating machine learning model (accuracy & precision)\n",
      "Deploy Machine Learning model as a Web Service using Flask\n",
      "\n",
      "Biaya\n",
      "Rp. 1.350.000 (Early Bird pembayaran akhir 3 Maret 2018)\n",
      "Rp. 1.550.000 (Normal pembayaran akhir 20 Maret 2018)\n",
      "Pendaftaran: klik tautan ini\n",
      "Informasi & narahubung: \n",
      "Sigit Arifianto: 081 7910 3604\n",
      "Email: sigitar@staff.uad.ac.id \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\tDiposting di Workshop \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Workshop Text Data Mining & Natural Language Processing (NLP)\n",
      "\n",
      "Dikirim pada  12/11/2017 oleh  ittc \n",
      "—\n",
      "Tidak ada komentar ↓ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Outcome\n",
      "Setelah workshop ini peserta akan mampu:\n",
      "\n",
      "Mengusai metode-metode penting dalam Text Mining dan NLP\n",
      "Menganalisis social media, graf dan visualisasinya\n",
      "Meringkas dokumen dan topic modeling\n",
      "Menganalisis sentiment pada teks\n",
      "Membuat model rekomendasi\n",
      "Membuat model text mining sendiri \n",
      "\n",
      "Materi\n",
      "\n",
      "Pengantar text data mining & social media analytic\n",
      "Representasi Data Teks\n",
      "Unsupervised Social Media Analytic\n",
      "Dimension Reduction\n",
      "Sentiment Analysis\n",
      "Social Media Graph Analysis\n",
      "Social Media Visualization II\n",
      "Document Summarization\n",
      "Recommendation Model\n",
      "\n",
      "Outline materi lengkap bisa dibaca di sini.\n",
      "Sasaran Peserta\n",
      "Maksimal 30 orang dengan latar belakang sebagai software engineer, software developer, programmer, business & system analyst, startup founder, practitioner/IT professional, marketing, peneliti dan akademisi\n",
      "Bentuk Workshop\n",
      "\n",
      "Pemaparan konsep dasar Text Data Mining & NLP (20%) dilanjutkan pengenalan studi kasus dan praktek langsung membuat model Text Mining sendiri (80%)\n",
      "Tools yang digunakan: Python\n",
      "Prasayarat (opsional): Memahami dasar machine learning & data mining\n",
      "\n",
      "Pemateri\n",
      "\n",
      "Taufik Edy Sutanto, Ph.D\n",
      "PhD in Data Science – Queensland University of Technology (QUT) Australia\n",
      "Dosen Fakultas Sains & Teknologi – Universitas Islam Negeri (UIN) Jakarta\n",
      "Pelaksanaan\n",
      "\n",
      "Hari/Tanggal: Kamis s.d Sabtu, 25-27 Januari 2018\n",
      "Waktu: 08.00 – 17.00 WIB\n",
      "Tempat: Labkom ITTC Lt. 4 Gedung ITC Kampus 1 UAD Jl. Kapas No. 9, Semaki Yogyakarta  \n",
      "\n",
      "Biaya & Pendaftaran\n",
      "Rp. 1.500.000 (Early Bird pembayaran akhir 10 Januari 2018)\n",
      "Rp. 1.800.000 (Normal pembayaran akhir 22 Januari 2018)\n",
      "Rp. 1.300.000 (Alumni workshop MLDM)\n",
      "Pendaftaran: klik tautan ini\n",
      "Informasi & narahubung\n",
      "Sigit Arifianto, S.T.\n",
      "HP. 081 7910 3604\n",
      "Email: sigitar@staff.uad.ac.id \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\tDiposting di Workshop \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Posting navigasi\n",
      "\n",
      " ← Posting lama\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cari untuk:\n",
      "\n",
      "\n",
      "\n",
      "  Pos-pos Terbaru \n",
      "\n",
      "Informatics Doctoral Bootcamp 2019\n",
      "\n",
      "\n",
      "Workshop Text Data Mining & Natural Language Processing (NLP) Batch 4\n",
      "\n",
      "\n",
      "Pelatihan Machine Learning & Data Mining for Software Engineer Batch 7\n",
      "\n",
      "\n",
      "Workshop Data Science Week 2019\n",
      "\n",
      "\n",
      "Pelatihan Data Mining & Machine Learning for Software Engineer Batch 6\n",
      "\n",
      "\n",
      "Komentar Terbaruittc pada Workshop Data Science Week 2019Nida pada Workshop Data Science Week 2019ittc pada Workshop Data Science Week 2019Suci Miranda pada Workshop Data Science Week 2019Workshop Text Data Mining & Natural Language Processing (NLP) Batch 2 – ITTC Universitas Ahmad Dahlan pada Pelatihan Data Mining & Machine Learning for Software EngineerArsip \n",
      "Februari 2019\n",
      "Juli 2018\n",
      "Februari 2018\n",
      "November 2017\n",
      "Oktober 2017\n",
      "September 2017\n",
      "Agustus 2017\n",
      "Juli 2017\n",
      "Juni 2017\n",
      "Mei 2017\n",
      "September 2016\n",
      "\n",
      "Kategori \n",
      "Android\n",
      "\n",
      "Workshop\n",
      "\n",
      "\n",
      "Meta \n",
      "Masuk\n",
      "RSS Entri\n",
      "RSS Komentar\n",
      "WordPress.org \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t© 2019 ITTC Universitas Ahmad Dahlan \n",
      "\n",
      "Responsif II didukung oleh WordPress \n",
      "\n",
      "↑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "URL = 'http://ittc.uad.ac.id'\n",
    "Doc = urllib.request.urlopen(URL).read()\n",
    "Doc = bs(Doc,'lxml').text\n",
    "print(Doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Json-Files\">Json Files</h2>\n",
    "\n",
    "<ul>\n",
    "\t<li>Populer digunakan untuk data dari Media Sosial dan NoSQL</li>\n",
    "\t<li>Portable: File Json memuat nama variabel dan nilainya (tidak seperti XML)</li>\n",
    "\t<li>Plain Text</li>\n",
    "\t<li>Schemaless: Setiap record tidak harus memiliki jumlah field yang tetap seperti csv</li>\n",
    "\t<li>JSON isomorfis dengan &quot;Dictionary&quot; di Python</li>\n",
    "\t<li>Contoh struktur file json:</li>\n",
    "</ul>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/json.png\" style=\"width: 200px; height: 211px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"nama\":\"Mukidi\",\"umur\":19,\"mobil\": \"BMW\",\"motor\":\"Forza\"}\\n',\n",
       " '{\"nama\":\"Udin\",\"umur\":47,\"mobil\": \"Calya\"}\\n',\n",
       " '{\"nama\":\"Inem\",\"umur\":17,\"motor\": \"Fit S\"}']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contoh load JSON\n",
    "import json\n",
    "\n",
    "file = 'data/contoh.json'\n",
    "f=open(file,encoding='utf-8', errors ='ignore', mode='r')\n",
    "J=f.readlines()\n",
    "f.close()\n",
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,t in enumerate(J):\n",
    "    J[i] = json.loads(t.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mukidi'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J[0]['nama']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset dari Modul\n",
    "contoh dataset dokumen yang cukup tenar: &quot;20 NewsGroup&quot;\n",
    "\n",
    "<img alt=\"\" src=\"images/6_20News.jpg\" style=\"height: 300px ; width: 533px\" />\n",
    "<p><a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups\" target=\"_blank\">http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups</a></p>\n",
    "\n",
    "<p><strong>Categories </strong>=&nbsp;</p>\n",
    "<pre>\n",
    "[&#39;alt.atheism&#39;, &#39;comp.graphics&#39;, &#39;comp.os.ms-windows.misc&#39;, &#39;comp.sys.ibm.pc.hardware&#39;, &#39;comp.sys.mac.hardware&#39;,\n",
    " &#39;comp.windows.x&#39;, &#39;misc.forsale&#39;, &#39;rec.autos&#39;, &#39;rec.motorcycles&#39;, &#39;rec.sport.baseball&#39;, &#39;rec.sport.hockey&#39;,\n",
    " &#39;sci.crypt&#39;, &#39;sci.electronics&#39;, &#39;sci.med&#39;, &#39;sci.space&#39;, &#39;soc.religion.christian&#39;, &#39;talk.politics.guns&#39;,\n",
    " &#39;talk.politics.mideast&#39;, &#39;talk.politics.misc&#39;, &#39;talk.religion.misc&#39;]</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Warning: Butuh koneksi internet\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['sci.med', 'talk.politics.misc',  'rec.autos']\n",
    "data = fetch_20newsgroups(subset='train', categories=categories,remove=('headers', 'footers', 'quotes'))\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ada baiknya data disimpan ke HardDisk, agar ketika kita perlu untuk me-load kembali\n",
    "# Tidak perlu koneksi internet\n",
    "import pickle\n",
    "\n",
    "f = open('data/tdm2Apr.pckl', 'wb')\n",
    "pickle.dump(data, f)\n",
    "f.close()\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data dari \"Pickle\" dapat dilakukan dengan cara berikut\n",
    "f = open('data/20newsgroups.pckl', 'rb')\n",
    "\n",
    "data = pickle.load(f)\n",
    "f.close()\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n",
      "['DESCR', 'data', 'filenames', 'target', 'target_names']\n",
      "I want to get a car alarm and I am thinking about getting an Ungo Box.\n",
      "    Does anyone out there have any knowledge or experience with any of\n",
      "    these alarms?  How about price ranges for the different models?\n",
      "    Are these good car alarms?  Please email me any responces.\n",
      "\n",
      "                cak3@ns3.lehigh.edu\n",
      "rec.autos\n",
      ".. _20newsgroups_dataset:\n",
      "\n",
      "The 20 newsgroups text dataset\n",
      "------------------------------\n",
      "\n",
      "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
      "20 topics split in two subsets: one for training (or development)\n",
      "and the other one for testing (or for performance evaluation). The split\n",
      "between the train and test set is based upon a messages posted before\n",
      "and after a specific date.\n",
      "\n",
      "This module contains two loaders. The first one,\n",
      ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
      "returns a list of the raw texts that can be fed to text feature\n",
      "extractors such as :class:`sklearn.feature_extraction.text.CountVectorizer`\n",
      "with custom parameters so as to extract feature vectors.\n",
      "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
      "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
      "extractor.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    =================   ==========\n",
      "    Classes                     20\n",
      "    Samples total            18846\n",
      "    Dimensionality               1\n",
      "    Features                  text\n",
      "    =================   ==========\n",
      "\n",
      "Usage\n",
      "~~~~~\n",
      "\n",
      "The :func:`sklearn.datasets.fetch_20newsgroups` function is a data\n",
      "fetching / caching functions that downloads the data archive from\n",
      "the original `20 newsgroups website`_, extracts the archive contents\n",
      "in the ``~/scikit_learn_data/20news_home`` folder and calls the\n",
      ":func:`sklearn.datasets.load_files` on either the training or\n",
      "testing set folder, or both of them::\n",
      "\n",
      "  >>> from sklearn.datasets import fetch_20newsgroups\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train')\n",
      "\n",
      "  >>> from pprint import pprint\n",
      "  >>> pprint(list(newsgroups_train.target_names))\n",
      "  ['alt.atheism',\n",
      "   'comp.graphics',\n",
      "   'comp.os.ms-windows.misc',\n",
      "   'comp.sys.ibm.pc.hardware',\n",
      "   'comp.sys.mac.hardware',\n",
      "   'comp.windows.x',\n",
      "   'misc.forsale',\n",
      "   'rec.autos',\n",
      "   'rec.motorcycles',\n",
      "   'rec.sport.baseball',\n",
      "   'rec.sport.hockey',\n",
      "   'sci.crypt',\n",
      "   'sci.electronics',\n",
      "   'sci.med',\n",
      "   'sci.space',\n",
      "   'soc.religion.christian',\n",
      "   'talk.politics.guns',\n",
      "   'talk.politics.mideast',\n",
      "   'talk.politics.misc',\n",
      "   'talk.religion.misc']\n",
      "\n",
      "The real data lies in the ``filenames`` and ``target`` attributes. The target\n",
      "attribute is the integer index of the category::\n",
      "\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n",
      "\n",
      "It is possible to load only a sub-selection of the categories by passing the\n",
      "list of the categories to load to the\n",
      ":func:`sklearn.datasets.fetch_20newsgroups` function::\n",
      "\n",
      "  >>> cats = ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
      "\n",
      "  >>> list(newsgroups_train.target_names)\n",
      "  ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "\n",
      "Converting text to vectors\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "In order to feed predictive or clustering models with the text data,\n",
      "one first need to turn the text into vectors of numerical values suitable\n",
      "for statistical analysis. This can be achieved with the utilities of the\n",
      "``sklearn.feature_extraction.text`` as demonstrated in the following\n",
      "example that extract `TF-IDF`_ vectors of unigram tokens\n",
      "from a subset of 20news::\n",
      "\n",
      "  >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "  >>> categories = ['alt.atheism', 'talk.religion.misc',\n",
      "  ...               'comp.graphics', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectorizer = TfidfVectorizer()\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> vectors.shape\n",
      "  (2034, 34118)\n",
      "\n",
      "The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\n",
      "components by sample in a more than 30000-dimensional space\n",
      "(less than .5% non-zero features)::\n",
      "\n",
      "  >>> vectors.nnz / float(vectors.shape[0])       # doctest: +ELLIPSIS\n",
      "  159.01327...\n",
      "\n",
      ":func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which \n",
      "returns ready-to-use token counts features instead of file names.\n",
      "\n",
      ".. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\n",
      ".. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\n",
      "\n",
      "\n",
      "Filtering text for more realistic training\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "It is easy for a classifier to overfit on particular things that appear in the\n",
      "20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\n",
      "high F-scores, but their results would not generalize to other documents that\n",
      "aren't from this window of time.\n",
      "\n",
      "For example, let's look at the results of a multinomial Naive Bayes classifier,\n",
      "which is fast to train and achieves a decent F-score::\n",
      "\n",
      "  >>> from sklearn.naive_bayes import MultinomialNB\n",
      "  >>> from sklearn import metrics\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  # doctest: +ELLIPSIS\n",
      "  0.88213...\n",
      "\n",
      "(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\n",
      "the training and test data, instead of segmenting by time, and in that case\n",
      "multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\n",
      "yet of what's going on inside this classifier?)\n",
      "\n",
      "Let's take a look at what the most informative features are:\n",
      "\n",
      "  >>> import numpy as np\n",
      "  >>> def show_top10(classifier, vectorizer, categories):\n",
      "  ...     feature_names = np.asarray(vectorizer.get_feature_names())\n",
      "  ...     for i, category in enumerate(categories):\n",
      "  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n",
      "  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
      "  ...\n",
      "  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\n",
      "  alt.atheism: edu it and in you that is of to the\n",
      "  comp.graphics: edu in graphics it is for and of to the\n",
      "  sci.space: edu it that is in and space to of the\n",
      "  talk.religion.misc: not it you in is that and to of the\n",
      "\n",
      "\n",
      "You can now see many things that these features have overfit to:\n",
      "\n",
      "- Almost every group is distinguished by whether headers such as\n",
      "  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n",
      "- Another significant feature involves whether the sender is affiliated with\n",
      "  a university, as indicated either by their headers or their signature.\n",
      "- The word \"article\" is a significant feature, based on how often people quote\n",
      "  previous posts like this: \"In article [article ID], [name] <[e-mail address]>\n",
      "  wrote:\"\n",
      "- Other features match the names and e-mail addresses of particular people who\n",
      "  were posting at the time.\n",
      "\n",
      "With such an abundance of clues that distinguish newsgroups, the classifiers\n",
      "barely have to identify topics from text at all, and they all perform at the\n",
      "same high level.\n",
      "\n",
      "For this reason, the functions that load 20 Newsgroups data provide a\n",
      "parameter called **remove**, telling it what kinds of information to strip out\n",
      "of each file. **remove** should be a tuple containing any subset of\n",
      "``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\n",
      "blocks, and quotation blocks respectively.\n",
      "\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')  # doctest: +ELLIPSIS\n",
      "  0.77310...\n",
      "\n",
      "This classifier lost over a lot of its F-score, just because we removed\n",
      "metadata that has little to do with topic classification.\n",
      "It loses even more if we also strip this metadata from the training data:\n",
      "\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  # doctest: +ELLIPSIS\n",
      "  0.76995...\n",
      "\n",
      "Some other classifiers cope better with this harder version of the task. Try\n",
      "running :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without\n",
      "the ``--filter`` option to compare the results.\n",
      "\n",
      ".. topic:: Recommendation\n",
      "\n",
      "  When evaluating text classifiers on the 20 Newsgroups data, you\n",
      "  should strip newsgroup-related metadata. In scikit-learn, you can do this by\n",
      "  setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n",
      "  lower because it is more realistic.\n",
      "\n",
      ".. topic:: Examples\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Contoh cara mengakses datanya\n",
    "print( type(data) )\n",
    "print(dir(data))\n",
    "print(data.data[0]) # Dokumen pertama\n",
    "print(data.target_names[0]) # Dokumen pertama\n",
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contoh Label 3 dokumen pertama:  [0 2 0]\n"
     ]
    }
   ],
   "source": [
    "# Rubah struktur data diatas ke dalam bentuk struktur data sederhana: \"list of documents\"\n",
    "Y = data.target # List: 0 = class 1, 1 = class 2, ... dst\n",
    "print('Contoh Label 3 dokumen pertama: ',Y[:3])\n",
    "X = [doc for doc in data.data] # setiap elemen dalam list adalah dokumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Vector-Space-Model---VSM\">Vector Space Model - VSM</h1>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/vsm.png\" style=\"width: 300px; height: 213px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "\t<li>Data yang biasanya kita ketahui berbentuk <strong>tabular </strong>(tabel/kolom-baris/matriks/<em>array</em>/larik), data seperti ini disebut data terstruktur (<strong><em>structured data</em></strong>).</li>\n",
    "\t<li>Data terstruktur dapat disimpan dengan baik di&nbsp;<em>spreadsheet</em>&nbsp;(misal:&nbsp;<em>Excel/CSV</em>) atau basis data (<em>database</em>) relasional dan secara umum dapat digunakan langsung oleh berbagai model/<em>tools</em>&nbsp;statistik/data mining konvensional.</li>\n",
    "\t<li>Sebagian data yang lain memiliki &ldquo;<em>tags</em>&rdquo; yang menjelaskan elemen semantik yang berbeda di dalamnya dan cenderung tidak memiliki skema (struktur) yang statis.</li>\n",
    "\t<li>Data seperti ini disebut data<em>&nbsp;<strong>semi-structured</strong></em>, contohnya data dalam bentuk &nbsp;<strong><a href=\"http://www.w3.org/XML/\" target=\"_blank\">XML</a></strong>.</li>\n",
    "\t<li>Apa bedanya? Apa maksudnya tidak memiliki skema yang statis? Penjelasan mudahnya bayangkan sebuah data terstruktur (tabular), namun dalam setiap baris (<em>record/instance</em>)-nya tidak memiliki jumlah variabel (peubah) yang sama.</li>\n",
    "\t<li>Tentu saja data seperti ini tidak sesuai jika disimpan dan diolah dengan&nbsp;<em>tools/software</em>&nbsp;yang mengasumsikan struktur yang statis pada setiap barisnya (misal: Excel dan SPSS).</li>\n",
    "</ul>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/3_tipeData.png\" style=\"height: 400px ; width: 430px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "\t<li>Data multimedia seperti teks, gambar atau video <strong>tidak dapat</strong>&nbsp;<strong>secara langsung</strong>&nbsp;dianalisa dengan model statistik/data mining.</li>\n",
    "\t<li>Sebuah proses awal&nbsp;<em>(pre-process)</em>&nbsp;harus dilakukan terlebih dahulu untuk merubah data-data tidak (semi) terstruktur tersebut menjadi bentuk yang dapat digunakan oleh model statistik/data mining konvensional.</li>\n",
    "\t<li>Terdapat berbagai macam cara mengubah data-data tidak terstruktur tersebut ke dalam bentuk yang lebih sederhana, dan ini adalah suatu bidang ilmu tersendiri yang cukup dalam. Sebagai contoh saja sebuah teks biasanya dirubah dalam bentuk vektor/<em>topics</em>&nbsp;terlebih dahulu sebelum diolah.</li>\n",
    "\t<li>Vektor data teks sendiri bermacam-macam jenisnya: ada yang berdasarkan eksistensi (<strong><em>binary</em></strong>), frekuensi dokumen (<strong>tf</strong>), frekuensi dan invers jumlah dokumennya dalam corpus (<strong><a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\" target=\"_blank\">tf-idf</a></strong>), <strong>tensor</strong>, dan sebagainya.</li>\n",
    "\t<li>&nbsp;Proses perubahan ini sendiri biasanya tidak&nbsp;<em>lossless</em>, artinya terdapat cukup banyak informasi yang hilang. Maksudnya bagaimana? Sebagai contoh ketika teks direpresentasikan dalam vektor (sering disebut sebagai model <strong>bag-of-words</strong>) maka informasi urutan antar kata menghilang.&nbsp;</li>\n",
    "</ul>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/3_structureData.png\" style=\"height:270px; width:578px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Contoh bentuk umum representasi dokumen:</strong></p>\n",
    "\n",
    "\n",
    "<p><img alt=\"\" src=\"images/3_Bentuk umum representasi dokumen.JPG\" style=\"height: 294px ; width: 620px\" /></p>\n",
    "\n",
    "<p>Pada Model <em>n-grams</em> kolom bisa juga berupa frase.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Document-Term-Matrix-:-Vector-Space-Model---VSM\">Document-Term Matrix : Vector Space Model - VSM</h2>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/vsm_matrix.png\" style=\"width: 500px; height: 283px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/3_rumus tfidf.png\" style=\"height:370px; width:367px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/3_tfidf logic.jpg\" style=\"height:359px; width:638px\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/3_variant tfidf.png\" style=\"height:334px; width:955px\" /></p>\n",
    "K = |d|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menggunakan modul SciKit untuk merubah data tidak terstruktur ke VSM\n",
    "# Scikit implementation http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1653, 22279)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VSM - \"binari\"\n",
    "binary_vectorizer = CountVectorizer(binary = True, lowercase=True, stop_words='english')\n",
    "binari = binary_vectorizer.fit_transform(X)\n",
    "binari.shape # ukuran VSM\n",
    "# Apakah \"max_df\" dan \"min_df\" ?\n",
    "# MySQL menggunakan max_df = 0.5 (!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x22279 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 22 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sparse vectors/matrix\n",
    "binari[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[ 7299 11933 14036  3984 17120  7433  9237 13231  6532 16413 15805  2043\n",
      "  7992 11614  6919  3638 20943  9085 20134  2040  4102 21640]\n"
     ]
    }
   ],
   "source": [
    "# Mengakses Datanya\n",
    "print(binari[0].data)\n",
    "print(binari[0].indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'want': 21640, 'car': 4102, 'alarm': 2040, 'thinking': 20134, 'getting': 9085, 'ungo': 20943\n"
     ]
    }
   ],
   "source": [
    "# Kolom dan term\n",
    "print(str(binary_vectorizer.vocabulary_)[:93])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nama = ['udin', 'wati', 'budi']\n",
    "umur = [12, 31, 22]\n",
    "\n",
    "D = {n:u for n,u in zip(nama,umur)}\n",
    "D['budi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = binary_vectorizer.vocabulary_\n",
    "D = {k:v for v,k in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1653, 22279)\n",
      "[1 1 1 1 1 1 1 1 1 1 2 1 1 2 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[ 7299 11933 14036  3984 17120  7433  9237 13231  6532 16413 15805  2043\n",
      "  7992 11614  6919  3638 20943  9085 20134  2040  4102 21640]\n"
     ]
    }
   ],
   "source": [
    "# VSM term Frekuensi : \"tf\"\n",
    "tf_vectorizer = CountVectorizer(binary = False, lowercase=True, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(X)\n",
    "\n",
    "print(tf.shape) # Sama\n",
    "print(tf[99].data) # Hanya data ini yg berubah\n",
    "print(tf[0].indices) # Letak kolomnya tetap sama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4102"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = tf_vectorizer.vocabulary_\n",
    "d['car']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [2, 3, 7]\n",
    "A.index(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-130-a85280e2226d>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-130-a85280e2226d>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    sum(tf(:,4102))\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "sum(tf(:,4102))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1653, 22279)\n",
      "[0.11408864 0.21167989 0.21190224 0.16463918 0.13203394 0.27597159\n",
      " 0.19363625 0.10661563 0.16885084 0.14406544 0.45280155 0.15204456\n",
      " 0.26147306 0.13108874 0.17682996 0.10137342 0.1726183  0.27597159\n",
      " 0.27597159 0.27597159 0.22640078 0.1169025 ]\n",
      "[21640  4102  2040 20134  9085 20943  3638  6919 11614  7992  2043 15805\n",
      " 16413  6532 13231  9237  7433 17120  3984 14036 11933  7299]\n"
     ]
    }
   ],
   "source": [
    "# VSM term Frekuensi : \"tf-idf\"\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "print(tfidf.shape) # Sama\n",
    "print(tfidf[0].data) # Hanya data ini yg berubah\n",
    "print(tfidf[0].indices) # Letak kolomnya berbeda, namun jumlah kolom dan elemennya tetap sama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure\n",
    "print(tfidf_vectorizer.vocabulary_['car'])\n",
    "print(tf_vectorizer.vocabulary_['car'])\n",
    "# Yes, it's the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hati-hati ... SciPy Vectorizer bisa menghasilkan Matriks dengan beberapa barisnya kosong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah baris kosong di VSM tfidf =  61\n",
      "[68, 71, 100, 144, 166, 168, 187, 192, 198, 239, 260, 265, 271, 276, 300, 407, 461, 483, 488, 489, 493, 529, 551, 583, 626, 661, 703, 743, 777, 786, 795, 807, 845, 870, 873, 875, 908, 911, 1017, 1025, 1062, 1098, 1105, 1129, 1135, 1140, 1158, 1203, 1217, 1277, 1287, 1369, 1463, 1467, 1473, 1506, 1512, 1526, 1552, 1570, 1645]\n"
     ]
    }
   ],
   "source": [
    "# Pada contoh diatas:\n",
    "baris_kosong = []\n",
    "nBaris, nKolom = tfidf.shape\n",
    "for i in range(nBaris):\n",
    "    if sum(tfidf[i].data)==0:\n",
    "        baris_kosong.append(i)\n",
    "print('Jumlah baris kosong di VSM tfidf = ', len(baris_kosong))\n",
    "print(baris_kosong)\n",
    "# Mengapa ada baris kosong ini? ==> Diskusi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><u>Solusi?</u></h2>\n",
    "\n",
    "<ol>\n",
    "\t<li><strong>Periksa </strong>terlebih dahulu baris-baris kosong tersebut. Penyebabnya bisa bermacam-macam: encoding, tokenizer (pattern), memang dokumennya kosong, dokumennya pendek dan hanya berisi stopwords, dsb.</li>\n",
    "\t<li><strong>Ignore</strong>: Exclude dokumen-dokumen tersebut dari analisis.</li>\n",
    "\t<li><strong>Separate analysis</strong>: Pisahkan terlebih dahulu dokumen-dokumen diatas, lalu analisa secara terpisah dengan parameter settings yang berbeda atau preprocessing yang berbeda.&nbsp;</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1592, 22279) (1653, 22279) (1592, 22279)\n"
     ]
    }
   ],
   "source": [
    "# Contoh untuk di \"ignore\"\n",
    "tfidf_nonZeroRows = tfidf[tfidf.getnnz(1)>0] # Remove Zero Rows\n",
    "tfidf_nonZeroCols = tfidf[:,tfidf.getnnz(0)>0] # Remove Zero Columns. Becareful, it \"might\" change the VSM interpretation (word index)\n",
    "tfidf_nonZeroRC = tfidf[tfidf.getnnz(1)>0][:,tfidf.getnnz(0)>0] # Remove Zero Rows and Columns\n",
    "print(tfidf_nonZeroRows.shape, tfidf_nonZeroCols.shape, tfidf_nonZeroRC.shape)\n",
    "# Jika jumlah kolom tidak berubah, maka interpretasi (index kata) VSM tidak berubah (aman :) ).\n",
    "# Perintah ini sudah diimplementasikan di Fungsi ittc.IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom tf-idf:\n",
    "* Menurut http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "* default formula tf-idf yang digunakan sk-learn adalah:\n",
    "* $tfidf = tf * log(\\frac{N}{df+1})$ ==> Smooth IDF\n",
    "* namun kita merubahnya menjadi:\n",
    "* $tfidf = tf * log(\\frac{N}{df})$ ==> Non Smooth IDF\n",
    "* $tfidf = tf * log(\\frac{N}{df+1})$ ==> linear_tf, Smooth IDF\n",
    "* $tfidf = (1+log(tf)) * log(\\frac{N}{df})$ ==> sublinear_tf, Non Smooth IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1653, 22279)\n",
      "[0.11389068 0.17884514 0.21429549 0.16488526 0.13190432 0.29976114\n",
      " 0.1948156  0.10640536 0.16918467 0.14402551 0.39010977 0.15209175\n",
      " 0.27505603 0.13095381 0.17736715 0.10115873 0.17304162 0.29976114\n",
      " 0.29976114 0.29976114 0.23040512 0.11671129]\n",
      "[21640  4102  2040 20134  9085 20943  3638  6919 11614  7992  2043 15805\n",
      " 16413  6532 13231  9237  7433 17120  3984 14036 11933  7299]\n"
     ]
    }
   ],
   "source": [
    "# VSM term Frekuensi : \"tf-idf\"\n",
    "tfidf_vectorizer = TfidfVectorizer(smooth_idf= False, sublinear_tf=True, lowercase=True, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "print(tfidf.shape) # Sama\n",
    "print(tfidf[0].data) # Hanya data ini yg berubah\n",
    "print(tfidf[0].indices) # Letak kolomnya = tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Best-Match-Formula-:-BM25\">Best-Match Formula : BM25</h2>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/3_bm25_simple.png\" style=\"height: 123px; width: 300px;\" /></p>\n",
    "\n",
    "<ol>\n",
    "\t<li>di IR nilai b dan k yang optimal adalah :&nbsp;<strong> <em>b</em> = 0.75&nbsp; dan k = [1.2 - 2.0]&nbsp; &nbsp;</strong><br />\n",
    "\tref:&nbsp;<em>Christopher, D. M., Prabhakar, R., &amp; Hinrich, S. C. H. &Uuml;. T. Z. E. (2008). Introduction to information retrieval.&nbsp;An Introduction To Information Retrieval,&nbsp;151, 177.</em></li>\n",
    "\t<li>Tapi kalau untuk TextMining (clustering) nilai <strong>k optimal adalah 20, nilai b = sembarang (boleh = 0.75)</strong><br />\n",
    "\tref:&nbsp;<em>Whissell, J. S., &amp; Clarke, C. L. (2011). Improving document clustering using Okapi BM25 feature weighting.&nbsp;Information retrieval,&nbsp;14(5), 466-487.</em></li>\n",
    "\t<li><strong>avgDL </strong>adalah rata-rata panjang dokumen di seluruh dataset dan <strong>DL </strong>adalah panjang dokumen D.<br />\n",
    "\thati-hati, ini berbeda dengan &nbsp;tf-idf MySQL diatas.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.17653172, 3.94426281, 2.17653172, 2.17653172, 2.17653172,\n",
       "       2.17653172, 2.17653172, 2.17653172, 2.17653172, 2.17653172,\n",
       "       3.94426281, 2.17653172, 2.17653172, 2.17653172, 2.17653172,\n",
       "       2.17653172, 2.17653172, 2.17653172, 2.17653172, 2.17653172,\n",
       "       2.17653172, 2.17653172])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_ = tau.BM25(tf,tfidf)\n",
    "bm25_[0].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/3_BM_family.png\" style=\"height:300px; width:391px\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Zipf's-Law:--&quot;power-law&quot;-:\">Zipf&#39;s Law: &quot;power law&quot; :</h3>\n",
    "\n",
    "<img alt=\"\" src=\"images/zipf.png\" style=\"width: 456px; height: 270px;\" />\n",
    "<ul>\n",
    "\t<li><a href=\"https://plus.maths.org/content/mystery-zipf\" target=\"_blank\">https://plus.maths.org/content/mystery-zipf</a></li>\n",
    "\t<li><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4176592/\" target=\"_blank\">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4176592/</a></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1653, 22578)\n",
      "(1653, 4709)\n"
     ]
    }
   ],
   "source": [
    "# Frequency Filtering di VSM\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_1 = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.75, min_df=5)\n",
    "tfidf_2 = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "print(tfidf_1.shape)\n",
    "print(tfidf_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alasan melakukan filtering berdasarkan frekuensi:\n",
    "* Intuitively filter noise \n",
    "* Curse of Dimensionality (akan dibahas kemudian)\n",
    "* Computational Complexity\n",
    "* Improving accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'udin76': 35, 'minum': 21, 'kopi': 14, 'pagi': 24, 'sambil': 28, 'makan': 18, 'pisang': 25, 'goreng': 11, 'is': 12, 'the': 33, 'best': 6, 'belajar': 4, 'nlp': 22, 'dan': 8, 'text': 32, 'mining': 20, 'ternyata': 31, 'seru': 29, 'banget': 3, 'sadiezz': 27, 'sudah': 30, 'lumayan': 17, 'lama': 15, 'bingits': 7, 'tukang': 34, 'bakso': 2, 'belum': 5, 'lewat': 16, 'aduh': 0, 'ga': 10, 'mie': 19, 'ayam': 1, 'p4k4i': 23, 'kesyap': 13, 'please': 26, 'deh': 9}\n"
     ]
    }
   ],
   "source": [
    "# Variasi pembentukan matriks VSM:\n",
    "d1 = '@udin76, Minum kopi pagi-pagi sambil makan pisang goreng is the best'\n",
    "d2 = 'Belajar NLP dan Text Mining ternyata seru banget sadiezz'\n",
    "d3 =  'Sudah lumayan lama bingits tukang Bakso belum lewat'\n",
    "d4 = 'Aduh ga banget makan Mie Ayam p4k4i kesyap, please deh'\n",
    "\n",
    "D = [d1, d2, d3, d4]\n",
    "# Jika kita menggunakan cara biasa: \n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "vsm = tfidf_vectorizer.fit_transform(D)\n",
    "print(tfidf_vectorizer.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'udin76': 69, 'minum': 41, 'kopi': 27, 'pagi': 47, 'sambil': 55, 'makan': 34, 'pisang': 50, 'goreng': 21, 'is': 23, 'the': 65, 'best': 13, 'udin76 minum': 70, 'minum kopi': 42, 'kopi pagi': 28, 'pagi pagi': 48, 'pagi sambil': 49, 'sambil makan': 56, 'makan pisang': 36, 'pisang goreng': 51, 'goreng is': 22, 'is the': 24, 'the best': 66, 'belajar': 9, 'nlp': 43, 'dan': 16, 'text': 63, 'mining': 39, 'ternyata': 61, 'seru': 57, 'banget': 6, 'sadiezz': 54, 'belajar nlp': 10, 'nlp dan': 44, 'dan text': 17, 'text mining': 64, 'mining ternyata': 40, 'ternyata seru': 62, 'seru banget': 58, 'banget sadiezz': 8, 'sudah': 59, 'lumayan': 32, 'lama': 29, 'bingits': 14, 'tukang': 67, 'bakso': 4, 'belum': 11, 'lewat': 31, 'sudah lumayan': 60, 'lumayan lama': 33, 'lama bingits': 30, 'bingits tukang': 15, 'tukang bakso': 68, 'bakso belum': 5, 'belum lewat': 12, 'aduh': 0, 'ga': 19, 'mie': 37, 'ayam': 2, 'p4k4i': 45, 'kesyap': 25, 'please': 52, 'deh': 18, 'aduh ga': 1, 'ga banget': 20, 'banget makan': 7, 'makan mie': 35, 'mie ayam': 38, 'ayam p4k4i': 3, 'p4k4i kesyap': 46, 'kesyap please': 26, 'please deh': 53}\n"
     ]
    }
   ],
   "source": [
    "# N-Grams VSM\n",
    "# Bermanfaat untuk menangkap frase kata, misal: \"ga banget\", \"pisang goreng\", dsb\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "vsm = tfidf_vectorizer.fit_transform(D)\n",
    "print(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 0]\n",
      " [1 1 0 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'seru banget': 0,\n",
       " 'seru': 1,\n",
       " 'the best': 2,\n",
       " 'lama': 3,\n",
       " 'text mining': 4,\n",
       " 'nlp': 5,\n",
       " 'ayam': 6}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vocabulary based VSM\n",
    "# Bermanfaat untuk menghasilkan hasil analisa yang \"bersih\"\n",
    "# variasi 2\n",
    "d1 = '@udin76, Minum kopi pagi-pagi sambil makan pisang goreng is the best'\n",
    "d2 = 'Belajar NLP dan Text Mining ternyata seru banget sadiezz'\n",
    "d3 =  'Sudah lumayan lama bingits tukang Bakso belum lewat seru'\n",
    "d4 = 'Aduh ga banget makan Mie Ayam p4k4i kesyap, please deh'\n",
    "D = [d1,d2,d3,d4]\n",
    "Vocab = {'seru banget':0, 'seru':1, 'the best':2, 'lama':3, 'text mining':4, 'nlp':5, 'ayam':6}\n",
    "tf_vectorizer = CountVectorizer(binary = False, vocabulary=Vocab)\n",
    "tf = tf_vectorizer.fit_transform(D)\n",
    "print(tf.toarray())\n",
    "tf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab = {'seru banget':0, 'the best':1, 'lama':2, 'text mining':3, 'nlp':4, 'ayam':5}\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, lowercase=True, vocabulary=Vocab)\n",
    "vsm = tfidf_vectorizer.fit_transform(D)\n",
    "print(tfidf_vectorizer.vocabulary_)\n",
    "# VSM terurut sesuai definisi dan terkesan lebih \"bersih\"\n",
    "# Perusahaan besar biasanya memiliki menggunakan teknik ini dengan vocabulary yang comprehensif\n",
    "# Sangat cocok untuk Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latihan: bentuk tfidf dan BM25 untuk (sembarang) 4 kategori lain dari data News20 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><strong>Word Embeddings</strong></h2>\n",
    "\n",
    "<h2><img alt=\"\" src=\"images/3_word_embeddings.png\" style=\"height: 296px ; width: 602px\" /></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/3_word2vec_example.png\" style=\"height:400px; width:667px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Word2Vec</h3>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/3_word2Vec.png\" style=\"height:400px; width:636px\" /><br />\n",
    "Dikembangkan oleh Tomas Mikolov - Google :</p>\n",
    "\n",
    "<p>Goldberg, Yoav; Levy, Omer. &quot;word2vec Explained: Deriving Mikolov et al.&#39;s Negative-Sampling Word-Embedding Method&quot;.&nbsp;<a href=\"https://en.wikipedia.org/wiki/ArXiv\">arXiv</a>:<a href=\"https://arxiv.org/abs/1402.3722\">1402.3722</a> </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/BoW_VS_WordEmbedding.png\" style=\"width: 248px; height: 372px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hati-hati. Gensim menerima input berupa \"sentences of tokens\"\n",
    "# Misal: [['ibu', 'memasak', 'di', 'dapur'], ['ibu', 'bekerja', 'di','dapur'],['adik','membantu','ibu','di','dapur'],['bapak','di','garasi']]\n",
    "# Kita akan menggunakan data News 20 lagi, namun bentuk datanya dirubah seperti contoh diatas\n",
    "import pickle\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['sci.med', 'talk.politics.misc',  'rec.autos']\n",
    "try:\n",
    "    f = open('data/20newsgroups.pckl', 'rb')\n",
    "    data = pickle.load(f)\n",
    "    f.close()\n",
    "except:\n",
    "    data = fetch_20newsgroups(subset='train', categories=categories,remove=('headers', 'footers', 'quotes'))\n",
    "    f = open('data/20newsgroups.pckl', 'wb')\n",
    "    pickle.dump(data, f)\n",
    "    f.close()\n",
    "data = [doc for doc in data.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a400e4affe2487e9c25dba0fc660cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "car alarm ungo box doe knowledge experience alarm price range model good car alarm email responces cak3 ns3.lehigh.edu\n"
     ]
    }
   ],
   "source": [
    "# PreProcess (clean) the data ==> Silahkan lihat Fungsi \"cleanText\" di TSutanto_lib.py\n",
    "# Silahkan modifikasi jika diperlukan.\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "stops, lemmatizer = tau.LoadStopWords(lang='en')\n",
    "for i,d in tqdm(enumerate(data)):\n",
    "    data[i] = tau.cleanText(d, lemma = lemmatizer, stops = stops, symbols_remove = True, min_charLen = 2)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'alarm', 'ungo', 'box', 'doe', 'knowledge', 'experience', 'alarm', 'price', 'range', 'model', 'good', 'car', 'alarm', 'email', 'responces', 'cak3', 'ns3.lehigh.edu']\n"
     ]
    }
   ],
   "source": [
    "# Rubah bentuk data seperti yang dibutuhkan genSim\n",
    "# Bisa juga dilakukan dengan memodifikasi fungsi \"cleanText\" (agar lebih efisien)\n",
    "from textblob import TextBlob\n",
    "\n",
    "data_we = []\n",
    "for doc in data:\n",
    "    Tokens = [str(w) for w in TextBlob(doc).words]\n",
    "    data_we.append(Tokens)\n",
    "print(data_we[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!...\n"
     ]
    }
   ],
   "source": [
    "# https://radimrehurek.com/gensim/models/word2vec.html\n",
    "# train word2vec dengan data di atas\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "L = 300 # Jumlah neurons = ukuran vektor = jumlah kolom\n",
    "model_wv = Word2Vec(data_we, min_count=5, size=L, window = 5, workers= -1)\n",
    "# min_count adalah jumlah kata minimal yang muncul di corpus\n",
    "# \"size\" adalah Dimensionality of the word vectors \n",
    "# (menurut beberapa literature untuk text disarankan 300-500)\n",
    "# \"window\" adalah jarak maximum urutan kata yang di pertimbangkan\n",
    "# workers = jumlah prosesor yang digunakan untuk menjalankan word2vec\n",
    "print('Done!...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!...\n"
     ]
    }
   ],
   "source": [
    "# di data yang sebenarnya (i.e. besar) Gensim sering membutuhkan waktu cukup lama\n",
    "# Untungnya kita bisa menyimpan dan me-load kembali hasil perhitungan model word2vec, misal\n",
    "model_wv.save('data/model_w2v')\n",
    "model_wv = Word2Vec.load('data/model_w2v')\n",
    "print('Done!...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hati-hati, Word2vec menggunakan Matriks Dense\n",
    "\n",
    "<p>Penggunaan memory oleh Gensim kurang lebih sebagai berikut:</p>\n",
    "\n",
    "<p>Jumlah kata x &quot;size&quot; x 12 bytes</p>\n",
    "\n",
    "<p>Misal terdapat 100 000 kata unik dan menggunakan 200 layers, maka penggunaan memory =&nbsp;</p>\n",
    "\n",
    "<p>100,000x200x12 bytes = ~229MB</p>\n",
    "\n",
    "<p>Jika jumlah size semakin banyak, maka jumlah training data yang diperlukan juga semakin banyak, namun model akan semakin akurat.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "[-5.3057249e-04  3.7869232e-04  9.3328435e-07  1.4621025e-03\n",
      " -7.6356524e-04]\n"
     ]
    }
   ],
   "source": [
    "# Melihat vector suatu kata\n",
    "vektor = model_wv.wv.__getitem__(['car'])\n",
    "print(len(vektor[0])) # Panjang vektor keseluruhan = jumlah neuron yang digunakan\n",
    "print(vektor[0][:5]) # 5 elemen pertama dari vektornya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tomorrow', 0.2193901687860489),\n",
       " ('diameter', 0.20878145098686218),\n",
       " ('violate', 0.20504318177700043),\n",
       " ('rash', 0.18541431427001953),\n",
       " ('bother', 0.1815052032470703),\n",
       " ('risk', 0.17290973663330078),\n",
       " ('movement', 0.17045022547245026),\n",
       " ('medical', 0.1696965992450714),\n",
       " ('frequency', 0.16665229201316833),\n",
       " ('compartment', 0.1650572419166565)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mencari kata terdekat menurut data training dan Word2Vec\n",
    "model_wv.wv.most_similar('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.03199897\n",
      "-0.09292211\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Melihat similarity antar kata\n",
    "print(model_wv.wv.similarity('car', 'supra'))\n",
    "print(model_wv.wv.similarity('car', 'alarm'))\n",
    "print(model_wv.wv.similarity('car', 'car'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/3_cosine.png\" style=\"height:400px; width:683px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hati-hati Cosine adalah similarity bukan distance\n",
    "Hal ini akan mempengaruhi interpretasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error! kata \" cars \" tidak ada di training data\n"
     ]
    }
   ],
   "source": [
    "# error jika kata tidak ada di training data\n",
    "# beckman bukan beckmans ==> hence di Word Embedding PreProcessing harus thourough\n",
    "\n",
    "kata = 'cars'\n",
    "try:\n",
    "    print(model_wv.wv.most_similar(kata))\n",
    "except:\n",
    "    print('error! kata \"',kata,'\" tidak ada di training data')\n",
    "# ini salah satu kelemahan Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips:\n",
    "\n",
    "<p>Hati-hati GenSim tidak menggunakan seluruh kata di training data!.</p>\n",
    "\n",
    "<p>Perintah berikut akan menghasilkan kata-kata yang terdapat di vocabulary GenSim</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['car', 'alarm', 'box', 'doe', 'knowledge', 'experience', 'price', 'range', 'model', 'good', 'email', 'hear', 'question'\n"
     ]
    }
   ],
   "source": [
    "Vocabulary = model_wv.wv.vocab\n",
    "print(str(Vocabulary.keys())[:130])\n",
    "# Gunakan vocabulary ini (rubah ke \"set\") untuk membuat program menjadi lebih robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vacuum', 0.00023490722), ('hose', 0.00023490722), ('incentive', 0.00023490722), ('digest', 0.00023490722), ('recurrence', 0.00023490722), ('dizziness', 0.00023490722), ('cabin', 0.00023490722), ('pride', 0.00023490722), ('commander', 0.00023490722), ('translation', 0.00023490722)]\n"
     ]
    }
   ],
   "source": [
    "# \"predict\" vector for new data without re-training from the beginning\n",
    "d1 = ['new','generation','nvidia','gpu','is','rtx']\n",
    "d2 = ['deep','learning','computation','mostly', 'on', 'gpu']\n",
    "d3 = ['the','rtx','gpu','capable','super','sampling','ssdl']\n",
    "D = [d1,d2,d3]\n",
    "model_wv.train(D, total_examples=len(D), epochs=model_wv.epochs)\n",
    "print(model_wv.predict_output_word('gpu'))\n",
    "# Word2Vec ndak bisa \"predict\" kata yang ndak ada di vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hati-hati menginterpretasikan hasil Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\" FastText-(Facebook-2016)\">&nbsp;FastText (Facebook-2016)</h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Menggunakan Sub-words: app, ppl, ple - apple</li>\n",
    "\t<li>Paper:&nbsp;https://arxiv.org/abs/1607.04606&nbsp;&nbsp;</li>\n",
    "\t<li>Website:&nbsp;https://fasttext.cc/</li>\n",
    "\t<li>Source:&nbsp;https://github.com/facebookresearch/fastText&nbsp;</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "L = 300 # Jumlah neurons = ukuran vektor = jumlah kolom\n",
    "model_FT = FastText(data_we, size=L, window=4, min_count=1, workers=2)\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('card', 0.9999957084655762),\n",
       " ('carve', 0.9999952912330627),\n",
       " ('carbone', 0.999995231628418),\n",
       " ('cart', 0.9999951720237732),\n",
       " ('carroll', 0.9999947547912598),\n",
       " ('carrier', 0.9999945163726807),\n",
       " ('carbonized', 0.9999943971633911),\n",
       " ('carry', 0.9999942779541016),\n",
       " ('career', 0.9999942183494568),\n",
       " ('caravan', 0.9999939203262329)]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mencari kata terdekat menurut data training dan Word2Vec\n",
    "model_FT.wv.most_similar('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9999909\n",
      "0.9999918\n"
     ]
    }
   ],
   "source": [
    "# Melihat similarity antar kata\n",
    "print(model_FT.wv.similarity('car', 'car'))\n",
    "print(model_FT.wv.similarity('car', 'carpal'))\n",
    "print(model_FT.wv.similarity('car', 'carter'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidak error jika kata tidak ada di training data\n",
    "\n",
    "kata = 'beckmans'\n",
    "try:\n",
    "    print(model_FT.wv.most_similar(kata))\n",
    "except:\n",
    "    print('error! kata \"',kata,'\" tidak ada di training data')\n",
    "# ini adalah kelebihan lain FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tapi hati-hati dengan hasil ini:\n",
    "print(model_FT.wv.most_similar('beckman'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec error!\n",
      "[('industrial', 0.9998847842216492), ('petri', 0.9998841881752014), ('hus', 0.999883770942688), ('bloat', 0.9998834133148193), ('idiom.berkeley.ca.us', 0.9998831748962402), ('tri', 0.9998831748962402), ('erlangen.de', 0.999882698059082), ('bus', 0.9998822212219238), ('versus', 0.999882161617279), ('kessner', 0.9998817443847656)]\n"
     ]
    }
   ],
   "source": [
    "# atau bahkan bisa seperti ini ...\n",
    "try:\n",
    "    print(model_wv.wv.most_similar('industri'))\n",
    "except:\n",
    "    print('Word2Vec error!')\n",
    "    \n",
    "try:\n",
    "    print(model_FT.wv.most_similar('industri'))\n",
    "except:\n",
    "    print('FastText error!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diskusi:\n",
    "<ul>\n",
    "\t<li>Apakah kelebihan dan kekurangan WE secara umum?</li>\n",
    "\t<li>Apakah kira-kira aplikasi WE?</li>\n",
    "\t<li>Apakah bisa dijadikan representasi dokumen? Bagaimana caranya?</li>\n",
    "\t<li>Bergantung pada apa sajakah performa model WE?</li>\n",
    "</ul>\n",
    "\n",
    "* Preprocessing apa yang sebaiknya dilakukan pada model Word Embedding?\n",
    "* Apakah Pos Tag bermanfaat disini? Jika iya bagaimana menggunakannya?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Text Analytics</h2>\n",
    "\n",
    "<ul>\n",
    "\t<li>Tidak seperti data terstruktur, data tidak terstruktur seperti teks termasuk salah satu data yang cukup sulit untuk divisualisasikan.<br />\n",
    "\t<img alt=\"\" src=\"images/11_charts.jpg\" style=\"height:150px; width:276px\" /></li>\n",
    "\t<li>Namun terdapat Tools seperti Voyant yang dapat membantu dalam visualisasi sekaligus analisis.<br />\n",
    "\t<img alt=\"\" src=\"images/11_voyant.png\" style=\"height:118px; width:426px\" /></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Voyant-dapat-digunakan-dalam-2-cara:\">Voyant dapat digunakan dalam 2 cara:</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>\n",
    "\t<p><strong>Online</strong>:&nbsp;<a href=\"https://voyant-tools.org/\" target=\"_blank\">https://voyant-tools.org/</a><br />\n",
    "\t<u>Kelebihan</u>: Sederhana &amp; portable, tanpa harus install di komputer kita.<br />\n",
    "\t<u>Kekurangan</u>: butuh koneksi internet, tidak cocok untuk data teks yang besar, privacy.</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><strong>Offline </strong>di komputer kita [Java Based]</p>\n",
    "\t</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Instalasi-Voyant:\">Instalasi Voyant:</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Unduh Voyant dari &nbsp;<a href=\"https://github.com/sgsinclair/VoyantServer\" target=\"_blank\">https://github.com/sgsinclair/VoyantServer</a></li>\n",
    "\t<li>Extract Voyant ke sembarang folder (disarankan <strong>C:\\VoyantServer</strong>)</li>\n",
    "\t<li>Unduh Java JDK dari &nbsp;<a href=\"http://www.oracle.com/technetwork/java/javase/downloads/index.html&amp;nbsp\" target=\"_blank\">http://www.oracle.com/technetwork/java/javase/downloads/index.html&amp;nbsp</a>;</li>\n",
    "\t<li>Install Java</li>\n",
    "\t<li>Tambahkan variable <strong>Java Home</strong> ke &quot;System Variable&quot;</li>\n",
    "\t<li>Jalankan VoyantServer.jar dari&nbsp;C:\\VoyantServer<br />\n",
    "\t<strong>Tips</strong>: Jalankan Voyant Server sebelum Jupyter atau ganti &quot;port&quot; di VVoyant Server.</li>\n",
    "</ol>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/JavaPath.PNG\" style=\"width: 275px ; height: 300px\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh\n",
    "# [1]. Pertama-tama kita perlu menyimpan tweet sebagai file teks biasa\n",
    "file = 'data/dataTweet_plain.txt'\n",
    "with open(file, 'w') as f:\n",
    "    for d in D:\n",
    "        f.write(d+'\\n')\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>[2]. Jalankan Voyant secara offline atau online di URL&nbsp;<a href=\"https://voyant-tools.org/\" target=\"_blank\">https://voyant-tools.org/</a></p>\n",
    "\n",
    "<p>[3]. Upload file yang baru saja kita simpan (dataTweet_plain.txt).</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Penggunaan-Voyant-1:-WordClouds\">Penggunaan Voyant 1: WordClouds</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.</li>\n",
    "\t<li>slider terms: mengkontrol banyaknya terms yang disertakan.</li>\n",
    "\t<li><strong>Summary </strong>(statistics)</li>\n",
    "\t<li><strong>Documents </strong>==&gt; add more</li>\n",
    "\t<li><strong>Phrases </strong>(n-grams like)</li>\n",
    "\t<li><strong>Export </strong>Visualisasi (kanan atas - pertama)</li>\n",
    "\t<li><strong>Options </strong>(kanan atas ke-3): Font, size, stopwords, whitelist</li>\n",
    "\t<li>&quot;?&quot; ==&gt; More Help</li>\n",
    "</ol>\n",
    "\n",
    "<p>&nbsp;</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Penggunaan-Voyant-2:-Bubbles\">Penggunaan Voyant 2: Bubbles</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.<br />\n",
    "\tAtau file yang sudah terupload sebelumnya</li>\n",
    "\t<li>&nbsp;Klik tanda 4-kotak kecil (kanan atas ke-3)</li>\n",
    "\t<li>Pilih Visualization Tools ==&gt; Bubbles</li>\n",
    "\t<li>Option: hanya stopwords</li>\n",
    "\t<li>Export: Hanya PNG</li>\n",
    "</ol>\n",
    "\n",
    "<p>&nbsp;</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Penggunaan-Voyant-3:-Word-Tree\">Penggunaan Voyant 3: Word Tree</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.<br />\n",
    "\tAtau file yang sudah terupload sebelumnya</li>\n",
    "\t<li>Klik branch untuk expand</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Penggunaan-Voyant-2:-Bubbles\">Penggunaan Voyant 4: Links</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.<br />\n",
    "\tAtau file yang sudah terupload sebelumnya</li>\n",
    "\t<li>Visualization Tools ==&gt; Links</li>\n",
    "\t<li>Klik sembarang terms untuk expand</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Penggunaan-Voyant-5:-Trends\">Penggunaan Voyant 5: Trends</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.<br />\n",
    "\tAtau file yang sudah terupload sebelumnya</li>\n",
    "\t<li>Document Tools ==&gt; Trends</li>\n",
    "\t<li>.. Butuh preprocessing ...&nbsp;</li>\n",
    "\t<li>Data harus terurut waktu</li>\n",
    "\t<li>Berikut contohnya</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets[0]['created_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeTeks = [(t['created_at'], d) for t,d in zip(Tweets,D)]\n",
    "timeTeks[0]\n",
    "# %a %b %d %l %m %M %z %Y https://www.foragoodstrftime.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rubah Teks ke DateTime Format \n",
    "from datetime import datetime\n",
    "# %a %b %d %l %m %M %z %Y https://www.foragoodstrftime.com/\n",
    "d1 = datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n",
    "d2 = datetime.strptime('Jun 1 2004  1:33PM', '%b %d %Y %I:%M%p')\n",
    "d1>d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeTeks = [((datetime.strptime(t,'%a %b %d %H:%M:%S %z %Y')),txt) for t,txt in timeTeks]\n",
    "timeTeks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = [(34,12),(67,20),(80,33)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.sort(key=lambda t:t[1])\n",
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeTeks.sort(key=lambda tup: tup[0])  # sorts Text by Time\n",
    "timeTeks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plainTeks = [t for T,t in timeTeks]\n",
    "plainTeks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpan ke File\n",
    "namaFile = 'data/TeksUrutWaktu.txt'\n",
    "with open(namaFile, 'w') as f:\n",
    "        for tweet in plainTeks:\n",
    "            f.write(tweet+'\\n')\n",
    "'Done!, silahkan upload file tsb ke Voyant'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Latihan :&nbsp;</strong></p>\n",
    "\n",
    "<ol>\n",
    "\t<li>Crawl twitter dengan salah satu topik yang sedang trending saat ini di Indonesia.&nbsp;</li>\n",
    "\t<li>Tentukan beberapa Important user-nya (centrality analysis)</li>\n",
    "\t<li>Bentuk graph komunitasnya.</li>\n",
    "\t<li>Analisa graph diatas di gephi.</li>\n",
    "\t<li>Analisa graphnya di Voyant Tools.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>End of Module 07</h1>\n",
    "<hr />\n",
    "<p><img alt=\"\" src=\"images/2_Studying_Linguistic.png\" style=\"height:500px; width:667px\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from html import unescape\n",
    "from nltk import word_tokenize as wt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def fixTags(T):\n",
    "    getHashtags = re.compile(r\"#(\\w+)\")\n",
    "    pisahtags = re.compile(r'[A-Z][^A-Z]*')\n",
    "    t = T\n",
    "    tagS = re.findall(getHashtags, T)\n",
    "    for tag in tagS:\n",
    "        proper_words = ' '.join(re.findall(pisahtags, tag))\n",
    "        t = t.replace('#'+tag,proper_words)\n",
    "    return t\n",
    "\n",
    "def cleanText(T, fix, patternS):\n",
    "    t = T\n",
    "    for pattern in patternS: #in case in the future we need more pattern cleaning\n",
    "        t = re.sub(pattern,' ',t)\n",
    "    t = unescape(t)\n",
    "    t = fixTags(t)\n",
    "    t = t.lower()\n",
    "    for slang, formal in fix.items():\n",
    "        t = t.replace(slang,formal)\n",
    "    t = wt(t) # tokenize\n",
    "    t = [lemmatizer.lemmatize(token) for token in t]\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix = {'awsm':'awesome', \"they're\":'they are'}\n",
    "urlPattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "\n",
    "patternS = [urlPattern]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "the octopus power is > shark ! & they are awesome ! so happy to see them here !\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# Jawaban Latihan No 1\n",
    "tweet = \"The #OctopiPower is &gt; Sharks! &amp; they're awsm! So happy to see them here http://www.octopusVSshark.com !\"\n",
    "T = [tweet, 'status lain']\n",
    "\n",
    "C = []\n",
    "for t in T:\n",
    "    C.append( cleanText(t, fix, patternS) )\n",
    "print(' '.join(C[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# Contoh Jawaban Sederhana Latihan No 2\n",
    "from nltk import word_tokenize as wt\n",
    "\n",
    "T = 'pesawat b29 dan mig276 adalah kepunyaan fhg347x dan _24x_'\n",
    "T_Clean = []\n",
    "for t in wt(T):\n",
    "    if sum(c.isalpha() for c in t)==len(t):\n",
    "        T_Clean.append(t)\n",
    "print(' '.join(T_Clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\"> End of Module 07\n",
    "\n",
    "<hr />\n",
    "<p><img alt=\"\" src=\"images/9_Sentiment_Analysis_Meme.jpg\" /></p>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
