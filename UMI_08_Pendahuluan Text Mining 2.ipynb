{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://tau-data.id/umi/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img alt=\"\" src=\"images/0_Cover.jpg\"/></center> \n",
    "\n",
    "## <center><font color=\"blue\">Modul 08: Pendahuluan NLP & Text Preprocessing II</font></center>\n",
    "<b><center>(C) Taufik Sutanto - 2019</center>\n",
    "<center>tau-data Indonesia ~ https://tau-data.id ~ taufik@tau-data.id</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><font color=\"blue\">Pendahuluan NLP & Text Preprocessing II: Data Crawling & Sentiment Analysis</font></center>\n",
    "<img alt=\"\" src=\"images/PDS_logo.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <font color=\"blue\">Workshop Schedule</font>\n",
    "\n",
    "## <font color=\"green\">Hari ke-4 (Kamis, 30 Jan 2020)</font>\n",
    "\n",
    "**Pendahuluan Natural Language Processing & Text PreProcessing**\n",
    "* 09:00 – 11:00 \tText Preprocessing\n",
    "* 11:00 – 12:00\tCrawling, Streaming, Scraping\n",
    "* 13:00 – 14:00\tText Analytics\n",
    "* 14:00 – 15.00\tSentiment Analysis\n",
    "* 15:00 – 16.00\tLatihan Text Analytics dan Sentiment Analysis \n",
    "\n",
    "Studi Kasus: **Text Analytics data media sosial perbincangan agama di media sosial**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Gathering: Crawling, Streaming, Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"SMA-adalah-sebuah-proses-pengumpulan-data-dari-media-sosial-dan-analisanya-untuk-mendapatkan-'insights'-atau-informasi-berharga-untuk-suatu-tujuan-tertentu-(definisi-adopted-dari-Gartner*)\">SMA adalah sebuah proses pengumpulan data dari media sosial dan analisanya untuk mendapatkan &#39;insights&#39; atau informasi berharga untuk suatu tujuan tertentu (definisi diadopsi dari Gartner*).</h3>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/8_SMA.JPG\" style=\"width: 600px; height: 304px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Case-Study:-twitter\">Case Study: twitter</h2>\n",
    "<ol>\n",
    "\t<li>API Keys</li>\n",
    "\t<li>Rules</li>\n",
    "\t<li>Crawling by searching</li>\n",
    "    <li>tweet Json</li>\n",
    "\t<li>Crawling by Streaming and Scrapping</li>\n",
    "    <li>twitter Social Media Analytics</li>\n",
    "</ol>\n",
    "<img alt=\"\" src=\"images/6_twitter.png\" style=\"width: 300px; height: 300px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>twitter API Keys</h2>\n",
    "\n",
    "<h2><img alt=\"\" src=\"images/6_Creating_API_Keys.png\" style=\"width: 854px ; height: 444px\" /></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Aturan-twitter\">Aturan, bentuk data, &amp; error codes twitter</h2>\n",
    "\n",
    "<ol>\n",
    "\t<li>\n",
    "\t<p><a href=\"https://dev.twitter.com/rest/public/rate-limiting\" target=\"_blank\">https://</a><a href=\"https://dev.twitter.com/rest/public/rate-limiting\" target=\"_blank\">dev.twitter.com/rest/public/rate-limiting</a></p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><a href=\"https://dev.twitter.com/overview/terms/agreement-and-policy\" target=\"_blank\">https://dev.twitter.com/overview/terms/agreement-and-policy</a></p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><a href=\"https://dev.twitter.com/overview/api/response-codes\" target=\"_blank\">https://</a><a href=\"https://dev.twitter.com/overview/api/response-codes\" target=\"_blank\">dev.twitter.com/overview/api/response-codes</a></p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><a href=\"https://dev.twitter.com/overview/api/tweets\" target=\"_blank\">https://</a><a href=\"https://dev.twitter.com/overview/api/tweets\" target=\"_blank\">dev.twitter.com/overview/api/tweets</a></p>\n",
    "\t</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh API Keys (Sesuaikan dengan API keys masing-masing)\n",
    "# consumer_key, consumer_secret, access_token, access_secret\n",
    "Ck = ''\n",
    "Cs = ''\n",
    "At = '-'\n",
    "As = ''\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Koneksi ke twitter\n",
    "import taudata as tau, seaborn as sns; sns.set()\n",
    "\n",
    "twitter = tau.twitter_connect(Ck, Cs, At, As)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'jokowi prabowo'\n",
    "n = 300 # jumlah tweet yang ingin diambil\n",
    "# Kalau menggunakan API saya diatas, mohon n diisi tidak lebih dari 200\n",
    "# \"Agar peserta lain juga bisa mencoba\" ==> Due to rate limiting\n",
    "# Jika API keys memakai API Bapak/Ibu sendiri, silahkan n diganti dengan\n",
    "# nilai yang jauh lebih besar (misal n = 10000).\n",
    "Tweets = tau.getTweets(twitter, topic, N = n) # lan = 'id' atau 'en' ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bentuk tweet = Json. Contoh tweet pertama:\n",
    "Tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh mengakses data spesifik pada tweet yang pertama:\n",
    "print('tweet pertama oleh \"{}\" : \"{}\"'.format(Tweets[0]['user']['screen_name'],Tweets[0]['full_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PreProcessing/cleaning dilakukan dengan cara yang sama dengan sebelumnya\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "stops, lemmatizer = tau.LoadStopWords(lang='id')\n",
    "for i,d in tqdm(enumerate(D)):\n",
    "    D[i] = tau.cleanText(d, lemma=lemmatizer, stops = stops, symbols_remove = True, min_charLen = 2)\n",
    "print(D[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan hasil crawling twitter\n",
    "ftopic = topic.replace('\"','_').replace(' ','-')\n",
    "fileName = 'data/Tweets.json'\n",
    "tau.saveTweets(Tweets,file=fileName)\n",
    "print('Saved to '+fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Me-load kembali jika (misal) analisa ingin dilakukan di lain waktu\n",
    "# Sengaja nama variabelnya saya bedakan (T2)\n",
    "T2 = tau.loadTweets(file='data/Tweets.json')\n",
    "print('tweet pertama oleh \"{}\" : \"{}\"'.format(T2[0]['user']['screen_name'],T2[0]['full_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh mengambil hanya data tweet\n",
    "D = [t['full_text'] for t in Tweets]\n",
    "D[:5] # 5 tweet pertama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Pemilihan-KeyWords\">Pemilihan KeyWords yang baik</h2>\n",
    "\n",
    "<ul>\n",
    "\t<li><a href=\"https://medium.com/lingvo-masino/how-to-choose-keywords-in-twitter-9c3b85c50290\" target=\"_blank\">https://medium.com/lingvo-masino/how-to-choose-keywords-in-twitter-9c3b85c50290</a></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Operator (penting)\n",
    "\n",
    "<ul>\n",
    "\t<li><img alt=\"\" src=\"images/query_Operator.png\" style=\"width: 661px; height: 554px;\" /></li>\n",
    "    <li>Detail: <a href=\"https://developer.twitter.com/en/docs/tweets/search/guides/standard-operators.html\" target=\"_blank\">https://developer.twitter.com/en/docs/tweets/search/guides/standard-operators.html</a></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mari kita coba #1\n",
    "topic = 'data from:taufikedys'\n",
    "n = 5 # Contoh jumlah tweet yang ingin diambil\n",
    "T = tau.getTweets(twitter, topic, N = n) # lan = 'id' atau 'en' ...\n",
    "[t['full_text'] for t in T]\n",
    "\n",
    "# Lesson ... becarefull with duplicates ==> use Hash Function!!!... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locality-Sensitive Hashing\n",
    "\n",
    "* https://en.wikipedia.org/wiki/Locality_sensitive_hashing \n",
    "* https://en.wikipedia.org/wiki/MinHash\n",
    "* http://nbviewer.jupyter.org/github/mattilyra/LSH/blob/master/examples/Introduction.ipynb\n",
    "* http://infolab.stanford.edu/~ullman/mmds/ch3.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JANGAN LUPA pip install datasketch\n",
    "# Di adopsi dari https://stackoverflow.com/a/41792983/2844866\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from nltk import ngrams\n",
    "\n",
    "def fHasher(data, lsh): # Create MinHash objects\n",
    "    minhashes = {}\n",
    "    for c, i in enumerate(data):\n",
    "      minhash = MinHash(num_perm=128)\n",
    "      for d in ngrams(i, 3):\n",
    "        minhash.update(\"\".join(d).encode('utf-8'))\n",
    "      lsh.insert(c, minhash)\n",
    "      minhashes[c] = minhash\n",
    "    return minhashes\n",
    "\n",
    "def CalcJaccard(minhashes, T):\n",
    "    for i in range(len(minhashes.keys())):\n",
    "      print(\"Candidates with Jaccard similarity > \",str(T),\"  for input\", i, \":\", lsh.query(minhashes[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an MinHashLSH index optimized for Jaccard threshold \"T\"\n",
    "# that accepts MinHash objects with \"P\" permutations functions\n",
    "T, P = 0.7, 128\n",
    "lsh = MinHashLSH(threshold=T, num_perm=P)\n",
    "\n",
    "data = ['testing tweet 1 ... big data science',\n",
    "  'RT: testing tweet 1 ... big data science',\n",
    "  'testing tweet 2 ... big data science',\n",
    "  'Don\\'t mind me, I\\'m a cat']\n",
    "\n",
    "H = fHasher(data, lsh)\n",
    "CalcJaccard(H,T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/Jaccard_VS_COsine.png\" style=\"width: 455px; height: 330px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/Jaccard_eg.png\" style=\"width: 663px; height: 449px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bagaimana kalau contoh diatas dihitung dengan cosine?\n",
    "* Jaccar (Tanimoto) domainnya binary\n",
    "* Memperhitungkan tidak hanya \"persamaan\", tapi juga \"perbedaan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Yandex\n",
    "geolocator = Yandex(lang='en_US')\n",
    "\n",
    "location = geolocator.geocode(\"Depok\", timeout=10)\n",
    "\n",
    "if location != None:\n",
    "    print(location.address)\n",
    "    print(location.latitude, \" -> \", location.longitude)\n",
    "else:\n",
    "    print('Not Found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mari kita coba #3 gunakan google (map) untuk koordinat suatu lokasi\n",
    "# http://thoughtfaucet.com/search-twitter-by-location/\n",
    "# misal search tweet tentang \"makanan\" di Depok dan sekitarnya\n",
    "import tweepy#, csv\n",
    "\n",
    "auth = tweepy.auth.OAuthHandler(Ck, Cs)\n",
    "auth.set_access_token(At, As)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "#csvFile = open('data/tweets.csv', 'a') # Open/Create a file to append data\n",
    "#csvWriter = csv.writer(csvFile)#Use csv Writer\n",
    "Geo, N = \"-6.365649,106.820871,30km\", 30\n",
    "qry = 'makanan'\n",
    "for tweet in tweepy.Cursor(api.search,q=qry,count=100,geocode=Geo).items(N):\n",
    "    print([tweet.created_at, tweet.text.encode('utf-8'), tweet.user.id, tweet.geo])\n",
    "    #csvWriter.writerow([tweet.created_at, tweet.text.encode('utf-8'), tweet.user.id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Streaming-Data\">Streaming and Scrapping Data</h1>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/Meme_Streaming_Data.jpg\" style=\"width: 307px; height: 309px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming tweets. Untuk percobaan pilih topicS sesuatu yg sedang trending/populer \"saat ini\".\n",
    "# Atau bisa coba dengan mengirim tweet sendiri :)\n",
    "from twython import TwythonStreamer\n",
    "import json \n",
    "\n",
    "def streamTwitter(topicS, lang, fileHasil):\n",
    "    f = open(fileHasil, 'w')\n",
    "    class MyStreamer(TwythonStreamer):\n",
    "        def on_success(self, data):\n",
    "            global count\n",
    "            #print(data)\n",
    "            f.write(json.dumps(data)+'\\n')\n",
    "            count+=1;print(\"\\rNbr of Tweets captured: {}\".format(count), end=\"\")\n",
    "            if count==maxTweet:\n",
    "                print('\\nFinished Recording %.0f tweets' %(maxTweet)); self.disconnect()\n",
    "        def on_error(self, status_code, data):\n",
    "            print('Error Status = %s' %status_code); self.disconnect()\n",
    "\n",
    "    print('Start Streaming Data, Please Wait ... ')\n",
    "    while count<maxTweet:\n",
    "        stream = MyStreamer(Ck, Cs, At, As)\n",
    "        stream.statuses.filter(track=topicS)\n",
    "    f.close()\n",
    "    print('Finished, file saved to: ', fileHasil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTweet, count = 3, 0 # Rubah sesuai dengan kebutuhan, Untuk percobaan ini cukup (misal) 3 tweet\n",
    "lang = set(['en','id']) # bahasa bisa dipilih > 1\n",
    "topicS = ['jerukrasaduren'] # Bisa>1\n",
    "file = 'data/'+'-'.join(topicS)+'Streaming_Tweets.json'\n",
    "\n",
    "streamTwitter(topicS, lang, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping (going beyond 7 days)\n",
    "\n",
    "## http://taufiksutanto.blogspot.com/2017/11/twitter-crawl-menembus-batas-waktu-7.html\n",
    "### Solusi bagi yang belum memiliki API keys\n",
    "### Scrapping Script attached bersama lecture notes ini (scrapping.py : merubah html ==> csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh mengubah html hasil scrapping ke CSV\n",
    "fileSource = 'data/contoh_scrap.htm'\n",
    "fileOutput = 'data/contoh_scrap.csv'\n",
    "\n",
    "tau.twitter_html2csv(fileSource, fileOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawling & Streaming dengan Modul Pattern (\"Tanpa API Keys\") ~ Sangat terbatas\n",
    "\n",
    "# topic, N=300, Nbatch = 100 (max), fullTalk=False, language ='id'\n",
    "topic = 'pilpres2019'\n",
    "\n",
    "Tweets_noAPI = tau.crawl(topic, N=10, Nbatch = 5, fullTalk=False, language='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets_noAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Loading-Data\">Cara Lain (Google SpreadSheet add-on)</h2>\n",
    "\n",
    "<ol>\n",
    "\t<li>Login ke account Google (e.g. gmail atau youtube): <a href=\"https://mail.google.com\" target=\"_blank\">https://mail.google.com</a></li>\n",
    "\t<li>Login ke Account twitter (create jika belum memliki) :&nbsp;<a href=\"https://twitter.com/\" target=\"_blank\">https://twitter.com/</a>&nbsp;</li>\n",
    "\t<li>Klik link berikut:&nbsp;<a href=\"https://chrome.google.com/webstore/detail/twitter-archiver/pkanpfekacaojdncfgbjadedbggbbphi\" target=\"_blank\">https://chrome.google.com/webstore/detail/twitter-archiver/pkanpfekacaojdncfgbjadedbggbbphi</a></li>\n",
    "\t<li>Install Add-On tersebut (No 3.)</li>\n",
    "\t<li>Buka Google Spreadsheet:&nbsp;<a href=\"https://docs.google.com/spreadsheets\" target=\"_blank\">https://docs.google.com/spreadsheets</a></li>\n",
    "\t<li>Klik Add-On ==&gt; Twitter Archiever ==&gt; Create New Rule</li>\n",
    "\t<li>Isi sesuai minat.</li>\n",
    "\t<li>Save/Download Google Spreadsheet-nya sebagai CSV.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SA = tau.sentiment(D[100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau.printSA(SA, N=3, emo = 'negatif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mari kita dalami lebih jauh Topic Pembicaraan ini\n",
    "tf, tm, vec = tau.getTopics(D, n_topics=4, Top_Words=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyLDAvis, pyLDAvis.sklearn; pyLDAvis.enable_notebook()\n",
    "\n",
    "pyLDAvis.sklearn.prepare(tf, tm, vec)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membentuk Graph Tweet berdasarkan Mentions\n",
    "G = tau.Graph(Tweets,Label = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering pada Graph = Community Detection\n",
    "Gt = tau.Community(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now examine, who are the most \"important\" users in this Graph?\n",
    "Gt = tau.Centrality(G, N=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penjelasan & Code yang lebih lengkap tentang Centrality & Community Analysis\n",
    "http://taufiksutanto.blogspot.com/2017/11/community-detection-centrality-teori.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud\n",
    "tau.wordClouds(Tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color=\"blue\">Outline Sentiment Analysis :</font>\n",
    "\n",
    "* Corpus-Based Sentiment Analysis\n",
    "* Metode Supervised untuk Sentiment Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mulai dengan loading data\n",
    "import pickle\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "f = open('data/20newsgroups.pckl', 'rb')\n",
    "data = pickle.load(f)\n",
    "f.close()\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merubah data ke bentuk yang biasa kita gunakan\n",
    "D = [doc for doc in data.data]\n",
    "Y = data.target\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre processing\n",
    "import taudata as tau \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "stops, lemmatizer = tau.LoadStopWords(lang='en')\n",
    "for i,d in tqdm(enumerate(D)):\n",
    "    D[i] = tau.cleanText(d, lemma=lemmatizer, stops = stops, symbols_remove = True, min_charLen = 2)\n",
    "print(D[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bentuk VSM-nya\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2)\n",
    "X = tfidf_vectorizer.fit_transform(D)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jangan lupa langkah penting ini! ... \n",
    "Y = Y[X.getnnz(1)>0] # delete label dokumen yang memiliki row =0 di tfidf-nya\n",
    "X = X[X.getnnz(1)>0] # Remove Zero Rows\n",
    "\n",
    "# Kita juga ingin menghapus di D untuk keperluan eksperimen nanti di Cell Bawah\n",
    "# Karena D adalah list (bukan numpy array), maka caranya sedikit berbeda\n",
    "DD = []\n",
    "for i,b in enumerate(X.getnnz(1)>0):\n",
    "    if b: # not zeros in the VSM\n",
    "        DD.append(D[i])\n",
    "        \n",
    "print(X.shape, len(Y), len(DD))\n",
    "1653-1588"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ketika menggunakan Supervised Learning, selalu awali dengan membagi data menjadi Training & test\n",
    "# Mengapa?\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.3)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN: http://scikit-learn.org/stable/modules/neighbors.html\n",
    "from sklearn import neighbors\n",
    "\n",
    "n_neighbors = 10\n",
    "weights = 'distance'\n",
    "kNN = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "kNN.fit(X_train, Y_train)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediksi dengan k-NN\n",
    "y_kNN = kNN.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Bagaimana mengevaluasinya?</h2>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/6_Evaluasi_ML.JPG\" style=\"height:400px; width:515px\" /></p>\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kita gunakan metric yang umum\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(Y_test, y_kNN)\n",
    "# Untuk data teks seperti ini ternyata cukup baik\n",
    "# k-NN cocok untuk aplikasi seperti apakah?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes: http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "nbc = gnb.fit(X_train.toarray(), Y_train) # Kelemahan Implementasinya disini\n",
    "\n",
    "y_nbc = nbc.predict(X_test.toarray())\n",
    "accuracy_score(Y_test, y_nbc)\n",
    "# Hati-hati Sparse ==> Dense bisa memenuhi memory untuk data relatif cukup besar\n",
    "# Akurasi cukup baik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree: http://scikit-learn.org/stable/modules/tree.html\n",
    "from sklearn import tree\n",
    "\n",
    "DT = tree.DecisionTreeClassifier()\n",
    "DT = DT.fit(X_train, Y_train)\n",
    "\n",
    "y_DT = DT.predict(X_test)\n",
    "accuracy_score(Y_test, y_DT)\n",
    "# Akurasi relatif rendah ==> Mengapa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mari coba perbaiki dengan Random Forest\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RandomForest = RandomForestClassifier()\n",
    "RandomForest.fit(X_train, Y_train)\n",
    "\n",
    "y_RF = RandomForest.predict(X_test)\n",
    "accuracy_score(Y_test, y_RF)\n",
    "# Sedikit membaik (expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM: http://scikit-learn.org/stable/modules/svm.html\n",
    "from sklearn import svm\n",
    "\n",
    "dSVM = svm.SVC(decision_function_shape='ovo') # oneversus one SVM\n",
    "dSVM.fit(X_train, Y_train)\n",
    "\n",
    "y_SVM = dSVM.predict(X_test)\n",
    "accuracy_score(Y_test, y_SVM)\n",
    "# Mengapa akurasinya rendah? Mengejutkan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mari coba perbaiki dengan berbagai kernel\n",
    "# http://scikit-learn.org/stable/modules/svm.html#svm-kernels\n",
    "for kernel in ('linear', 'poly', 'rbf'):\n",
    "    dSVM = svm.SVC(kernel=kernel, gamma='auto')\n",
    "    dSVM.fit(X_train, Y_train)\n",
    "    y_SVM = dSVM.predict(X_test)\n",
    "    print(accuracy_score(Y_test, y_SVM))\n",
    "# Ah!!!... much better .... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network: http://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "NN = MLPClassifier(hidden_layer_sizes=(2, 100))\n",
    "NN.fit(X_train, Y_train)\n",
    "\n",
    "y_NN = NN.predict(X_test)\n",
    "accuracy_score(Y_test, y_NN)\n",
    "# Cukup Baik, coba rubah jumlah layer dan Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunggu dulu ... yang kita lakukan belum cukup valid/objektif ... Mengapa?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Cross Validation</h1>\n",
    "\n",
    "<h1><img alt=\"\" src=\"images/6_Cross_validation.png\" style=\"height:274px; width:485px\" /></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "\n",
    "svm_ = svm.SVC(kernel='linear')\n",
    "mulai = time.time()\n",
    "scores_svm = cross_val_score(svm_, X, Y, cv=10) # perhatikan sekarang kita menggunakan seluruh data\n",
    "waktu = time.time() - mulai\n",
    "# Interval Akurasi 95 CI \n",
    "print(\"Accuracy SVM: %0.2f (+/- %0.2f), Waktu = %0.3f detik\" % (scores_svm.mean(), scores_svm.std() * 2, waktu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagaimana dengan k-NN ?\n",
    "mulai = time.time()\n",
    "scores_kNN = cross_val_score(kNN, X, Y, cv=10) # perhatikan sekarang kita menggunakan seluruh data\n",
    "waktu = time.time() - mulai\n",
    "# Interval Akurasi 95 CI \n",
    "print(\"Accuracy k-NN: %0.2f (+/- %0.2f), Waktu = %0.3f detik\" % (scores_kNN.mean(), scores_kNN.std() * 2, waktu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kita bisa juga mengeluarkan metric evaluasi lainnya\n",
    "scores = cross_val_score(kNN, X, Y, cv=10, scoring='f1_macro')\n",
    "print(\"F1-Score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "# scoring pilih dari sini: http://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('classic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'SVM':scores_svm,'kNN':scores_kNN})\n",
    "sns.boxplot(data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apakah menggunakan Word Embedding di model-model diatas lebih baik?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperti Biasa kita mulai dengan tokenisasi dari setiap dokumennya\n",
    "from textblob import TextBlob\n",
    "\n",
    "data_we = []\n",
    "for doc in tqdm(DD): # Hati-hati kita pakai \"DD\"\n",
    "    Tokens = [str(w) for w in TextBlob(doc).words]\n",
    "    data_we.append(Tokens)\n",
    "print(data_we[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat Word Embeddingnya\n",
    "from gensim.models import FastText\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "N = len(data_we)\n",
    "L = 300 # Jumlah neurons = ukuran vektor = jumlah kolom\n",
    "model_FT = FastText(data_we, size=L, window=5, min_count=1, workers=-1)\n",
    "vsm_we = tau.we2vsm(model_FT, data_we)\n",
    "vsm_we.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lakukan klassifikasi seperti sebelumnya\n",
    "scores_kNN_we = cross_val_score(kNN, vsm_we, Y, cv=10) \n",
    "scores_svm_we = cross_val_score(svm_, vsm_we, Y, cv=10)\n",
    "\n",
    "df = pd.DataFrame({'SVM_WE':scores_svm_we,'kNN_WE':scores_kNN_we})\n",
    "sns.boxplot(data=df)\n",
    "plt.show()\n",
    "# Apakah lebih baik?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Latihan/Studi-Kasus:\">Latihan/Studi Kasus:</h1>\n",
    "\n",
    "<ol>\n",
    "\t<li>Pilih sembarang 5 kategori dari data 20 NewsGroup</li>\n",
    "\t<li>Lakukan seperti yang kita telah lakukan sebelumnya hingga Cross validasi</li>\n",
    "\t<li>Dengan data yang sama, coba rubah parameter preprocessingnya (misal MaxDf dan minDf): Apakah akurasinya membaik/memburuk?</li>\n",
    "\t<li>Coba bandingkan jika menggunakan stemming/lemma dan tidak.</li>\n",
    "\t<li>Coba membuat BoxPlot dari semua metode yang dibahas diatas.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/9_Sentiment_Analysis_Meme.jpg\" style=\"height:300px; width:400px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Apakah sentiment analysis?</strong></p>\n",
    "\n",
    "<p>Sentiment Analysis adalah suatu proses komputasi untuk menentukan apakah suatu penrnyataan bermakna positive, negative, atau netral.</p>\n",
    "\n",
    "<p>Terkadang disebut juga sebagai&nbsp;<strong>opinion mining.</strong></p>\n",
    "\n",
    "<p><strong>Contoh aplikasi Sentiment Analysis</strong></p>\n",
    "\n",
    "<ul>\n",
    "\t<li><strong>Business: tanggapan konsumen atas suatu produk</strong>.</li>\n",
    "\t<li><strong>Politics:&nbsp;</strong>Sentimen masyarakat sebagai strategi pemenangan pemilu/pilkada.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/9_SA_techniques.jpg\" style=\"height:300px; width:536px\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "# Lexicon Based berdasarkan \n",
    "# pattern = https://www.clips.uantwerpen.be/pages/pattern-en#sentiment\n",
    "Sentence = \"I hate Bakpia\"\n",
    "testimonial = TextBlob(Sentence)\n",
    "print(testimonial.sentiment)\n",
    "print('Polarity=Sentimen =', testimonial.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Sentiment menghasilkan Tuple berpasangan (<strong>Polaritas</strong>, <strong>Subjectivitas</strong>).&nbsp;</p>\n",
    "\n",
    "<p>Polaritas memiliki nilai [-1, 1] ==&gt; negative~positive Sentimen</p>\n",
    "\n",
    "<p>Subjectivity memiliki nilai antara 0 sampai 1, dimana 0 paling objective dan 1 paling subjective.</p>\n",
    "\n",
    "<p>Selain itu Modul Pattern juga dapat digunakan untuk melakukan <strong>Modality </strong>dan <strong>Mood </strong>Analysis ... :)&nbsp;</p>\n",
    "\n",
    "Nilai Modality antara -1 sampai +1 dimana nilai &gt;0.5 menyatakan Fakta.&nbsp; [Akurasi fungsi ini sekitar 68% di data Wikipedia]</p>\n",
    "\n",
    "<p>Sedangkan Fungsi Mood akan mengeluarkan output sebagai berikut:<br />\n",
    "<img alt=\"\" src=\"images/9_Mood.jpg\" style=\"height:100px; width:392px\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexicon Based (NLP) sangat flexible ... termasuk juga untuk menentukan Mood\n",
    "from pattern.en import parse, Sentence, parse\n",
    "from pattern.en import modality, mood\n",
    "s = \"According to science, breakfast is the most important meal.\" # weaseling\n",
    "s = parse(s, lemmata=True)\n",
    "s = Sentence(s)\n",
    "print(modality(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import mood\n",
    "s = \"I wish it would stop raining\" \n",
    "s = parse(s, lemmata=False)\n",
    "s = Sentence(s)\n",
    "print(mood(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagaimana Dengan Bahasa Indonesia?\n",
    "<p>[A simple trick]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = TextBlob(kalimat).translate(to='en')\n",
    "type(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SenSubModMood_ID(kalimat):\n",
    "    K = TextBlob(kalimat).translate(to='en')\n",
    "    pol,sub = K.sentiment\n",
    "    if pol>0:\n",
    "        pol='positive'\n",
    "    elif pol<0:\n",
    "        pol='negative'\n",
    "    else:\n",
    "        pol = 'netral'\n",
    "    if sub>0.5:\n",
    "        sub = 'Subjektif'\n",
    "    else:\n",
    "        sub = \"Objektif\"\n",
    "    s = parse(str(K), lemmata=True)\n",
    "    s = Sentence(s)\n",
    "    mod = modality(s)\n",
    "    if mod>0.5:\n",
    "        mod='Yakin'\n",
    "    else:\n",
    "        mod='Ragu'\n",
    "    mooD = mood(s)\n",
    "    return pol, sub, mod, mooD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kalimat = 'makan bakpia pakai kecap enak'\n",
    "SenSubModMood_ID(kalimat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "# Warning, mungkin lambat karena membentuk model classifier* terlebih dahulu.\n",
    "# *Berdasarkan NLTK corpus ==> Language dependent\n",
    "Sentence = \"Textblob is amazingly simple to use\"\n",
    "blob = TextBlob(Sentence, analyzer=NaiveBayesAnalyzer())\n",
    "blob.sentiment\n",
    "# Good Explanation: https://medium.com/nlpython/sentiment-analysis-analysis-ee5da4448e37\n",
    "# Output probabilitas prediksinya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagaimana dengan Sentiment Analysis menggunakan NBC untuk Bahasa indonesia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = ['suka', 'demen']\n",
    "pos_score = [0.8, 0.9]\n",
    "\n",
    "zip(pos,pos_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    " \n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "def bentukClassifier(wPos, wNeg): # ,Nt\n",
    "    positive_features = [(word_feats(pos), 'pos') for pos in wPos]\n",
    "    negative_features = [(word_feats(neg), 'neg') for neg in wNeg]\n",
    "    #neutral_features = [(word_feats(neu), 'neu') for neu in Nt]\n",
    "    train_set = negative_features + positive_features# + neutral_features\n",
    "    return NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "def prediksiSentiment(kalimat, wPos, wNeg, negasi):\n",
    "    pos, neg = 0.0, 0.0\n",
    "    posWords, negWords = [], []\n",
    "    K = cleanText(kalimat)\n",
    "    for w in wPos:\n",
    "        if w in K:\n",
    "            for ww in negasi:\n",
    "                kebalikan = False\n",
    "                inverted = ww+' '+w\n",
    "                if inverted in K:\n",
    "                    negWords.append(inverted)\n",
    "                    kebalikan = True\n",
    "                    break\n",
    "            if not kebalikan:\n",
    "                posWords.append(w)\n",
    "    for w in wNeg:\n",
    "        if w in K:\n",
    "            for ww in negasi:\n",
    "                kebalikan = False\n",
    "                inverted = ww+' '+w\n",
    "                if inverted in K:\n",
    "                    posWords.append(inverted)\n",
    "                    kebalikan = True\n",
    "                    break\n",
    "            if not kebalikan:\n",
    "                negWords.append(w)\n",
    "    \n",
    "    nPos, nNeg = len(posWords), len(negWords)\n",
    "    sum_ = nPos + nNeg\n",
    "    if sum_ == 0 or nPos==nNeg:\n",
    "        return 'netral', 0.0\n",
    "    else:\n",
    "        nPos, nNeg = nPos/sum_, nNeg/sum_\n",
    "        if nPos>nNeg and nPos>0.01:\n",
    "            return 'positif', nPos\n",
    "        elif nNeg>nPos and nNeg<-0.01:\n",
    "            return 'negatif', nNeg\n",
    "        else:\n",
    "            return 'netral', (nPos + nNeg)/2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fEmot = 'D:/Gdrive/Programs/Python/notebooks/Pixel-TagarConsulting/data/emoticon.txt'\n",
    "emotS = tau.loadCorpus(file=fEmot, sep='\\t')\n",
    "emotS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wPos = ('keren', 'suka', 'cinta', 'bagus', 'mantap', 'sadis', 'top')\n",
    "wNeg = ('jelek', 'benci','buruk', 'najis')\n",
    "wordS = (wPos, wNeg)\n",
    "negasi = ['ga', 'tidak']\n",
    "emotS = (emotPos, emotNeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"makan pempek minumnya teh panas, biasa aja :)\"\n",
    "prediksiSentiment(sentence, wPos, wNeg, negasi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagaimana jika mau melakukannya dengan model klasifikasi (supervised learning) lain seperti modul sebelumnya?\n",
    "(e.g. SVM, NN, DT, k-NN, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text Classification : independent variable\n",
    "d1 = 'Minum kopi pagi-pagi sambil makan pisang goreng is the best'\n",
    "d2 = 'Belajar NLP dan Text Mining ternyata seru banget'\n",
    "d3 = 'Palembang agak mendung hari ini'\n",
    "d4 =  'Sudah lumayan lama tukang Bakso belum lewat'\n",
    "d5 = 'Aduh ga banget makan Mie Ayam pakai kecap, please deh'\n",
    "d6 = 'Benci banget kalau melihat orang buang sampah sembarangan di jalan'\n",
    "d7 = 'Kalau liat orang ga taat aturan rasanya ingin ngegampar aja'\n",
    "d8 = 'Nikmatnya meniti jalan jalan penuh romansa di tengah kota bernuansa pendidikan'\n",
    "d9 = 'kemajuan bangsa ini ada pada kegigihan masyarakat dalam belajar dan bekerja'\n",
    "D = [d1,d2,d3,d4,d5,d6,d7,d8,d9]\n",
    "'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependent variable, misal 0=positif, 1=netral, 2=negatif\n",
    "Class = [0,0,1,1,2,2,2,1,0]\n",
    "dic = {0:'positif', 1:'netral', 2:'negatif'}\n",
    "print([dic[c] for c in Class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bentuk VSM-nya seperti kemarin (skip preprocessing)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "vsm = vectorizer.fit_transform(D)\n",
    "vsm = vsm[vsm.getnnz(1)>0][:,vsm.getnnz(0)>0] # Remove zero rows and columns\n",
    "print(vsm.shape)\n",
    "str(vectorizer.vocabulary_)[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lakukan klasifikasi (misal dengan SVM)\n",
    "dSVM = svm.SVC(kernel='linear')\n",
    "sen = dSVM.fit(vsm, Class).predict(vsm)\n",
    "print(accuracy_score(Class, sen))\n",
    "# Memakai seluruh training data karena sampel yang sangat kecil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\"> End of Module 08\n",
    "\n",
    "<hr />\n",
    "<p><img alt=\"\" src=\"images/6_SocMed_cartoon.png\" /></p>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
